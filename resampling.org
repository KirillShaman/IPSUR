#+STARTUP:   indent
#+TITLE:     Resampling Methods
#+AUTHOR:    G. Jay Kerns
#+EMAIL:     gkerns@ysu.edu
#+DESCRIPTION: This chapter introduces resampling methods in the context of estimation and hypothesis testing.
#+KEYWORDS: resampling bootstrap permutation testing
#+LANGUAGE:  en
#+OPTIONS:   \n:nil @:t ::t |:t ^:{} -:t f:nil *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:nil
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LINK_UP: index.html   
#+LINK_HOME: http://ipsur.org/index.html
#+BABEL: :session *R* :exports results :results value raw :cache no :tangle yes
#+INCLUDE: "prelim.R" src R :exports none :results hide

#+BEGIN_HTML
---
layout: chapter
title: IPSUR - Resampling Methods
previous: nonparametric-statistics.html
next: time-series.html
description: This chapter introduces resampling methods in the context of estimation and hypothesis testing.
keywords: resampling bootstrap permutation testing
---

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">Resampling Methods</a>
<ul>
<li><a href="#sec-1-1">Introduction</a></li>
<li><a href="#sec-1-2">Bootstrap Standard Errors</a></li>
<li><a href="#sec-1-3">Bootstrap Confidence Intervals</a>
<ul>
<li><a href="#sec-1-3-1">Percentile Confidence Intervals</a></li>
<li><a href="#sec-1-3-2">Student's \(t\) intervals (``normal intervals'')</a></li>
</ul>
</li>
<li><a href="#sec-1-4">Resampling in Hypothesis Tests</a>
<ul>
<li><a href="#sec-1-4-1">Comparison with the Two Sample <i>t</i> test</a></li>
</ul>
</li>
<li><a href="#sec-1-5">Exercises</a></li>
</ul>
</li>
</ul>
</div>
</div>

#+END_HTML

* Resampling Methods                                                 :resamp:
:PROPERTIES:
:tangle: R/resamp.R
:END:
#+latex: \label{cha:resampling-methods}

#+latex: \noindent 
Computers have changed the face of statistics. Their quick computational speed and flawless accuracy, coupled with large data sets acquired by the researcher, make them indispensable for many modern analyses. In particular, resampling methods (due in large part to Bradley Efron) have gained prominence in the modern statistician's repertoire. We first look at a classical problem to get some insight why. 

I have seen /Statistical Computing with \(\mathsf{R}\)/ by Rizzo \cite{Rizzo2008} and I recommend it to those looking for a more advanced treatment with additional topics. I believe that /Monte Carlo Statistical Methods/ by Robert and Casella \cite{Robert2004} has a new edition that integrates \(\mathsf{R}\) into the narrative.

#+latex: \paragraph*{What do I want them to know?}

- basic philosophy of resampling and why it is important
- resampling for standard errors and confidence intervals
- resampling for hypothesis tests (permutation tests)

** Introduction
#+latex: \label{sec:Introduction-Resampling}

-  Classical question :: Given a population of interest, how may we effectively learn some of its salient features, /e.g./, the population's mean? One way is through representative random sampling. Given a random sample, we summarize the information contained therein by calculating a reasonable statistic, /e.g./, the sample mean. Given a value of a statistic, how do we know whether that value is significantly different from that which was expected? We don't; we look at the /sampling distribution/ of the statistic, and we try to make probabilistic assertions based on a confidence level or other consideration. For example, we may find ourselves saying things like, "With 95% confidence, the true population mean is greater than zero".

- Problem :: Unfortunately, in most cases the sampling distribution is /unknown/. Thus, in the past, in efforts to say something useful, statisticians have been obligated to place some restrictive assumptions on the underlying population. For example, if we suppose that the population has a normal distribution, then we can say that the distribution of \(\overline{X}\) is normal, too, with the same mean (and a smaller standard deviation). It is then easy to draw conclusions, make inferences, and go on about our business. 

- Alternative :: We don't know what the underlying population distributions is, so let us /estimate/ it, just like we would with any other parameter. The statistic we use is the /empirical CDF/, that is, the function that places mass \(1/n\) at each of the observed data points \(x_{1},\ldots,x_{n}\) (see Section \ref{sec:empirical-distribution}). As the sample size increases, we would expect the approximation to get better and better (with IID observations, it does, and there is a wonderful theorem by Glivenko and Cantelli that proves it). And now that we have an (estimated) population distribution, it is easy to find the sampling distribution of any statistic we like: just *sample* from the empirical CDF many, many times, calculate the statistic each time, and make a histogram. Done! Of course, the number of samples needed to get a representative histogram is prohibitively large... human beings are simply too slow (and clumsy) to do this tedious procedure.

Fortunately, computers are very skilled at doing simple, repetitive tasks very quickly and accurately. So we employ them to give us a reasonable idea about the sampling distribution of our statistic, and we use the generated sampling distribution to guide our inferences and draw our conclusions. If we would like to have a better approximation for the sampling distribution (within the confines of the information contained in the original sample), we merely tell the computer to sample more. In this (restricted) sense, we are limited only by our current computational speed and pocket book.

In short, here are some of the benefits that the advent of resampling methods has given us:
- Fewer assumptions. :: We are no longer required to assume the population is normal or the sample size is large (though, as before, the larger the sample the better). 
- Greater accuracy. :: Many classical methods are based on rough upper bounds or Taylor expansions. The bootstrap procedures can be iterated long enough to give results accurate to several decimal places, often beating classical approximations.  
- Generality. :: Resampling methods are easy to understand and apply to a large class of seemingly unrelated procedures. One no longer needs to memorize long complicated formulas and algorithms.

#+begin_rem
Due to the special structure of the empirical CDF, to get an IID sample we just need to take a random sample of size \(n\), with replacement, from the observed data \(x_{1},\ldots,x_{n}\). Repeats are expected and acceptable. Since we already sampled to get the original data, the term /resampling/ is used to describe the procedure.
#+end_rem


#+latex: \paragraph*{General bootstrap procedure.}

The above discussion leads us to the following general procedure to approximate the sampling distribution of a statistic \(S=S(x_{1},x_{2},\ldots,x_{n})\) based on an observed simple random sample \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\) of size \(n\): 

1. Create many many samples \(\mathbf{x}_{1}^{\ast},\ldots,\mathbf{x}_{M}^{\ast}\), called /resamples/, by sampling with replacement from the data. 

1. Calculate the statistic of interest \(S(\mathbf{x}_{1}^{\ast}),\ldots,S(\mathbf{x}_{M}^{\ast})\) for each resample. The distribution of the resample statistics is called a /bootstrap distribution/.
 
1. The bootstrap distribution gives information about the sampling distribution of the original statistic \(S\). In particular, the bootstrap distribution gives us some idea about the center, spread, and shape of the sampling distribution of \(S\).

** Bootstrap Standard Errors
#+latex: \label{sec:Bootstrap-Standard-Errors}

Since the bootstrap distribution gives us information about a statistic's sampling distribution, we can use the bootstrap distribution to estimate properties of the statistic. We will illustrate the bootstrap procedure in the special case that the statistic \(S\) is a standard error. 

#+latex: \begin{example}
#+latex: \label{exa:Bootstrap-se-mean}

*Standard error of the mean.*  In this example we illustrate the bootstrap by estimating the standard error of the sample meanand we will do it in the special case that the underlying population is \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\).  

Of course, we do not really need a bootstrap distribution here because from Section \ref{sec:sampling-from-normal-dist} we know that \(\overline{X}\sim\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{n})\), but we proceed anyway to investigate how the bootstrap performs when we know what the answer should be ahead of time.

We will take a random sample of size \(n=25\) from the population. Then we will /resample/ the data 1000 times to get 1000 resamples of size 25. We will calculate the sample mean of each of the resamples, and will study the data distribution of the 1000 values of \(\overline{x}\).

#+begin_src R :exports code :results silent 
srs <- rnorm(25, mean = 3)
resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
#+end_src

A histogram of the 1000 values of \(\overline{x}\) is shown in Figure \ref{fig:Bootstrap-se-mean}, and was produced by the following code.

#+srcname: Bootstrap-se-mean
#+begin_src R :exports code :results silent
hist(xbarstar, breaks = 40, prob = TRUE)
curve(dnorm(x, 3, 0.2), add = TRUE) # overlay true normal density
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/Bootstrap-se-mean.ps
  <<Bootstrap-se-mean>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/Bootstrap-se-mean.svg
  <<Bootstrap-se-mean>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/Bootstrap-se-mean.ps}
  \caption[Bootstrapping the standard error of the mean, simulated data]{\small The original data were 25 observations generated from a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1)\) distribution. We next resampled to get 1000 resamples, each of size 25, and calculated the sample mean for each resample. A histogram of the 1000 values of \(\overline{x}\) is shown above. Also shown (with a solid line) is the true sampling distribution of \(\overline{X}\), which is a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=0.2)\) distribution. Note that the histogram is centered at the sample mean of the original data, while the true sampling distribution is centered at the true value of \(\mu=3\). The shape and spread of the histogram is similar to the shape and spread of the true sampling distribution.}
  \label{fig:Bootstrap-se-mean}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Bootstrap-se-mean" class="figure">
  <p><img src="svg/Bootstrap-se-mean.svg" width=500 alt="svg/Bootstrap-se-mean.svg" /></p>
  <p>Bootstrapping the standard error of the mean, simulated data.</p>
</div>
#+end_html

We have overlain what we know to be the true sampling distribution of \(\overline{X}\), namely, a \(\mathsf{norm}(\mathtt{mean}=3,\,\mathtt{sd}=1/\sqrt{25})\) distribution. The histogram matches the true sampling distribution pretty well with respect to shape and spread... but notice how the histogram is off-center a little bit. This is not a coincidence -- in fact, it can be shown that the mean of the bootstrap distribution is exactly the mean of the original sample, that is, the value of the statistic that we originally observed. Let us calculate the mean of the bootstrap distribution and compare it to the mean of the original sample:

#+begin_src R :exports both :results output pp 
mean(xbarstar)
mean(srs)
mean(xbarstar) - mean(srs)
#+end_src

#+latex: \end{example}

Notice how close the two values are. The difference between them is an estimate of how biased the original statistic is, the so-called /bootstrap estimate of bias/. Since the estimate is so small we would expect our original statistic (\(\overline{X}\)) to have small bias, but this is no surprise to us because we already knew from Section \ref{sub:simple-random-samples} that \(\overline{X}\) is an unbiased estimator of the population mean.

Now back to our original problem, we would like to estimate the standard error of \(\overline{X}\). Looking at the histogram, we see that the spread of the bootstrap distribution is similar to the spread of the sampling distribution. Therefore, it stands to reason that we could estimate the standard error of \(\overline{X}\) with the sample standard deviation of the resample statistics. Let us try and see.

#+begin_src R :exports both :results output pp 
sd(xbarstar)
#+end_src

We know from theory that the true standard error is \(1/\sqrt{25}=0.20\). Our bootstrap estimate is not very far from the theoretical value. 

#+begin_rem
What would happen if we take more resamples? Instead of 1000 resamples, we could increase to, say, 2000, 3000, or even 4000... would it help? The answer is both yes and no. Keep in mind that with resampling methods there are two sources of randomness: that from the original sample, and that from the subsequent resampling procedure. An increased number of resamples would reduce the variation due to the second part, but would do nothing to reduce the variation due to the first part.

We only took an original sample of size \(n=25\), and resampling more and more would never generate more information about the population than was already there. In this sense, the statistician is limited by the information contained in the original sample. 
#+end_rem

#+latex: \begin{example}
#+latex: \label{exa:Bootstrap-se-median}

*Standard error of the median.* We look at one where we do not know the answer ahead of time. This example uses the =rivers=\index{Data sets!rivers@\texttt{rivers}} data set. Recall the stemplot on page \vpageref{ite:stemplot-rivers} that we made for these data which shows them to be markedly right-skewed, so a natural estimate of center would be the sample median. Unfortunately, its sampling distribution falls out of our reach. We use the bootstrap to help us with this problem, and the modifications to the last example are trivial.

#+begin_src R :exports both :results output pp 
resamps <- replicate(1000, sample(rivers, 141, TRUE), simplify = FALSE)
medstar <- sapply(resamps, median, simplify = TRUE)
sd(medstar)
#+end_src

#+srcname: Bootstrapping-se-median
#+begin_src R :exports code :results silent
hist(medstar, breaks = 40, prob = TRUE)
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file ps/Bootstrapping-se-median.ps
  <<Bootstrapping-se-median>>
#+end_src

#+begin_src R :exports none :results graphics silent :noweb yes :file svg/Bootstrapping-se-median.svg
  <<Bootstrapping-se-median>>
#+end_src

#+begin_latex
\begin{figure}[th]
  \includegraphics[angle=270, totalheight=4in]{ps/Bootstrapping-se-median.ps}
  \caption[Bootstrapping the standard error of the median for the \texttt{rivers} data]{\small Bootstrapping the standard error of the median for the \texttt{rivers} data.}
  \label{fig:Bootstrapping-se-median}
\end{figure}
#+end_latex

#+begin_html
<div id="fig-Bootstrapping-se-median" class="figure">
  <p><img src="svg/Bootstrapping-se-median.svg" width=500 alt="svg/Bootstrapping-se-median.svg" /></p>
  <p>Bootstrapping the standard error of the median for the <code>rivers</code> data.</p>
</div>
#+end_html

The graph is shown in Figure \ref{fig:Bootstrapping-se-median}, and was produced by the following code.

#+begin_src R :exports code :eval never
hist(medstar, breaks = 40, prob = TRUE)
#+end_src

#+begin_src R :exports both :results output pp 
median(rivers)
mean(medstar)
mean(medstar) - median(rivers)
#+end_src

#+latex: \end{example}

#+latex: \begin{example}
*The boot package in R*. It turns out that there are many bootstrap procedures and commands already built into base \(\mathsf{R}\), in the =boot= package. Further, inside the =boot= package there is even a function called =boot=\index{boot@\texttt{boot}}. The basic syntax is of the form:

:  boot(data, statistic, R)

#+latex: \end{example}
 Here, =data= is a vector (or matrix) containing the data to be resampled, =statistic= is a defined function, /of two arguments/, that tells which statistic should be computed, and the parameter \(\mathsf{R}\) specifies how many resamples should be taken.

For the standard error of the mean (Example \ref{exa:Bootstrap-se-mean}):

#+begin_src R :exports both :results output pp 
library(boot)
mean_fun <- function(x, indices) mean(x[indices])
boot(data = srs, statistic = mean_fun, R = 1000)
#+end_src

For the standard error of the median (Example \ref{exa:Bootstrap-se-median}):

#+begin_src R :exports both :results output pp 
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
#+end_src

We notice that the output from both methods of estimating the standard errors produced similar results. In fact, the =boot= procedure is to be preferred since it invisibly returns much more information (which we will use later) than our naive script and it is much quicker in its computations.

#+begin_rem
Some things to keep in mind about the bootstrap:

- For many statistics, the bootstrap distribution closely resembles the sampling distribution with respect to spread and shape. However, the bootstrap will not have the same center as the true sampling distribution. While the sampling distribution is centered at the population mean (plus any bias), the bootstrap distribution is centered at the original value of the statistic (plus any bias). The =boot= function gives an empirical estimate of the bias of the statistic as part of its output. 

- We tried to estimate the standard error, but we could have (in principle) tried to estimate something else. Note from the previous remark, however, that it would be useless to estimate the population mean \(\mu\) using the bootstrap since the mean of the bootstrap distribution is the observed \(\overline{x}\).  

- You don't get something from nothing. We have seen that we can take a random sample from a population and use bootstrap methods to get a very good idea about standard errors, bias, and the like. However, one must not get lured into believing that by doing some random resampling somehow one gets more information about the parameters than that which was contained in the original sample. Indeed, there is some uncertainty about the parameter due to the randomness of the original sample, and there is even more uncertainty introduced by resampling. One should think of the bootstrap as just another estimation method, nothing more, nothing less.

#+end_rem

** Bootstrap Confidence Intervals
#+latex: \label{sec:Bootstrap-Confidence-Intervals}

*** Percentile Confidence Intervals

As a first try, we want to obtain a 95% confidence interval for a parameter. Typically the statistic we use to estimate the parameter is centered at (or at least close by) the parameter; in such cases a 95% confidence interval for the parameter is nothing more than a 95% confidence interval for the statistic. And to find a 95% confidence interval for the statistic we need only go to its sampling distribution to find an interval that contains 95% of the area. (The most popular choice is the equal-tailed interval with 2.5% in each tail.)

This is incredibly easy to accomplish with the bootstrap. We need only to take a bunch of bootstrap resamples, order them, and choose the \(\alpha/2\)th and \((1-\alpha)\)th percentiles. There is a function =boot.ci=\index{boot.ci@\texttt{boot.ci}} in \(\mathsf{R}\) already created to do just this. Note that in order  to use the function =boot.ci= we must first run the =boot= function and save the output in a variable, for example, =data.boot=. We then plug =data.boot= into the function =boot.ci=.


#+latex: \begin{example}
#+latex: \label{exa:percentile-interval-median-first}

*Percentile interval for the expected value of the median.* We will try the naive approach where we generate the resamples and calculate the percentile interval by hand.

#+begin_src R :exports both :results output pp 
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE), simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
#+end_src

#+latex: \end{example}

#+latex: \begin{example}
*Confidence interval for expected value of the median, second try.*  Now we will do it the right way with the =boot= function.

#+begin_src R :exports both :results output pp 
library(boot)
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
#+end_src

#+latex: \end{example}

*** Student's \(t\) intervals (``normal intervals'')

The idea is to use confidence intervals that we already know and let the bootstrap help us when we get into trouble. We know that a \(100(1-\alpha)\%\) confidence interval for the mean of a \(SRS(n)\) from a normal distribution is 
\begin{equation} 
\overline{X}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\frac{S}{\sqrt{n}},
\end{equation} 
where \(\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)\) is the appropriate critical value from Student's \(t\) distribution, and we remember that an estimate for the standard error of \(\overline{X}\) is \(S/\sqrt{n}\). Of course, the estimate for the standard error will change when the underlying population distribution is not normal, or when we use a statistic more complicated than \(\overline{X}\). In those situations the bootstrap will give us quite reasonable estimates for the standard error. And as long as the sampling distribution of our statistic is approximately bell-shaped with small bias, the interval 
\begin{equation}
\mbox{statistic}\pm\mathsf{t}_{\alpha/2}(\mathtt{df}=n-1)*\mathrm{SE}(\mbox{statistic})
\end{equation}
 will have approximately \(100(1-\alpha)\%\) confidence of containing \(\mathbb{E}(\mathrm{statistic})\). 

#+latex: \begin{example}
We will use the t-interval method to find the bootstrap CI for the median. We have looked at the bootstrap distribution; it appears to be symmetric and approximately mound shaped. Further, we may check that the bias is approximately 40, which on the scale of these data is practically negligible. Thus, we may consider looking at the \(t\)-intervals. Note that, since our sample is so large, instead of \(t\)-intervals we will essentially be using \(z\)-intervals. 
#+latex: \end{example}

We see that, considering the scale of the data, the confidence intervals compare with each other quite well.

#+begin_rem
We have seen two methods for bootstrapping confidence intervals for a statistic. Which method should we use? If the bias of the bootstrap distribution is small and if the distribution is close to normal, then the percentile and \(t\)-intervals will closely agree. If the intervals are noticeably different, then it should be considered evidence that the normality and bias conditions are not met. In this case, /neither/ interval should be used.
#+end_rem

- \(BC_{a}\): bias-corrected and accelerated
   - transformation invariant
   - more correct and accurate
   - not monotone in coverage level?
- \(t\)-intervals
   - more natural
   - numerically unstable
- Can do things like transform scales, compute confidence intervals, and then transform back.
- Studentized bootstrap confidence intervals where is the Studentized version of is the  order statistic of the simulation

** Resampling in Hypothesis Tests
#+latex: \label{sec:Resampling-in-Hypothesis}

The classical two-sample problem can be stated as follows: given two groups of interest, we would like to know whether these two groups are significantly different from one another or whether the groups are reasonably similar. The standard way to decide is to 
1. Go collect some information from the two groups and calculate an associated statistic, for example, \(\overline{X}_{1}-\overline{X}_{2}\). 
1. Suppose that there is no difference in the groups, and find the distribution of the statistic in 1. 
1. Locate the observed value of the statistic with respect to the distribution found in 2. A value in the main body of the distribution is not spectacular, it could reasonably have occurred by chance. A value in the tail of the distribution is unlikely, and hence provides evidence /against/ the null hypothesis that the population distributions are the same.  

Of course, we usually compute a /p-value/, defined to be the probability of the observed value of the statistic or more extreme when the null hypothesis is true. Small \(p\)-values are evidence against the null hypothesis. It is not immediately obvious how to use resampling methods here, so we discuss an example.

#+latex: \begin{example}
A study concerned differing dosages of the antiretroviral drug AZT. The common dosage is 300mg daily. Higher doses cause more side affects, but are they significantly higher? We examine for a 600mg dose. The data are as follows: We compare the scores from the two groups by computing the difference in their sample means. The 300mg data were entered in x1 and the 600mg data were entered into x2. The observed difference was

| 300mg  | 284 | 279 | 289 | 292 | 287 | 295 | 285 | 279 | 306 | 298 |
| 600mg  | 298 | 307 | 297 | 279 | 291 | 335 | 299 | 300 | 306 | 291 |

The average amounts can be found:
: > mean(x1)
: [1] 289.4
: > mean(x2)
: [1] 300.3

with an observed difference of =mean(x2) - mean(x1) = 10.9=. As expected, the 600 mg measurements seem to have a higher average, and we might be interested in trying to decide if the average amounts are =significantly= different. The null hypothesis should be that there is no difference in the amounts, that is, the groups are more or less the same. If the null hypothesis were true, then the two groups would indeed be the same, or just one big group. In that case, the observed difference in the sample means just reflects the random assignment into the arbitrary =x1= and =x2= categories. It is now clear how we may resample, consistent with the null hypothesis.

#+latex: \paragraph*{Procedure:}

1. Randomly resample 10 scores from the combined scores of =x1= and =x2=, and assign then to the =x1= group. The rest will then be in the =x2= group. Calculate the difference in (re)sampled means, and store that value.  
1. Repeat this procedure many, many times and draw a histogram of the resampled statistics, called the /permutation distribution/. Locate the observed difference 10.9 on the histogram to get the \(p\)-value. If the \(p\)-value is small, then we consider that evidence against the hypothesis that the groups are the same. 

#+latex: \end{example}

#+begin_rem
In calculating the permutation test /p-value/, the formula is essentially the proportion of resample statistics that are greater than or equal to the observed value. Of course, this is merely an /estimate/ of the true \(p\)-value. As it turns out, an adjustment of \(+1\) to both the numerator and denominator of the proportion improves the performance of the estimated \(p\)-value, and this adjustment is implemented in the =ts.perm= function.
#+end_rem

#+begin_src R :exports both :results output pp 
library(coin)
oneway_test(len ~ supp, data = ToothGrowth)
#+end_src

*** Comparison with the Two Sample /t/ test

We know from Chapter \ref{cha:Hypothesis-Testing} to use the two-sample \(t\)-test to tell whether there is an improvement as a result of taking the intervention class. Note that the \(t\)-test assumes normal underlying populations, with unknown variance, and small sample \(n=10\). What does the \(t\)-test say? Below is the output. 

#+begin_src R :exports both :results output pp 
t.test(len ~ supp, data = ToothGrowth, 
       alt = "greater", var.equal = TRUE)
#+end_src

#+begin_src R :exports none :results silent
A <- show(oneway_test(len ~ supp, data = ToothGrowth))
B <- t.test(len ~ supp, data = ToothGrowth, alt = "greater", var.equal = TRUE)
#+end_src

The \(p\)-value for the \(t\)-test was \( SRC_R{round(B$p.value, 3)} \), while the permutation test \(p\)-value was \( SRC_R{round(A$p.value, 3)} \). Note that there is an underlying normality assumption for the \(t\)-test, which isn't present in the permutation test. If the normality assumption may be questionable, then the permutation test would be more reasonable. We see what can happen when using a test in a situation where the assumptions are not met: smaller \(p\)-values. In situations where the normality assumptions are not met, for example, small sample scenarios, the permutation test is to be preferred. In particular, if accuracy is very important then we should use the permutation test. 

#+begin_rem
Here are some things about permutation tests to keep in mind.
- While the permutation test does not require normality of the populations (as contrasted with the \(t\)-test), nevertheless it still requires that the two groups are exchangeable; see Section \ref{sec:Exchangeable-Random-Variables}. In particular, this means that they must be identically distributed under the null hypothesis. They must have not only the same means, but they must also have the same spread, shape, and everything else. This assumption may or may not be true in a given example, but it will rarely cause the \(t\)-test to outperform the permutation test, because even if the sample standard deviations are markedly different it does not mean that the population standard deviations are different. In many situations the permutation test will also carry over to the \(t\)-test.
- If the distribution of the groups is close to normal, then the \(t\)-test \(p\)-value and the bootstrap \(p\)-value will be approximately equal. If they differ markedly, then this should be considered evidence that the normality assumptions do not hold.  
- The generality of the permutation test is such that one can use all kinds of statistics to compare the two groups. One could compare the difference in variances or the difference in (just about anything). Alternatively, one could compare the ratio of sample means, \(\overline{X}_{1}/\overline{X}_{2}\). Of course, under the null hypothesis this last quantity should be near 1. 
- Just as with the bootstrap, the answer we get is subject to variability due to the inherent randomness of resampling from the data. We can make the variability as small as we like by taking sufficiently many resamples. How many? If the conclusion is very important (that is, if lots of money is at stake), then take thousands. For point estimation problems typically, \(R=1000\) resamples, or so, is enough. In general, if the true \(p\)-value is \(p\) then the standard error of the estimated \(p\)-value is \(\sqrt{p(1-p)/R}\). You can choose \(R\) to get whatever accuracy desired.
#+end_rem

- Other possible testing designs:
   - Matched Pairs Designs. 
   - Relationship between two variables. 

#+latex: \newpage{}

** Exercises
#+latex: \setcounter{thm}{0}
