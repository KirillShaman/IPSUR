<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
lang="en" xml:lang="en">
<head>
<title>\fontsize{30}{35}\selectfont Introduction to Probability and Statistics Using \textsf{R}</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="\fontsize{24}{28}\selectfont \noun{Second Edition}"/>
<meta name="author" content="\fontsize{24}{28}\selectfont G. Jay Kerns"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">

<h1 class="title">\fontsize{30}{35}\selectfont Introduction to Probability and Statistics Using \textsf{R}</h1>
<p>This book was expanded from lecture materials I use in a one semester upper-division undergraduate course entitled <i>Probability and Statistics</i> at Youngstown State University. Those lecture materials, in turn, were based on notes that I transcribed as a graduate student at Bowling Green State University. The course for which the materials were written is 50-50 Probability and Statistics, and the attendees include mathematics, engineering, and computer science majors (among others). The catalog prerequisites for the course are a full year of calculus.
</p>
<p>
The book can be subdivided into three basic parts. The first part includes the introductions and elementary <i>descriptive statistics</i>; I want the students to be knee-deep in data right out of the gate. The second part is the study of <i>probability</i>, which begins at the basics of sets and the equally likely model, journeys past discrete/continuous  random variables, and continues through to multivariate distributions. The chapter on sampling distributions paves the way to the third part, which is <i>inferential statistics</i>. This last part includes point and interval estimation, hypothesis testing, and finishes with introductions to selected topics in applied statistics.
</p>
<p>
I usually only have time in one semester to cover a small subset of this book. I cover the material in Chapter 2 in a class period that is supplemented by a take-home assignment for the students. I spend a lot of time on Data Description, Probability, Discrete, and Continuous Distributions. I mention selected facts from Multivariate Distributions in passing, and discuss the meaty parts of Sampling Distributions before moving right along to Estimation (which is another chapter I dwell on considerably). Hypothesis Testing goes faster after all of the previous work, and by that time the end of the semester is in sight. I normally choose one or two final chapters (sometimes three) from the remaining to survey, and regret at the end that I did not have the chance to cover more.
</p>
<p>
In an attempt to be correct I have included material in this book which I would normally not mention during the course of a standard lecture. For instance, I normally do not highlight the intricacies of measure theory or integrability conditions when speaking to the class. Moreover, I often stray from the matrix approach to multiple linear regression because many of my students have not yet been formally trained in linear algebra. That being said, it is important to me for the students to hold something in their hands which acknowledges the world of mathematics and statistics beyond the classroom, and which may be useful to them for many semesters to come. It also mirrors my own experience as a student.
</p>
<p>
The vision for this document is a more or less self contained, essentially complete, correct, introductory textbook. There should be plenty of exercises for the student, with full solutions for some, and no solutions for others (so that the instructor may assign them for grading). By <code>Sweave</code>'s dynamic nature it is possible to write randomly generated exercises and I had planned to implement this idea already throughout the book. Alas, there are only 24 hours in a day. Look for more in future editions.
</p>
<p>
Seasoned readers will be able to detect my origins: <i>Probability and Statistical Inference</i> by Hogg and Tanis \cite{Hogg2006}, <i>Statistical Inference</i> by Casella and Berger \cite{Casella2002}, and <i>Theory of Point Estimation</i> and <i>Testing Statistical Hypotheses</i> by Lehmann \cite{Lehmann1998,Lehmann1986}. I highly recommend each of those books to every reader of this one. Some \textsf{R} books with ``introductory'' in the title that I recommend are <i>Introductory Statistics with R</i> 
by Dalgaard \cite{Dalgaard2008} and <i>Using R for Introductory Statistics</i> by Verzani \cite{Verzani2005}. Surely there are many, many other good introductory books about \textsf{R}, but frankly, I have tried to steer clear of them for the past year or so to avoid any undue influence on my own writing.
</p>
<p>
I would like to make special mention of two other books: <i>Introduction to Statistical Thought</i> by Michael Lavine \cite{Lavine2009} and <i>Introduction to Probability</i> by Grinstead and Snell \cite{Grinstead1997}. Both of these books are <i>free</i> and are what ultimately convinced me to release \IPSUR under a free license, too.
</p>
<p>
Please bear in mind that the title of this book is ``Introduction to Probability and Statistics Using \textsf{R}'', and not ``Introduction to \textsf{R} Using Probability and Statistics'', nor even ``Introduction to Probability and Statistics and \textsf{R} Using Words''. The people at the party are Probability and Statistics; the handshake is \textsf{R}. There are several important topics about \textsf{R} which some individuals will feel are underdeveloped, glossed over, or wantonly omitted. Some will feel the same way about the probabilistic and/or statistical content. Still others will just want to learn \textsf{R} and skip all of the mathematics.
</p>
<p>
Despite any misgivings: here it is, warts and all. I humbly invite said individuals to take this book, with the GNU Free Documentation License (GNU-FDL) in hand, and make it better. In that spirit there are at least a few ways in my view in which this book could be improved.
</p>
<dl>
<dt>Better data.</dt><dd>The data analyzed in this book are almost entirely from the <code>datasets</code> package in base \textsf{R}, and here is why:

<ul>
<li>I made a conscious effort to minimize dependence on contributed packages,

</li>
<li>The data are instantly available, already in the correct format, so we need not take time to manage them, and 

</li>
<li>The data are <i>real</i>.
</li>
</ul>

</dd>
</dl>


<p>
I made no attempt to choose data sets that would be interesting to the students; rather, data were chosen for their potential to convey a statistical point. Many of the data sets are decades old or more (for instance, the data used to introduce simple linear regression are the speeds and stopping distances of cars in the 1920's).
</p>
<p>
In a perfect world with infinite time I would research and contribute recent, <i>real</i> data in a context crafted to engage the students in <i>every</i> example. One day I hope to stumble over said time. In the meantime, I will add new data sets incrementally as time permits.
</p>
<dl>
<dt>More proofs.</dt><dd>I would like to include more proofs for the sake of completeness (I understand that some people would not consider more proofs to be improvement). Many proofs have been skipped entirely, and I am not aware of any rhyme or reason to the current omissions. I will add more when I get a chance.  

</dd>
<dt>More and better~graphics.</dt><dd>I have not used the <code>ggplot2</code> package \cite{Wickam2009} because I do not know how to use it yet. It is on my to-do list.

</dd>
<dt>More and better exercises.</dt><dd>There are only a few exercises in the first edition simply because I have not had time to write more. I have toyed with the <code>exams</code> package \cite{exams} and I believe that it is a right way to move forward. As I learn more about what the package can do I would like to incorporate it into later editions of this book.
</dd>
</dl>


<p>
\IPSUR contains many interrelated parts: the <i>Document</i>, the <i>Program</i>, the <i>Package</i>, and the <i>Ancillaries</i>. In short, the <i>Document</i> is what you are reading right now. The <i>Program</i> provides an efficient means to modify the Document. The <i>Package</i> is an \textsf{R} package that houses the Program and the Document. Finally, the <i>Ancillaries</i> are extra materials that reside in the Package and were produced by the Program to supplement use of the Document. We briefly describe each of them in turn.
</p>

<p>
The <i>Document</i> is that which you are reading right now &ndash; \IPSUR's <i>raison d'^etre</i>. There are transparent copies (nonproprietary text files) and opaque copies (everything else). See the GNU-FDL in Appendix <a href="#cha-GNU-Free-Documentation">GNU-Free-Documentation</a> for more precise language and details.
</p>

<dl>
<dt>IPSUR.tex</dt><dd>is a transparent copy of the Document to be typeset with a \LaTeX{} distribution such as Mik\TeX{} or \TeX{} Live. Any reader is free to modify the Document and release the modified version in accordance with the provisions of the GNU-FDL. Note that this file cannot be used to generate a randomized copy of the Document. Indeed, in its released form it is only capable of typesetting the exact version of \IPSUR which you are currently reading. Furthermore, the <code>.tex</code> file is unable to generate any of the ancillary materials. 

</dd>
<dt>IPSUR-xxx.eps, IPSUR-xxx.pdf</dt><dd>are the image files for every graph in the Document. These are needed when typesetting with \LaTeX{}.

</dd>
<dt>IPSUR.pdf</dt><dd>is an opaque copy of the Document. This is the file that instructors would likely want to distribute to students. 

</dd>
<dt>IPSUR.dvi</dt><dd>is another opaque copy of the Document in a different file format.
</dd>
</dl>



<p>
The <i>Program</i> includes <code>IPSUR.lyx</code> and its nephew <code>IPSUR.Rnw</code>; the purpose of each is to give individuals a way to quickly customize the Document for their particular purpose(s).
</p>

<dl>
<dt>IPSUR.lyx</dt><dd>is the source \LyX{} file for the Program, released under the GNU General Public License (GNU GPL) Version 3. This file is opened, modified, and compiled with \LyX{}, a sophisticated open-source document processor, and may be used (together with <code>Sweave</code>) to generate a randomized, modified copy of the Document with brand new data sets for some of the exercises and the solution manuals (in the Second Edition). Additionally, \LyX{} can easily activate/deactivate entire blocks of the document, /e.g./~the \textsf{proofs} of the theorems, the student \textsf{solutions} to the exercises, or the instructor \textsf{answers} to the problems, so that the new author may choose which sections (s)he would like to include in the final Document (again, Second Edition). The <code>IPSUR.lyx</code> file is all that a person needs (in addition to a properly configured system &ndash; see Appendix <a href="#cha-Instructions-for-Instructors">Instructions-for-Instructors</a>) to generate/compile/export to all of the other formats described above and below, which includes the ancillary materials <code>IPSUR.Rdata</code> and <code>IPSUR.R</code>.

</dd>
<dt>IPSUR.Rnw</dt><dd>is another form of the source code for the Program, also released under the GNU GPL Version 3. It was produced by exporting <code>IPSUR.lyx</code> into\textsf{ R}/Sweave format (<code>.Rnw</code>). This file may be processed with Sweave to generate a randomized copy of <code>IPSUR.tex</code> &ndash; a transparent copy of the Document &ndash; together with the ancillary materials <code>IPSUR.Rdata</code> and <code>IPSUR.R</code>. Please note, however, that <code>IPSUR.Rnw</code> is just a simple text file which does not support many of the extra features that \LyX{} offers such as WYSIWYM editing, instantly (de)activating branches of the manuscript, and more. 
</dd>
</dl>



<p>
There is a contributed package on <code>CRAN</code>, called <code>IPSUR</code>. The package affords many advantages, one being that it houses the  Document in an easy-to-access medium. Indeed, a student can have the Document at his/her fingertips with only three commands:
</p>



<pre class="example">install.packages("IPSUR")
library(IPSUR)
read(IPSUR)

</pre>




<p>
Another advantage goes hand in hand with the Program's license; since \IPSUR is free, the source code must be freely available to anyone that wants it. A package hosted on <code>CRAN</code> allows the author to obey the license by default.
</p>
<p>
A much more important advantage is that the excellent facilities at \textsf{R}-Forge are building and checking the package daily against patched and development versions of the absolute latest pre-release of \textsf{R}. If any problems surface then I will know about it within 24 hours.
</p>
<p>
And finally, suppose there is some sort of problem. The package structure makes it <i>incredibly</i> easy for me to distribute bug-fixes and corrected typographical errors. As an author I can make my corrections, upload them to the repository at \textsf{R}-Forge, and they will be reflected <i>worldwide</i> within hours. We aren't in Kansas anymore, Toto.
</p>

<p>
These are extra materials that accompany \IPSUR. They reside in the <code>/etc</code> subdirectory of the package source. 
</p>
<dl>
<dt>IPSUR.RData</dt><dd>is a saved image of the \textsf{R} workspace at the completion of the Sweave processing of \IPSUR. It can be loaded into memory with \textsf{File} \(\triangleright\) \textsf{Load Workspace} or with the command <code>load("/path/to/IPSUR.Rdata")</code>. Either method will make every single object in the file immediately available and in memory. In particular, the data BLANK from Exercise BLANK in Chapter BLANK on page BLANK will be loaded. Type BLANK at the command line (after loading <code>IPSUR.RData</code>) to see for yourself.  

</dd>
<dt>IPSUR.R</dt><dd>is the exported \textsf{R} code from <code>IPSUR.Rnw</code>. With this script, literally every \textsf{R} command from the entirety of \IPSUR can be resubmitted at the command line.
</dd>
</dl>



<p>
We use the notation <code>x</code> or <code>stem.leaf</code> notation to denote objects, functions, <i>etc</i>.. The sequence ``\textsf{Statistics} \textsf{$\triangleright$} \textsf{Summaries} \textsf{$\triangleright$} \textsf{Active Dataset}'' means to click the \textsf{Statistics} menu item, next click the \textsf{Summaries} submenu item, and finally click \textsf{Active Dataset}.
</p>

<p>
This book would not have been possible without the firm mathematical and statistical foundation provided by the professors at Bowling Green State University, including Drs. G\'{a}bor Sz\'{e}kely, Craig Zirbel, Arjun K. Gupta, Hanfeng Chen, Truc Nguyen, and James Albert. I would also like to thank Drs.~Neal Carothers and Kit Chan. 
</p>
<p>
I would also like to thank my colleagues at Youngstown State University for their support. In particular, I would like to thank Dr. G. Andy Chang for showing me what it means to be a statistician.
</p>
<p>
I would like to thank Richard Heiberger for his insightful comments and improvements to several points and displays in the manuscript. 
</p>
<p>
Finally, and most importantly, I would like to thank my wife for her patience and understanding while I worked hours, days, months, and years on a <i>free book</i>. Looking back, I can't believe I ever got away with it.
</p>



<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 An Introduction to Probability and Statistics </a>
<ul>
<li><a href="#sec-1-1">1.1 Probability </a></li>
<li><a href="#sec-1-2">1.2 Statistics </a></li>
</ul>
</li>
<li><a href="#sec-2">2 Data Description </a>
<ul>
<li><a href="#sec-2-1">2.1 Types of Data </a>
<ul>
<li><a href="#sec-2-1-1">2.1.1 Quantitative data </a></li>
<li><a href="#sec-2-1-2">2.1.2 Displaying Quantitative Data </a></li>
<li><a href="#sec-2-1-3">2.1.3 Qualitative Data, Categorical Data, and Factors </a></li>
<li><a href="#sec-2-1-4">2.1.4 Displaying Qualitative Data </a></li>
<li><a href="#sec-2-1-5">2.1.5 Logical Data </a></li>
<li><a href="#sec-2-1-6">2.1.6 Missing Data </a></li>
<li><a href="#sec-2-1-7">2.1.7 Other Data Types </a></li>
</ul>
</li>
<li><a href="#sec-2-2">2.2 Features of Data Distributions </a>
<ul>
<li><a href="#sec-2-2-1">2.2.1 Center </a></li>
<li><a href="#sec-2-2-2">2.2.2 Spread </a></li>
<li><a href="#sec-2-2-3">2.2.3 Shape </a></li>
<li><a href="#sec-2-2-4">2.2.4 Clusters and Gaps </a></li>
<li><a href="#sec-2-2-5">2.2.5 Extreme Observations and other Unusual Features </a></li>
</ul>
</li>
<li><a href="#sec-2-3">2.3 Descriptive Statistics </a>
<ul>
<li><a href="#sec-2-3-1">2.3.1 Frequencies and Relative Frequencies </a></li>
<li><a href="#sec-2-3-2">2.3.2 Measures of Center </a></li>
<li><a href="#sec-2-3-3">2.3.3 Order Statistics and the Sample Quantiles </a></li>
<li><a href="#sec-2-3-4">2.3.4 Measures of Spread </a></li>
<li><a href="#sec-2-3-5">2.3.5 Measures of Shape \label{sub:Measures-of-Shape} </a></li>
</ul>
</li>
<li><a href="#sec-2-4">2.4 Exploratory Data Analysis </a>
<ul>
<li><a href="#sec-2-4-1">2.4.1 More About Stemplots </a></li>
<li><a href="#sec-2-4-2">2.4.2 Hinges and the Five Number Summary </a></li>
<li><a href="#sec-2-4-3">2.4.3 Boxplots </a></li>
<li><a href="#sec-2-4-4">2.4.4 Outliers </a></li>
<li><a href="#sec-2-4-5">2.4.5 Standardizing variables </a></li>
</ul>
</li>
<li><a href="#sec-2-5">2.5 Multivariate Data and Data Frames </a>
<ul>
<li><a href="#sec-2-5-1">2.5.1 Bivariate Data </a></li>
<li><a href="#sec-2-5-2">2.5.2 Multivariate Data </a></li>
</ul>
</li>
<li><a href="#sec-2-6">2.6 Comparing Populations </a>
<ul>
<li><a href="#sec-2-6-1">2.6.1 Numerically </a></li>
<li><a href="#sec-2-6-2">2.6.2 Graphically </a></li>
<li><a href="#sec-2-6-3">2.6.3 Lattice Graphics </a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-3">3 Probability </a>
<ul>
<li><a href="#sec-3-1">3.1 Sample Spaces </a>
<ul>
<li><a href="#sec-3-1-1">3.1.1 Sampling from Urns </a></li>
</ul>
</li>
<li><a href="#sec-3-2">3.2 Events </a>
<ul>
<li><a href="#sec-3-2-1">3.2.1 Functions for Finding Subsets </a></li>
<li><a href="#sec-3-2-2">3.2.2 Set Union, Intersection, and Difference </a></li>
</ul>
</li>
<li><a href="#sec-3-3">3.3 Model Assignment </a>
<ul>
<li><a href="#sec-3-3-1">3.3.1 The Measure Theory Approach </a></li>
<li><a href="#sec-3-3-2">3.3.2 Relative Frequency Approach </a></li>
<li><a href="#sec-3-3-3">3.3.3 The Subjective Approach </a></li>
<li><a href="#sec-3-3-4">3.3.4 Equally Likely Model (ELM) </a></li>
<li><a href="#sec-3-3-5">3.3.5 Words of Warning </a></li>
</ul>
</li>
<li><a href="#sec-3-4">3.4 Properties of Probability </a>
<ul>
<li><a href="#sec-3-4-1">3.4.1 Probability Functions </a></li>
<li><a href="#sec-3-4-2">3.4.2 Properties </a></li>
<li><a href="#sec-3-4-3">3.4.3 Assigning Probabilities </a></li>
</ul>
</li>
<li><a href="#sec-3-5">3.5 Counting Methods </a>
<ul>
<li><a href="#sec-3-5-1">3.5.1 Ordered Samples </a></li>
<li><a href="#sec-3-5-2">3.5.2 Unordered Samples </a></li>
</ul>
</li>
<li><a href="#sec-3-6">3.6 Conditional Probability </a>
<ul>
<li><a href="#sec-3-6-1">3.6.1 Properties and Rules </a></li>
</ul>
</li>
<li><a href="#sec-3-7">3.7 Independent Events </a>
<ul>
<li><a href="#sec-3-7-1">3.7.1 Independent, Repeated Experiments </a></li>
</ul>
</li>
<li><a href="#sec-3-8">3.8 Bayes' Rule </a></li>
<li><a href="#sec-3-9">3.9 Random Variables </a></li>
<li><a href="#sec-3-10">3.10 Chapter Exercises </a></li>
</ul>
</li>
<li><a href="#sec-4">4 Discrete Distributions </a>
<ul>
<li><a href="#sec-4-1">4.1 Discrete Random Variables </a>
<ul>
<li><a href="#sec-4-1-1">4.1.1 Probability Mass Functions </a></li>
<li><a href="#sec-4-1-2">4.1.2 Mean, Variance, and Standard Deviation </a></li>
</ul>
</li>
<li><a href="#sec-4-2">4.2 The Discrete Uniform Distribution </a></li>
<li><a href="#sec-4-3">4.3 The Binomial Distribution </a>
<ul>
<li><a href="#sec-4-3-1">4.3.1 The Binomial Model </a></li>
</ul>
</li>
<li><a href="#sec-4-4">4.4 Expectation and Moment Generating Functions </a>
<ul>
<li><a href="#sec-4-4-1">4.4.1 The Expectation Operator </a></li>
<li><a href="#sec-4-4-2">4.4.2 Moment Generating Functions </a></li>
</ul>
</li>
<li><a href="#sec-4-5">4.5 The Empirical Distribution </a></li>
<li><a href="#sec-4-6">4.6 Other Discrete Distributions </a>
<ul>
<li><a href="#sec-4-6-1">4.6.1 Dependent Bernoulli Trials </a></li>
<li><a href="#sec-4-6-2">4.6.2 Waiting Time Distributions </a></li>
<li><a href="#sec-4-6-3">4.6.3 Arrival Processes </a></li>
</ul>
</li>
<li><a href="#sec-4-7">4.7 Functions of Discrete Random Variables </a></li>
<li><a href="#sec-4-8">4.8 Chapter Exercises </a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> An Introduction to Probability and Statistics </h2>
<div class="outline-text-2" id="text-1">


<p>
\pagenumbering{arabic} 
</p>





<p>
\noindent This chapter has proved to be the hardest to write, by far. The trouble is that there is so much to say &ndash; and so many people have already said it so much better than I could. When I get something I like I will release it here.
</p>
<p>
In the meantime, there is a lot of information already available to a person with an Internet connection. I recommend to start at Wikipedia, which is not a flawless resource but it has the main ideas with links to reputable sources.
</p>
<p>
In my lectures I usually tell stories about Fisher, Galton, Gauss, Laplace, Quetelet, and the Chevalier de Mere.
</p>

</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Probability </h3>
<div class="outline-text-3" id="text-1-1">


<p>
The common folklore is that probability has been around for millennia but did not gain the attention of mathematicians until approximately 1654 when the Chevalier de Mere had a question regarding the fair division of a game's payoff to the two players, if the game had to end prematurely.
</p>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Statistics </h3>
<div class="outline-text-3" id="text-1-2">


<p>
Statistics concerns data; their collection, analysis, and interpretation. In this book we distinguish between two types of statistics: descriptive and inferential. 
</p>
<p>
Descriptive statistics concerns the summarization of data. We have a data set and we would like to describe the data set in multiple ways. Usually this entails calculating numbers from the data, called descriptive measures, such as percentages, sums, averages, and so forth.
</p>
<p>
Inferential statistics does more. There is an inference associated with the data set, a conclusion drawn about the population from which the data originated.
</p>
<p>
I would like to mention that there are two schools of thought of statistics: frequentist and bayesian. The difference between the schools is related to how the two groups interpret the underlying probability (see Section <a href="#sec-Interpreting-Probabilities">Interpreting-Probabilities</a>). The frequentist school gained a lot of ground among statisticians due in large part to the work of Fisher, Neyman, and Pearson in the early twentieth century. That dominance lasted until inexpensive computing power became widely available; nowadays the bayesian school is garnering more attention and at an increasing rate.
</p>
<p>
This book is devoted mostly to the frequentist viewpoint because that is how I was trained, with the conspicuous exception of Sections <a href="#sec-Bayes--Rule">Bayes'-Rule</a> and <a href="#sec-Conditional-Distributions">Conditional-Distributions</a>. I plan to add more bayesian material in later editions of this book.
</p>




</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Data Description </h2>
<div class="outline-text-2" id="text-2">

<p>\label{cha:Describing-Data-Distributions}
</p>
<p>
\noindent In this chapter we introduce the different types of data that a statistician is likely to encounter, and in each subsection we give some examples of how to display the data of that particular type. Once we see how to display data distributions, we next introduce the basic properties of data distributions. We qualitatively explore several data sets. Once that we have intuitive properties of data sets, we next discuss how we may numerically measure and describe those properties with descriptive statistics.
</p>

<ul>
<li>different data types, such as quantitative versus qualitative, nominal versus ordinal, and discrete versus continuous
</li>
<li>basic graphical displays for assorted data types, and some of their (dis)advantages 
</li>
<li>fundamental properties of data distributions, including center, spread, shape, and crazy observations
</li>
<li>methods to describe data (visually/numerically) with respect to the properties, and how the methods differ depending on the data type
</li>
<li>all of the above in the context of grouped data, and in particular, the concept of a factor
</li>
</ul>


</div>

<div id="outline-container-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Types of Data </h3>
<div class="outline-text-3" id="text-2-1">

<p>\label{sec:Types-of-Data}
</p>
<p>
Loosely speaking, a datum is any piece of collected information, and a data set is a collection of data related to each other in some way. We will categorize data into five types and describe each in turn:
</p>
<dl>
<dt>Quantitative</dt><dd>data associated with a measurement of some quantity on an observational unit,
</dd>
<dt>Qualitative</dt><dd>data associated with some quality or property of an observational unit,
</dd>
<dt>Logical</dt><dd>data which represent true or false and play an important role later,
</dd>
<dt>Missing</dt><dd>data which should be there but are not, and
</dd>
<dt>Other types</dt><dd>everything else under the sun.
</dd>
</dl>


<p>
In each subsection we look at some examples of the type in question and introduce methods to display them.
</p>

</div>

<div id="outline-container-2-1-1" class="outline-4">
<h4 id="sec-2-1-1"><span class="section-number-4">2.1.1</span> Quantitative data </h4>
<div class="outline-text-4" id="text-2-1-1">

<p>\label{sub:Quantitative-Data}
</p>
<p>
Quantitative data are any data that measure or are associated with a measurement of the quantity of something. They invariably assume numerical values. Quantitative data can be further subdivided into two categories. 
</p>
<ul>
<li><i>Discrete data</i> take values in a finite or countably infinite set of numbers, that is, all possible values could (at least in principle) be written down in an ordered list. Examples include: counts, number of arrivals, or number of successes. They are often represented by integers, say, 0, 1, 2, <i>etc</i>.
</li>
<li><i>Continuous data</i> take values in an interval of numbers. These are also known as scale data, interval data, or measurement data. Examples include: height, weight, length, time, <i>etc</i>. Continuous data are often characterized by fractions or decimals: 3.82, 7.0001, 4 \(\frac{5}{8}\), <i>etc</i>.
</li>
</ul>


<p>
Note that the distinction between discrete and continuous data is not always clear-cut. Sometimes it is convenient to treat data as if they were continuous, even though strictly speaking they are not continuous. See the examples.
</p>
<p>
<b>Annual Precipitation in US Cities.</b> The vector <code>precip</code>\index{Data sets!precip@\texttt{precip}} contains average amount of rainfall (in inches) for each of 70 cities in the United States and Puerto Rico. Let us take a look at the data:
</p>



<pre class="example">str(precip)

</pre>




<pre class="example">
 Named num [1:70] 67 54.7 7 48.5 14 17.2 20.7 13 43.4 40.2 ...
 - attr(*, "names")= chr [1:70] "Mobile" "Juneau" "Phoenix" "Little Rock" ...
</pre>





<pre class="example">precip[1:4]

</pre>




<pre class="example">
     Mobile      Juneau     Phoenix Little Rock 
       67.0        54.7         7.0        48.5
</pre>


<p>
The output shows that <code>precip</code> is a numeric vector which has been <i>named</i>, that is, each value has a name associated with it (which can be set with the <code>names</code>\index{names@\texttt{names}} function). These are quantitative continuous data.
</p>

<p>
<b>Lengths of Major North American Rivers.</b> The U.S. Geological Survey recorded the lengths (in miles) of several rivers in North America. They are stored in the vector <code>rivers</code>\index{Data sets!rivers@\texttt{rivers}} in the <code>datasets</code> package (which ships with base \textsf{R}). See <code>?rivers</code>. Let us take a look at the data with the <code>str</code>\index{str@\texttt{str}} function.
</p>



<pre class="example">str(rivers)

</pre>




<pre class="example">
 num [1:141] 735 320 325 392 524 ...
</pre>



<p>
The output says that <code>rivers</code> is a numeric vector of length 141, and the first few values are 735, 320, 325, <i>etc</i>. These data are definitely quantitative and it appears that the measurements have been rounded to the nearest mile. Thus, strictly speaking, these are discrete data. But we will find it convenient later to take data like these to be continuous for some of our statistical procedures. 
</p>
<p>
<b>Yearly Numbers of Important Discoveries.</b> The vector <code>discoveries</code>\index{Data sets!discoveries@\texttt{discoveries}} contains numbers of “great” inventions/discoveries in each year from 1860 to 1959, as reported by the 1975 World Almanac. Let us take a look at the data:
</p>



<pre class="example">str(discoveries)

</pre>




<pre class="example">
 Time-Series [1:100] from 1860 to 1959: 5 3 0 2 0 3 2 3 6 1 ...
</pre>



<p>
The output is telling us that <code>discoveries</code> is a <i>time series</i> (see Section <a href="#sub-other-data-types">other-data-types</a> for more) of length 100. The entries are integers, and since they represent counts this is a good example of discrete quantitative data. We will take a closer look in the following sections.
</p>
</div>

</div>

<div id="outline-container-2-1-2" class="outline-4">
<h4 id="sec-2-1-2"><span class="section-number-4">2.1.2</span> Displaying Quantitative Data </h4>
<div class="outline-text-4" id="text-2-1-2">

<p>\label{sub:Displaying-Quantitative-Data}
</p>
<p>
One of the first things to do when confronted by quantitative data (or any data, for that matter) is to make some sort of visual display to gain some insight into the data's structure. There are almost as many display types from which to choose as there are data sets to plot. We describe some of the more popular alternatives. 
</p>
<p>
\label{par:Strip-charts}
</p>
<p>
These can be used for discrete or continuous data, and usually look best when the data set is not too large. Along the horizontal axis is a numerical scale above which the data values are plotted. We can do it in \textsf{R} with a call to the <code>stripchart</code>\index{stripchart@\texttt{stripchart}} function. There are three available methods.
</p>
<dl>
<dt>overplot</dt><dd>plots ties covering each other. This method is good to display only the distinct values assumed by the data set.

</dd>
<dt>jitter</dt><dd>adds some noise to the data in the \(y\) direction in which case the data values are not covered up by ties.

</dd>
<dt>stack</dt><dd>plots repeated values stacked on top of one another. This method is best used for discrete data with a lot of ties; if there are no repeats then this method is identical to overplot.
</dd>
</dl>


<p>
See Figure <a href="#fig-stripcharts">stripcharts</a>, which was produced by the following code.
</p>



<pre class="example">stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number")

</pre>




<p>
The leftmost graph is a strip chart of the <code>precip</code> data. The graph shows tightly clustered values in the middle with some others falling balanced on either side, with perhaps slightly more falling to the left. Later we will call this a symmetric distribution, see Section <a href="#sub-Shape">Shape</a>. The middle graph is of the <code>rivers</code> data, a vector of length 141. There are several repeated values in the rivers data, and if we were to use the overplot method we would lose some of them in the display. This plot shows a what we will later call a right-skewed shape with perhaps some extreme values on the far right of the display. The third graph strip charts <code>discoveries</code> data which are literally a textbook example of a right skewed distribution.
</p>



<pre class="example">par(mfrow = c(3,1)) # 3 plots: 3 rows, 1 column
stripchart(precip, xlab="rainfall")
stripchart(rivers, method="jitter", xlab="length")
stripchart(discoveries, method="stack", xlab="number")
par(mfrow = c(1,1)) # back to normal

</pre>








<p>
The <code>DOTplot</code>\index{DOTplot@\texttt{DOTplot}} function in the <code>UsingR</code>\index{R packages!UsingR@\texttt{UsingR}} package \cite{usingr} is another alternative.
</p>


<p>
These are typically used for continuous data. A histogram is constructed by first deciding on a set of classes, or bins, which partition the real line into a set of boxes into which the data values fall. Then vertical bars are drawn over the bins with height proportional to the number of observations that fell into the bin. 
</p>
<p>
These are one of the most common summary displays, and they are often misidentified as ``Bar Graphs'' (see below.) The scale on the \(y\) axis can be frequency, percentage, or density (relative frequency). The term histogram was coined by Karl Pearson in 1891, see \cite{Miller}.
</p>
<p>
<b>Annual Precipitation in US Cities.</b> We are going to take another look at the <code>precip</code>\index{Data sets!precip@\texttt{precip}} data that we investigated earlier. The strip chart in Figure <a href="#fig-Various-stripchart-methods">Various-stripchart-methods,</a> suggested a loosely balanced distribution; let us now look to see what a histogram says. 
</p>
<p>
There are many ways to plot histograms in \textsf{R}, and one of the easiest is with the <code>hist</code>\index{hist@\texttt{hist}} function. The following code produces the plots in Figure <a href="#fig-histograms">histograms</a>.
</p>



<pre class="example">hist(precip, main = "")
hist(precip, freq = FALSE, main = "")

</pre>




<p>
Notice the argument \texttt{main = ""} which suppresses the main title from being displayed &ndash; it would have said ``Histogram of <code>precip</code>'' otherwise. The plot on the left is a frequency histogram (the default), and the plot on the right is a relative frequency histogram (<code>freq = FALSE</code>). 
</p>







<p>
Please be careful regarding the biggest weakness of histograms: the graph obtained strongly depends on the bins chosen. Choose another set of bins, and you will get a different histogram. Moreover, there are not any definitive criteria by which bins should be defined; the best choice for a given data set is the one which illuminates the data set's underlying structure (if any). Luckily for us there are algorithms to automatically choose bins that are likely to display well, and more often than not the default bins do a good job. This is not always the case, however, and a responsible statistician will investigate many bin choices to test the stability of the display.
</p>
<p>
Recall that the strip chart in Figure <a href="#fig-Various-stripchart-methods">Various-stripchart-methods</a> suggested a relatively balanced shape to the <code>precip</code> data distribution. Watch what happens when we change the bins slightly (with the <code>breaks</code> argument to <code>hist</code>). See Figure <a href="#fig-histograms-bins">histograms-bins</a> which was produced by the following code.
</p>




<pre class="example">hist(precip, breaks = 10, main = "")
hist(precip, breaks = 200, main = "")

</pre>









<p>
The leftmost graph (with <code>breaks = 10</code>) shows that the distribution is not balanced at all. There are two humps: a big one in the middle and a smaller one to the left. Graphs like this often indicate some underlying group structure to the data; we could now investigate whether the cities for which rainfall was measured were similar in some way, with respect to geographic region, for example.
</p>
<p>
The rightmost graph in Figure <a href="#fig-histograms-bins">histograms-bins</a> shows what happens when the number of bins is too large: the histogram is too grainy and hides the rounded appearance of the earlier histograms. If we were to continue increasing the number of bins we would eventually get all observed bins to have exactly one element, which is nothing more than a glorified strip chart.
</p>


<p>
Stemplots have two basic parts: <i>stems</i> and <i>leaves</i>. The final digit of the data values is taken to be a <i>leaf</i>, and the leading digit(s) is (are) taken to be <i>stems</i>. We draw a vertical line, and to the left of the line we list the stems. To the right of the line, we list the leaves beside their corresponding stem. There will typically be several leaves for each stem, in which case the leaves accumulate to the right. It is sometimes necessary to round the data values, especially for larger data sets.
</p>
<p>
\label{exa:-ukdriverdeaths-first}
<code>UKDriverDeaths</code>\index{Data sets!UKDriverDeaths@\texttt{UKDriverDeaths}} is a time series that contains the total car drivers killed or seriously injured in Great Britain monthly from Jan 1969 to Dec 1984. See <code>?UKDriverDeaths</code>. Compulsory seat belt use was introduced on January 31, 1983. We construct a stem and leaf diagram in \textsf{R} with the <code>stem.leaf</code>\index{stem.leaf@\texttt{stem.leaf}} function from the <code>aplpack</code>\index{R packages@\textsf{R} packages!aplpack@\texttt{aplpack}} package\cite{aplpack}.
</p>



<pre class="example">library(aplpack)
stem.leaf(UKDriverDeaths, depth = FALSE)

</pre>





<pre class="example">1 | 2: represents 120
 leaf unit: 10
            n: 192
   10 | 57
   11 | 136678
   12 | 123889
   13 | 0255666888899
   14 | 00001222344444555556667788889
   15 | 0000111112222223444455555566677779
   16 | 01222333444445555555678888889
   17 | 11233344566667799
   18 | 00011235568
   19 | 01234455667799
   20 | 0000113557788899
   21 | 145599
   22 | 013467
   23 | 9
   24 | 7
HI: 2654
</pre>



<p>
The display shows a more or less balanced mound-shaped distribution, with one or maybe two humps, a big one and a smaller one just to its right. Note that the data have been rounded to the tens place so that each datum gets only one leaf to the right of the dividing line.
</p>
<p>
Notice that the \texttt{depth}s\index{depths} have been suppressed. To learn more about this option and many others, see Section <a href="#sec-Exploratory-Data-Analysis">Exploratory-Data-Analysis</a>. Unlike a histogram, the original data values may be recovered from the stemplot display &ndash; modulo the rounding &ndash; that is, starting from the top and working down we can read off the data values 1050, 1070, 1110, 1130, and so forth. 
</p>

<p>
Done with the <code>plot</code>\index{plot@\texttt{plot}} function. These are good for plotting data which are ordered, for example, when the data are measured over time. That is, the first observation was measured at time 1, the second at time 2, <i>etc</i>. It is a two dimensional plot, in which the index (or time) is the \(x\) variable and the measured value is the \(y\) variable. There are several plotting methods for index plots, and we mention two of them:
</p>
<dl>
<dt>spikes</dt><dd>draws a vertical line from the \(x\)-axis to the observation height (\texttt{type = "h"}).
</dd>
<dt>points</dt><dd>plots a simple point at the observation height (=type = "p"=).
</dd>
</dl>

<p><b>Level of Lake Huron 1875-1972.</b> Brockwell and Davis \cite{Brockwell1991} give the annual measurements of the level (in feet) of Lake Huron from 1875&ndash;1972. The data are stored in the time series <code>LakeHuron</code>\index{Data sets!LakeHuron@\texttt{LakeHuron}}. See <code>?LakeHuron</code>. Figure <a href="#fig-indpl-lakehuron">indpl-lakehuron</a> was produced with the following code:
</p>



<pre class="example">plot(LakeHuron, type = "h")
plot(LakeHuron, type = "p")

</pre>




<p>
The plots show an overall decreasing trend to the observations, and there appears to be some seasonal variation that increases over time. 
</p>







<p>
Coming soon.
</p>
</div>

</div>

<div id="outline-container-2-1-3" class="outline-4">
<h4 id="sec-2-1-3"><span class="section-number-4">2.1.3</span> Qualitative Data, Categorical Data, and Factors </h4>
<div class="outline-text-4" id="text-2-1-3">

<p>\label{sub:Qualitative-Data}
</p>
<p>
Qualitative data are simply any type of data that are not numerical, or do not represent numerical quantities. Examples of qualitative variables include a subject's name, gender, race/ethnicity, political party, socioeconomic status, class rank, driver's license number, and social security number (SSN).
</p>
<p>
Please bear in mind that some data <i>look</i> to be quantitative but are <i>not</i>, because they do not represent numerical quantities and do not obey mathematical rules. For example, a person's shoe size is typically written with numbers: 8, or 9, or 12, or \(12\,\frac{1}{2}\). Shoe size is not quantitative, however, because if we take a size 8 and combine with a size 9 we do not get a size 17.
</p>
<p>
Some qualitative data serve merely to <i>identify</i> the observation (such a subject's name, driver's license number, or SSN). This type of data does not usually play much of a role in statistics. But other qualitative variables serve to <i>subdivide</i> the data set into categories; we call these <i>factors</i>. In the above examples, gender, race, political party, and socioeconomic status would be considered factors (shoe size would be another one). The possible values of a factor are called its <i>levels</i>. For instance, the factor <i>gender</i> would have two levels, namely, male and female. Socioeconomic status typically has three levels: high, middle, and low.
</p>
<p>
Factors may be of two types: <i>nominal</i>\index{nominal data} and <i>ordinal</i>\index{ordinal data}. Nominal factors have levels that correspond to names of the categories, with no implied ordering. Examples of nominal factors would be hair color, gender, race, or political party. There is no natural ordering to ``Democrat'' and ``Republican''; the categories are just names associated with different groups of people. 
</p>
<p>
In contrast, ordinal factors have some sort of ordered structure to the underlying factor levels. For instance, socioeconomic status would be an ordinal categorical variable because the levels correspond to ranks associated with income, education, and occupation. Another example of ordinal categorical data would be class rank.
</p>
<p>
Factors have special status in \textsf{R}. They are represented internally by numbers, but even when they are written numerically their values do not convey any numeric meaning or obey any mathematical rules (that is, Stage III cancer is not Stage I cancer + Stage II cancer).
</p>
<p>
The <code>state.abb</code>\index{Data sets!state.abb@\texttt{state.abb}}
vector gives the two letter postal abbreviations for all 50 states.
</p>



<pre class="example">str(state.abb)

</pre>




<pre class="example">
 chr [1:50] "AL" "AK" "AZ" "AR" "CA" "CO" "CT" "DE" ...
</pre>


<p>
These would be ID data. The <code>state.name</code>\index{Data sets!state.name@\texttt{state.name}} vector lists all of the complete names and those data would also be ID.
</p>
<p>
<b>U.S.~State Facts and Features.</b> The U.S. Department of Commerce of the U.S. Census Bureau releases all sorts of information in the <i>Statistical Abstract of the United States</i>, and the <code>state.region</code>\index{Data sets!state.region@\texttt{state.region}} data lists each of the 50 states and the region to which it belongs, be it Northeast, South, North Central, or West. See <code>?state.region</code>.
</p>



<pre class="example">str(state.region)
state.region[1:5]

</pre>




<pre class="example">
 Factor w/ 4 levels "Northeast","South",..: 2 4 4 2 4 4 1 2 2 2 ...
[1] South West  West  South West 
Levels: Northeast South North Central West
</pre>


<p>
The <code>str</code>\index{str@\texttt{str}} output shows that <code>state.region</code> is already stored internally as a factor and it lists a couple of the factor levels. To see all of the levels we printed the first five entries of the vector in the second line.
</p>
</div>

</div>

<div id="outline-container-2-1-4" class="outline-4">
<h4 id="sec-2-1-4"><span class="section-number-4">2.1.4</span> Displaying Qualitative Data </h4>
<div class="outline-text-4" id="text-2-1-4">

<p>\label{sub:Displaying-Qualitative-Data}
</p>
<p>
\label{par:Tables}
</p>
<p>
One of the best ways to summarize qualitative data is with a table of the data values. We may count frequencies with the <code>table</code> function or list proportions with the <code>prop.table</code>\index{prop.table@\texttt{prop.table}} function (whose input is a frequency table). In the \textsf{R} Commander you can do it with \textsf{Statistics} \textsf{$\triangleright$} \textsf{Frequency Distribution}&hellip; Alternatively, to look at tables for all factors in the <code>Active data set</code>\index{Active data set@\texttt{Active data set}} you can do \textsf{Statistics} \textsf{$\triangleright$} \textsf{Summaries} \textsf{$\triangleright$} \textsf{Active Dataset}.
</p>



<pre class="example">Tbl &lt;- table(state.division)

</pre>






<pre class="example">Tbl

</pre>




<pre class="example">
state.division
       New England    Middle Atlantic     South Atlantic 
                 6                  3                  8 
East South Central West South Central East North Central 
                 4                  4                  5 
West North Central           Mountain            Pacific 
                 7                  8                  5
</pre>





<pre class="example">Tbl/sum(Tbl)      # relative frequencies

</pre>




<pre class="example">
state.division
       New England    Middle Atlantic     South Atlantic 
              0.12               0.06               0.16 
East South Central West South Central East North Central 
              0.08               0.08               0.10 
West North Central           Mountain            Pacific 
              0.14               0.16               0.10
</pre>





<pre class="example">prop.table(Tbl)   # same thing

</pre>




<pre class="example">
state.division
       New England    Middle Atlantic     South Atlantic 
              0.12               0.06               0.16 
East South Central West South Central East North Central 
              0.08               0.08               0.10 
West North Central           Mountain            Pacific 
              0.14               0.16               0.10
</pre>



<p>
\label{par:Bar-Graphs}
</p>
<p>
A bar graph is the analogue of a histogram for categorical data. A bar is displayed for each level of a factor, with the heights of the bars proportional to the frequencies of observations falling in the respective categories. A disadvantage of bar graphs is that the levels are ordered alphabetically (by default), which may sometimes obscure patterns in the display. 
</p>
<p>
<b>U.S.~State Facts and Features.</b> The <code>state.region</code> data lists each of the 50 states and the region to which it belongs, be it Northeast, South, North Central, or West. See <code>?state.region</code>. It is already stored internally as a factor. We make a bar graph with the <code>barplot</code>\index{barplot@\texttt{barplot}} function: 
</p>




<pre class="example">barplot(table(state.region), cex.names = 0.50)
barplot(prop.table(table(state.region)), cex.names = 0.50)

</pre>




<p>
See Figure <a href="#fig-bar-gr-stateregion">bar-gr-stateregion</a>. The display on the left is a frequency bar graph because the \(y\) axis shows counts, while the display on the left is a relative frequency bar graph. The only difference between the two is the scale. Looking at the graph we see that the majority of the fifty states are in the South, followed by West, North Central, and finally Northeast. Over 30\% of the states are in the South.
</p>
<p>
Notice the <code>cex.names</code>\index{cex.names@\texttt{cex.names}} argument that we used, above. It shrinks the names on the \(x\) axis by 50% which makes them easier to read. See <code>?par</code>\index{par@\texttt{par}} for a detailed list of additional plot parameters.
</p>









<p>
\label{par:Pareto-Diagrams}
</p>
<p>
A pareto diagram is a lot like a bar graph except the bars are rearranged such that they decrease in height going from left to right. The rearrangement is handy because it can visually reveal structure (if any) in how fast the bars decrease &ndash; this is much more difficult when the bars are jumbled. 
</p>
<p>
<b>U.S. State Facts and Features.</b> The <code>state.division</code>\index{Data sets!state.division@\texttt{state.division}} data record the division (New England, Middle Atlantic, South Atlantic, East South Central, West South Central, East North Central, West North Central, Mountain, and Pacific) of the fifty states. We can make a pareto diagram with either the <code>RcmdrPlugin.IPSUR</code>\index{R packages@\textsf{R} packages!RcmdrPlugin.IPSUR@\texttt{RcmdrPlugin.IPSUR}} package or with the <code>pareto.chart</code>\index{pareto.chart@\texttt{pareto.chart}} function from the <code>qcc</code>\index{R packages@\textsf{R} packages!qcc@\texttt{qcc}} package \cite{qcc}. See Figure <a href="#fig-Pareto-chart">Pareto-chart</a>. The code follows.
</p>




<pre class="example">library(qcc)
pareto.chart(table(state.division), ylab="Frequency")

</pre>










<p>
\label{par:Dotcharts}
</p>
<p>
These are a lot like a bar graph that has been turned on its side with the bars replaced by dots on horizontal lines. They do not convey any more (or less) information than the associated bar graph, but the strength lies in the economy of the display. Dot charts are so compact that it is easy to graph very complicated multi-variable interactions together in one graph. See Section <a href="#sec-Comparing-Data-Sets">Comparing-Data-Sets</a>. We will give an example here using the same data as above for comparison. The graph was produced by the following code.
</p>



<pre class="example">x &lt;- table(state.region)
dotchart(as.vector(x), labels = names(x))

</pre>







<p>
See Figure <a href="#fig-dot-charts">dot-charts</a>. Compare it to Figure <a href="#fig-bar-gr-stateregion">bar-gr-stateregion</a>.
</p>

<p>
\label{par:Pie-Graphs}
</p>
<p>
These can be done with \textsf{R} and the \textsf{R} Commander, but they fallen out of favor in recent years because researchers have determined that while the human eye is good at judging linear measures, it is notoriously bad at judging relative areas (such as those displayed by a pie graph). Pie charts are consequently a very bad way of displaying information. A bar chart or dot chart is a preferable way of displaying qualitative data. See <code>?pie</code>\index{pie@\texttt{pie}} for more information.
</p>
<p>
We are not going to do any examples of a pie graph and discourage their use elsewhere. 
</p>
</div>

</div>

<div id="outline-container-2-1-5" class="outline-4">
<h4 id="sec-2-1-5"><span class="section-number-4">2.1.5</span> Logical Data </h4>
<div class="outline-text-4" id="text-2-1-5">

<p>\label{sub:Logical-Data}
</p>
<p>
There is another type of information recognized by \textsf{R} which does not fall into the above categories. The value is either <code>TRUE</code> or <code>FALSE</code> (note that equivalently you can use <code>1 = TRUE</code>, <code>0 = FALSE</code>). Here is an example of a logical vector:
</p>



<pre class="example">x &lt;- 5:9
y &lt;- (x &lt; 7.3)
y

</pre>




<pre class="example">
[1]  TRUE  TRUE  TRUE FALSE FALSE
</pre>


<p>
Many functions in \textsf{R} have options that the user may or may not want to activate in the function call. For example, the <code>stem.leaf</code> function has the <code>depths</code> argument which is <code>TRUE</code> by default. We saw in Section <a href="#sub-Quantitative-Data">Quantitative-Data</a> how to turn the option off, simply enter <code>stem.leaf(x, depths = FALSE)</code> and they will not be shown on the display.
</p>
<p>
We can swap <code>TRUE</code> with <code>FALSE</code> with the exclamation point <code>!</code>.
</p>



<pre class="example">!y

</pre>




<pre class="example">
[1] FALSE FALSE FALSE  TRUE  TRUE
</pre>


</div>

</div>

<div id="outline-container-2-1-6" class="outline-4">
<h4 id="sec-2-1-6"><span class="section-number-4">2.1.6</span> Missing Data </h4>
<div class="outline-text-4" id="text-2-1-6">

<p>\label{sub:Missing-Data}
</p>
<p>
Missing data are a persistent and prevalent problem in many statistical analyses, especially those associated with the social sciences. \textsf{R} reserves the special symbol <code>NA</code> to representing missing data.
</p>
<p>
Ordinary arithmetic with <code>NA</code> values give <code>NA</code>'s (addition, subtraction, <i>etc</i>.) and applying a function to a vector that has an <code>NA</code> in it will usually give an <code>NA</code>.
</p>



<pre class="example">x &lt;- c(3, 7, NA, 4, 7)
y &lt;- c(5, NA, 1, 2, 2)
x + y

</pre>




<pre class="example">
[1]  8 NA NA  6  9
</pre>


<p>
Some functions have a <code>na.rm</code> argument which when <code>TRUE</code> will ignore missing data as if they were not there (such as <code>mean</code>, <code>var</code>, <code>sd</code>, <code>IQR</code>, <code>mad</code>, &hellip;). 
</p>



<pre class="example">sum(x)
sum(x, na.rm = TRUE)

</pre>




<pre class="example">
[1] NA
[1] 21
</pre>


<p>
Other functions do not have a <code>na.rm</code> argument and will return <code>NA</code> or an error if the argument has \texttt{NA}s. In those cases we can find the locations of any \texttt{NA}s with the <code>is.na</code> function and remove those cases with the <code>[]</code> operator.
</p>



<pre class="example">is.na(x)
z &lt;- x[!is.na(x)]
sum(z)

</pre>




<pre class="example">
[1] FALSE FALSE  TRUE FALSE FALSE
[1] 21
</pre>


<p>
The analogue of <code>is.na</code> for rectangular data sets (or data frames) is the <code>complete.cases</code> function. See Appendix <a href="#sec-Editing-Data-Sets">Editing-Data-Sets</a>.
</p>
</div>

</div>

<div id="outline-container-2-1-7" class="outline-4">
<h4 id="sec-2-1-7"><span class="section-number-4">2.1.7</span> Other Data Types </h4>
<div class="outline-text-4" id="text-2-1-7">

<p>\label{sub:other-data-types}
</p>

</div>
</div>

</div>

<div id="outline-container-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Features of Data Distributions </h3>
<div class="outline-text-3" id="text-2-2">

<p>\label{sec:features-of-data}
</p>
<p>
Given that the data have been appropriately displayed, the next step is to try to identify salient features represented in the graph. The acronym to remember is \emph{C}enter, \emph{U}nusual features, \emph{S}pread, and \emph{S}hape. (CUSS).
</p>

</div>

<div id="outline-container-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Center </h4>
<div class="outline-text-4" id="text-2-2-1">

<p>\label{sub:Center}
</p>
<p>
One of the most basic features of a data set is its center. Loosely speaking, the center of a data set is associated with a number that represents a middle or general tendency of the data. Of course, there are usually several values that would serve as a center, and our later tasks will be focused on choosing an appropriate one for the data at hand. Judging from the histogram that we saw in Figure <a href="#fig-histograms-bins">histograms-bins</a>, a measure of center would be about  [1] 35. 
</p>
</div>

</div>

<div id="outline-container-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> Spread </h4>
<div class="outline-text-4" id="text-2-2-2">

<p>\label{sub:Spread}
</p>
<p>
The spread of a data set is associated with its variability; data sets with a large spread tend to cover a large interval of values, while data sets with small spread tend to cluster tightly around a central value. 
</p>
</div>

</div>

<div id="outline-container-2-2-3" class="outline-4">
<h4 id="sec-2-2-3"><span class="section-number-4">2.2.3</span> Shape </h4>
<div class="outline-text-4" id="text-2-2-3">

<p>\label{sub:Shape}
</p>
<p>
When we speak of the <i>shape</i> of a data set, we are usually referring to the shape exhibited by an associated graphical display, such as a histogram. The shape can tell us a lot about any underlying structure to the data, and can help us decide which statistical procedure we should use to analyze them.
</p>

<p>
A distribution is said to be <i>right-skewed</i> (or <i>positively skewed</i>) if the right tail seems to be stretched from the center. A <i>left-skewed</i> (or <i>negatively skewed</i>) distribution is stretched to the left side. A symmetric distribution has a graph that is balanced about its center, in the sense that half of the graph may be reflected about a central line of symmetry to match the other
half.
</p>
<p>
We have already encountered skewed distributions: both the discoveries data in Figure <a href="#fig-stripcharts">stripcharts</a> and the <code>precip</code> data in Figure <a href="#fig-histograms-bins">histograms-bins</a> appear right-skewed. The <code>UKDriverDeaths</code> data in Example <a href="#exa--ukdriverdeaths-first">-ukdriverdeaths-first</a> is relatively symmetric (but note the one extreme value 2654 identified at the bottom of the stemplot).
</p>

<p>
Another component to the shape of a distribution is how ``peaked'' it is. Some distributions tend to have a flat shape with thin tails. These are called <i>platykurtic</i>, and an example of a platykurtic distribution is the uniform distribution; see Section <a href="#sec-The-Continuous-Uniform">The-Continuous-Uniform</a>. On the other end of the spectrum are distributions with a steep peak, or spike, accompanied by heavy tails; these are called <i>leptokurtic</i>. Examples of leptokurtic distributions are the Laplace distribution and the logistic distribution. See Section <a href="#sec-Other-Continuous-Distributions">Other-Continuous-Distributions</a>. In between are distributions (called <i>mesokurtic</i>) with a rounded peak and moderately sized tails. The standard example of a mesokurtic distribution is the famous bell-shaped curve, also known as the Gaussian, or normal, distribution, and the binomial distribution can be mesokurtic for specific choices of \(p\). See Sections <a href="#sec-binom-dist">binom-dist</a> and <a href="#sec-The-Normal-Distribution">The-Normal-Distribution</a>.
</p>
</div>

</div>

<div id="outline-container-2-2-4" class="outline-4">
<h4 id="sec-2-2-4"><span class="section-number-4">2.2.4</span> Clusters and Gaps </h4>
<div class="outline-text-4" id="text-2-2-4">

<p>\label{sub:clusters-and-gaps}
</p>
<p>
Clusters or gaps are sometimes observed in quantitative data distributions. They indicate clumping of the data about distinct values, and gaps may exist between clusters. Clusters often suggest an underlying grouping to the data. For example, take a look at the <code>faithful</code> data which contains the duration of <code>eruptions</code> and the <code>waiting</code> time between eruptions of the Old Faithful geyser in Yellowstone National Park. (Do not be frightened by the complicated information at the left of the display for now; we will learn how to interpret it in Section <a href="#sec-Exploratory-Data-Analysis">Exploratory-Data-Analysis</a>).\label{exa:stemleaf-multiple-lines-stem}
</p>



<pre class="example">library(aplpack)
stem.leaf(faithful$eruptions)

</pre>


<p>
n<sub>example</sub>
1 | 2: represents 1.2
 leaf unit: 0.1
            n: 272
   12     s | 667777777777
   51    1. | 888888888888888888888888888899999999999
   71    2* | 00000000000011111111
   87     t | 2222222222333333
   92     f | 44444
   94     s | 66
   97    2. | 889
   98    3* | 0
  102     t | 3333
  108     f | 445555
  118     s | 6666677777
  (16)   3. | 8888888889999999
  138    4* | 0000000000000000111111111111111
  107     t | 22222222222233333333333333333
   78     f | 44444444444445555555555555555555555
   43     s | 6666666666677777777777
   21    4. | 88888888888899999
    4    5* | 0001
</p>
<p>
There are definitely two clusters of data here; an upper cluster and a lower cluster. 
</p>
</div>

</div>

<div id="outline-container-2-2-5" class="outline-4">
<h4 id="sec-2-2-5"><span class="section-number-4">2.2.5</span> Extreme Observations and other Unusual Features </h4>
<div class="outline-text-4" id="text-2-2-5">

<p>\label{sub:Extreme-Observations-and}
</p>
<p>
Extreme observations fall far from the rest of the data. Such observations are troublesome to many statistical procedures; they cause exaggerated estimates and instability. It is important to identify extreme observations and examine the source of the data more closely. There are many possible reasons underlying an extreme observation:
</p>
<ul>
<li><b>Maybe the value is a typographical error.</b> Especially with large data sets becoming more prevalent, many of which being recorded by hand, mistakes are a common problem. After closer scrutiny, these can often be fixed.

</li>
<li><b>Maybe the observation was not meant for the study</b>, because it does not belong to the population of interest. For example, in medical research some subjects may have relevant complications in their genealogical history that would rule out their participation in the experiment. Or when a manufacturing company investigates the properties of one of its devices, perhaps a particular product is malfunctioning and is not representative of the majority of the items.

</li>
<li><b>Maybe it indicates a deeper trend or phenomenon.</b> Many of the most influential scientific discoveries were made when the investigator noticed an unexpected result, a value that was not predicted by the classical theory. Albert Einstein, Louis Pasteur, and others built their careers on exactly this circumstance.
</li>
</ul>

</div>
</div>

</div>

<div id="outline-container-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Descriptive Statistics </h3>
<div class="outline-text-3" id="text-2-3">

<p>\label{sec:Descriptive-Statistics}
</p>

</div>

<div id="outline-container-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> Frequencies and Relative Frequencies </h4>
<div class="outline-text-4" id="text-2-3-1">

<p>\label{sub:Frequencies-and-Relative}
</p>
<p>
These are used for categorical data. The idea is that there are a number of different categories, and we would like to get some idea about how the categories are represented in the population. 
</p>
</div>

</div>

<div id="outline-container-2-3-2" class="outline-4">
<h4 id="sec-2-3-2"><span class="section-number-4">2.3.2</span> Measures of Center </h4>
<div class="outline-text-4" id="text-2-3-2">

<p>\label{sub:Measures-of-Center}
</p>
<p>
The <i>sample mean</i> is denoted \(\xbar\) (read ``\(x\)-bar'') and is simply the arithmetic average of the observations:
</p>


\begin{equation} 
\xbar=\frac{x_{1}+x_{2}+\cdots+x_{n}}{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation}

<ul>
<li>Good: natural, easy to compute, has nice mathematical properties
</li>
<li>Bad: sensitive to extreme values
</li>
</ul>


<p>
It is appropriate for use with data sets that are not highly skewed without extreme observations.
</p>
<p>
The <i>sample median</i> is another popular measure of center and is denoted \(\tilde{x}\). To calculate its value, first sort the data into an increasing sequence of numbers. If the data set has an odd number of observations then \(\tilde{x}\) is the value of the middle observation, which lies in position \((n+1)/2\); otherwise, there are two middle observations and \(\tilde{x}\) is the average of those middle values.
</p>
<ul>
<li>Good: resistant to extreme values, easy to describe
</li>
<li>Bad: not as mathematically tractable, need to sort the data to calculate
</li>
</ul>


<p>
One desirable property of the sample median is that it is <i>resistant</i> to extreme observations, in the sense that the value of \(\tilde{x}\) depends only on those data values in the middle, and is quite unaffected by the actual values of the outer observations in the ordered list. The same cannot be said for the sample mean. Any significant changes in the magnitude of an observation \(x_{k}\) results in a corresponding change in the value of the mean. Consequently, the sample mean is said to be <i>sensitive</i> to extreme observations.
</p>
<p>
The <i>trimmed mean</i> is a measure designed to address the sensitivity of the sample mean to extreme observations. The idea is to ``trim'' a fraction (less than 1/2) of the observations off each end of the ordered list, and then calculate the sample mean of what remains. We will denote it by \(\xbar_{t=0.05}\).
</p>
<ul>
<li>Good: resistant to extreme values, shares nice statistical properties
</li>
<li>Bad: need to sort the data
</li>
</ul>


<ul>
<li>You can calculate frequencies or relative frequencies with the <code>table</code> function, and relative frequencies with <code>prop.table(table())</code>. \item You can calculate the sample mean of a data vector <code>x</code> with the command <code>mean(x)</code>.

</li>
<li>You can calculate the sample median of <code>x</code> with the command <code>median(x)</code>. \item You can calculate the trimmed mean with the <code>trim</code> argument; <code>mean(x, trim = 0.05)</code>.
</li>
</ul>


</div>

</div>

<div id="outline-container-2-3-3" class="outline-4">
<h4 id="sec-2-3-3"><span class="section-number-4">2.3.3</span> Order Statistics and the Sample Quantiles </h4>
<div class="outline-text-4" id="text-2-3-3">

<p>\label{sub:Order-Statistics-and}
</p>
<p>
A common first step in an analysis of a data set is to sort the values. Given a data set \(x_{1}\), \(x_{2}\), &hellip;, \(x_{n}\), we may sort the values to obtain an increasing sequence
</p>


\begin{equation} 
x_{(1)}\leq x_{(2)}\leq x_{(3)}\leq\cdots\leq x_{(n)}
\end{equation}

<p>
and the resulting values are called the <i>order statistics</i>. The \(k^{\mathrm{th}}\) entry in the list, \(x_{(k)}\), is the \(k^{\mathrm{th}}\) order statistic, and approximately $100(k/n)$% of the observations fall below \(x_{(k)}\). The order statistics give an indication of the shape of the data distribution, in the sense that a person can look at the order statistics and have an idea about where the data are concentrated, and where they are sparse.
</p>
<p>
The <i>sample quantiles</i> are related to the order statistics. Unfortunately, there is not a universally accepted definition of them. Indeed, \textsf{R} is equipped to calculate quantiles using nine distinct definitions! We will describe the default method (<code>type = 7</code>), but the interested reader can see the details for the other methods with <code>?quantile</code>.
</p>
<p>
Suppose the data set has \(n\) observations. Find the sample quantile of order \(p\) (\(0&lt;p&lt;1\)), denoted \(\tilde{q}_{p}\) , as follows: 
</p>
<dl>
<dt>First step:</dt><dd>sort the data to obtain the order statistics \(x_{(1)}\), \(x_{(2)}\), &hellip;,\(x_{(n)}\). 
</dd>
<dt>Second step:</dt><dd>calculate \((n-1)p+1\) and write it in the form \(k.d\), where \(k\) is an integer and \(d\) is a decimal.
</dd>
<dt>Third step:</dt><dd>The sample quantile \(\tilde{q}_{p}\) is
   \begin{equation}
      \tilde{q}_{p}=x_{(k)}+d(x_{(k+1)}-x_{(k)}).
   \end{equation}
</dd>
</dl>

<p>The interpretation of \(\tilde{q}_{p}\) is that approximately $100p$\% of the data fall below the value \(\tilde{q}_{p}\) . 
</p>
<p>
Keep in mind that there is not a unique definition of percentiles, quartiles, <i>etc</i>. Open a different book, and you'll find a different definition. The difference is small and seldom plays a role except in small data sets with repeated values. In fact, most people do not even notice in common use.
</p>
<p>
Clearly, the most popular sample quantile is \(\tilde{q}_{0.50}\), also known as the sample median, \(\tilde{x}\). The closest runners-up are the <i>first quartile</i> \(\tilde{q}_{0.25}\) and the <i>third quartile</i> \(\tilde{q}_{0.75}\) (the <i>second quartile</i> is the median). 
</p>


<p>
we can find the order statistics of a data set stored in a vector <code>x</code> with the command <code>sort(x)</code>.
</p>
<p>
We can calculate the sample quantiles of any order \(p\) where \(0&lt;p&lt;1\) for a data set stored in a data vector <code>x</code> with the <code>quantile</code> function, for instance, the command <code>quantile(x, probs = c(0, 0.25, 0.37))</code> will return the smallest observation, the first quartile, \(\tilde{q}_{0.25}\), and the 37th sample quantile, \(\tilde{q}_{0.37}\). For \(\tilde{q}_{p}\) simply change the values in the <code>probs</code> argument to the value \(p\).
</p>

<p>
we can find the order statistics of a variable in the <code>Active data set</code> by doing \textsf{Data \(\triangleright\) Manage variables in Active data set &hellip; \(\triangleright\) Compute new variable}&hellip;. In the \textsf{Expression to compute} dialog simply type <code>sort(varname)</code>, where <code>varname</code> is the variable that it is desired to sort.
</p>
<p>
In <code>Rcmdr</code>, we can calculate the sample quantiles for a particular variable with the sequence \textsf{Statistics \(\triangleright\) Summaries \(\triangleright\) Numerical Summaries}&hellip; We can automatically calculate the quartiles for all variables in the <code>Active data set</code> with the sequence \textsf{Statistics \(\triangleright\) Summaries \(\triangleright\) Active Dataset}.
</p>

</div>

</div>

<div id="outline-container-2-3-4" class="outline-4">
<h4 id="sec-2-3-4"><span class="section-number-4">2.3.4</span> Measures of Spread </h4>
<div class="outline-text-4" id="text-2-3-4">

<p>\label{sub:Measures-of-Spread}
</p>

<p>
The <i>sample variance</i> is denoted \(s^{2}\) and is calculated with the formula
</p>


\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\xbar)^{2}.
\end{equation}

<p>
The <i>sample standard deviation</i> is \(s=\sqrt{s^{2}}\). Intuitively, the sample variance is approximately the average squared distance of the observations from the sample mean. The sample standard deviation is used to scale the estimate back to the measurement units of the original data.
</p>
<ul>
<li>Good: tractable, has nice mathematical/statistical properties
</li>
<li>Bad: sensitive to extreme values
</li>
</ul>


<p>
We will spend a lot of time with the variance and standard deviation in the coming chapters. In the meantime, the following two rules give some meaning to the standard deviation, in that there are bounds on how much of the data can fall past a certain distance from the mean.
</p>
<p>
Chebychev's Rule: The proportion of observations within \(k\) standard deviations of the mean is at least \(1-1/k^{2}\), <i>i.e.</i>, at least 75\%, 89\%, and 94\% of the data are within 2, 3, and 4 standard deviations of the mean, respectively.
</p>
<p>
Note that Chebychev's Rule does not say anything about when \(k=1\), because \(1-1/1^{2}=0\), which states that at least 0\% of the observations are within one standard deviation of the mean (which is not saying much).
</p>
<p>
Chebychev's Rule applies to any data distribution, <i>any</i> list of numbers, no matter where it came from or what the histogram looks like. The price for such generality is that the bounds are not very tight; if we know more about how the data are shaped then we can say more about how much of the data can fall a given distance from the mean.
</p>
<p>
\label{fac:Empirical-Rule}Empirical Rule: If data follow a bell-shaped
curve, then approximately 68\%, 95\%, and 99.7\% of the data are within
1, 2, and 3 standard deviations of the mean, respectively. 
</p>


<p>
Just as the sample mean is sensitive to extreme values, so the associated measure of spread is similarly sensitive to extremes. Further, the problem is exacerbated by the fact that the extreme distances are squared. We know that the sample quartiles are resistant to extremes, and a measure of spread associated with them is the <i>interquartile range</i> (\(IQR\)) defined by \(IQR=q_{0.75}-q_{0.25}\).
</p>
<ul>
<li>Good: stable, resistant to outliers, robust to nonnormality, easy to explain
</li>
<li>Bad: not as tractable, need to sort the data, only involves the middle 50\% of the data.
</li>
</ul>



<p>
A measure even more robust than the \(IQR\) is the <i>median absolute deviation</i> (\(MAD\)). To calculate it we first get the median \(\widetilde{x}\), next the <i>absolute deviations</i> \(|x_{1}-\tilde{x}|\), \(|x_{2}-\tilde{x}|\), &hellip;, \(|x_{n}-\tilde{x}|\), and the \(MAD\) is proportional to the median of those deviations:
</p>


\begin{equation}
MAD\propto\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|).
\end{equation}

<p>
That is, the \(MAD=c\cdot\mbox{median}(|x_{1}-\tilde{x}|,\ |x_{2}-\tilde{x}|,\ldots,|x_{n}-\tilde{x}|)\), where \(c\) is a constant chosen so that the \(MAD\) has nice properties. The value of \(c\) in \textsf{R} is by default \(c=1.4286\). This value is chosen to ensure that the estimator of \(\sigma\) is correct, on the average, under suitable sampling assumptions (see Section <a href="#sec-Point-Estimation-1">Point-Estimation-1</a>).
</p>
<ul>
<li>Good: stable, very robust, even more so than the \(IQR\).
</li>
<li>Bad: not tractable, not well known and less easy to explain.
</li>
</ul>


<p>
We have seen three different measures of spread which, for a given data set, will give three different answers. Which one should we use? It depends on the data set. If the data are well behaved, with an approximate bell-shaped distribution, then the sample mean and sample standard deviation are natural choices with nice mathematical properties. However, if the data have an unusual or skewed shape with several extreme values, perhaps the more resistant choices among the \(IQR\) or \(MAD\) would be more appropriate.
</p>
<p>
However, once we are looking at the three numbers it is important to understand that the estimators are not all measuring the same quantity, on the average. In particular, it can be shown that when the data follow an approximately bell-shaped distribution, then on the average, the sample standard deviation \(s\) and the \(MAD\) will be the approximately the same value, namely, \(\sigma\), but the \(IQR\) will be on the average 1.349 times larger than \(s\) and the \(MAD\). See <a href="#cha-Sampling-Distributions">Sampling-Distributions</a> for more details.
</p>


<p>
we may compute the sample range with <code>range(x)</code> and the sample variance with <code>var(x)</code>, where <code>x</code> is a numeric vector. The sample standard deviation is <code>sqrt(var(x))</code> or just <code>sd(x)</code>. The \(IQR\) is <code>IQR(x)</code> and the median absolute deviation is <code>mad(x)</code>.
</p>
<p>
we can calculate the sample standard deviation with the \textsf{Statistics \(\triangleright\) Summaries \(\triangleright\) Numerical Summaries}&hellip; combination. \textsf{R} Commander does not calculate the \(IQR\) or \(MAD\) in any of the menu selections, by default.
</p>
</div>

</div>

<div id="outline-container-2-3-5" class="outline-4">
<h4 id="sec-2-3-5"><span class="section-number-4">2.3.5</span> Measures of Shape \label{sub:Measures-of-Shape} </h4>
<div class="outline-text-4" id="text-2-3-5">



<p>
The <i>sample skewness</i>, denoted by \(g_{1}\), is defined by the formula
</p>


\begin{equation}
g_{1}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\xbar)^{3}}{s^{3}}.
\end{equation}

<p>
The sample skewness can be any value \(-\infty&lt;g_{1}&lt;\infty\). The sign of \(g_{1}\) indicates the direction of skewness of the distribution. Samples that have \(g_{1}&gt;0\) indicate right-skewed distributions (or positively skewed), and samples with \(g_{1}&lt;0\) indicate left-skewed distributions (or negatively skewed). Values of \(g_{1}\) near zero indicate a symmetric distribution. These are not hard and fast rules, however. The value of \(g_{1}\) is subject to sampling variability and thus only provides a suggestion to the skewness of the underlying distribution. 
</p>
<p>
We still need to know how big is ``big'', that is, how do we judge whether an observed value of \(g_{1}\) is far enough away from zero for the data set to be considered skewed to the right or left? A good rule of thumb is that data sets with skewness larger than \(2\sqrt{6/n}\) in magnitude are substantially skewed, in the direction of the sign of \(g_{1}\). See Tabachnick \&amp; Fidell \cite{Tabachnick2006} for details.
</p>

<p>
The <i>sample excess kurtosis</i>, denoted by \(g_{2}\), is given by the formula
</p>


\begin{equation}
g_{2}=\frac{1}{n}\frac{\sum_{i=1}^{n}(x_{i}-\xbar)^{4}}{s^{4}}-3.
\end{equation}

<p>
The sample excess kurtosis takes values \(-2\leq g_{2}&lt;\infty\). The subtraction of 3 may seem mysterious but it is done so that mound shaped samples have values of \(g_{2}\) near zero. Samples with \(g_{2}&gt;0\) are called <i>leptokurtic</i>, and samples with \(g_{2}&lt;0\) are called <i>platykurtic</i>. Samples with \(g_{2}\approx0\) are called <i>mesokurtic</i>.
</p>
<p>
As a rule of thumb, if \(|g_{2}|&gt;4\sqrt{6/n}\) then the sample excess kurtosis is substantially different from zero in the direction of the sign of \(g_{2}\). See Tabachnick \&amp; Fidell \cite{Tabachnick2006} for details.
</p>
<p>
Notice that both the sample skewness and the sample kurtosis are invariant with respect to location and scale, that is, the values of \(g_{1}\) and \(g_{2}\) do not depend on the measurement units of the data. 
</p>

<p>
The <code>e1071</code> package \cite{Dimitriadoue1071} has the <code>skewness</code> function for the sample skewness and the <code>kurtosis</code> function for the sample excess kurtosis. Both functions have a <code>na.rm</code> argument which is <code>FALSE</code> by default.
</p>
<p>
We said earlier that the <code>discoveries</code> data looked positively skewed; let's see what the statistics say:
</p>



<pre class="example">library(e1071)
skewness(discoveries)
2*sqrt(6/length(discoveries))

</pre>




<pre class="example">
[1] 0.4898979
</pre>


<p>
The data are definitely skewed to the right. Let us check the sample excess kurtosis of the <code>UKDriverDeaths</code> data:
</p>



<pre class="example">kurtosis(UKDriverDeaths)
4*sqrt(6/length(UKDriverDeaths))

</pre>




<pre class="example">
[1] 0.1382454
[1] 0.7071068
</pre>


<p>
so that the <code>UKDriverDeaths</code> data appear to be mesokurtic, or at least not substantially leptokurtic.
</p>
</div>
</div>

</div>

<div id="outline-container-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> Exploratory Data Analysis </h3>
<div class="outline-text-3" id="text-2-4">

<p>\label{sec:Exploratory-Data-Analysis}
</p>
<p>
This field was founded (mostly) by John Tukey (1915-2000). Its tools are useful when not much is known regarding the underlying causes associated with the data set, and are often used for checking assumptions. For example, suppose we perform an experiment and collect some data&hellip; now what? We look at the data using exploratory visual tools.
</p>

</div>

<div id="outline-container-2-4-1" class="outline-4">
<h4 id="sec-2-4-1"><span class="section-number-4">2.4.1</span> More About Stemplots </h4>
<div class="outline-text-4" id="text-2-4-1">


<p>
There are many bells and whistles associated with stemplots, and the <code>stem.leaf</code> function can do many of them.
</p>
<dl>
<dt>Trim Outliers:</dt><dd>Some data sets have observations that fall far from the bulk of the other data (in a sense made more precise in Section <a href="#sub-Outliers">Outliers</a>). These extreme observations often obscure the underlying structure to the data and are best left out of the data display. The <code>trim.outliers</code> argument (which is <code>TRUE</code> by default) will separate the extreme observations from the others and graph the stemplot without them; they are listed at the bottom (respectively, top) of the stemplot with the label <code>HI</code> (respectively <code>LO</code>). 

</dd>
<dt>Split Stems:</dt><dd>The standard stemplot has only one line per stem, which means that all observations with first digit <code>3</code> are plotted on the same line, regardless of the value of the second digit. But this gives some stemplots a ``skyscraper'' appearance, with too many observations stacked onto the same stem. We can often fix the display by increasing the number of lines available for a given stem. For example, we could make two lines per stem, say, <code>3*</code> and <code>3.</code>. Observations with second digit 0 through 4 would go on the upper line, while observations with second digit 5 through 9 would go on the lower line. (We could do a similar thing with five lines per stem, or even ten lines per stem.) The end result is a more spread out stemplot which often looks better. A good example of this was shown on page \pageref{exa:stemleaf-multiple-lines-stem}. 

</dd>
<dt>Depths:</dt><dd>these are used to give insight into the balance of the observations as they accumulate toward the median. In a column beside the standard stemplot, the frequency of the stem containing the sample median is shown in parentheses. Next, frequencies are accumulated from the outside inward, including the outliers. Distributions that are more symmetric will have better balanced depths on either side of the sample median.
</dd>
</dl>


<p>
The basic command is <code>stem(x)</code> or a more sophisticated version written by Peter Wolf called <code>stem.leaf(x)</code> in the \textsf{R} Commander. We will describe <code>stem.leaf</code> since that is the one used by \textsf{R} Commander.
</p>

<p>
WARNING: Sometimes when making a stem plot the result will not be what you expected. There are several reasons for this: 
</p>
<ul>
<li>Stemplots by default will trim extreme observations (defined in Section <a href="#sub-Outliers">Outliers</a>) from the display. This in some cases will result in stemplots that are not as wide as expected.

</li>
<li>The leafs digit is chosen automatically by <code>stem.leaf</code> according to an algorithm that the computer believes will represent the data well. Depending on the choice of the digit, <code>stem.leaf</code> may drop digits from the data or round the values in unexpected ways.
</li>
</ul>


<p>
Let us take a look at the <code>rivers</code> data set\label{ite:stemplot-rivers}.
</p>



<pre class="example">stem.leaf(rivers)

</pre>





<pre class="example">1 | 2: represents 120
 leaf unit: 10
            n: 141
    1     1 | 3
   29     2 | 0111133334555556666778888899
   64     3 | 00000111122223333455555666677888999
  (18)    4 | 011222233344566679
   59     5 | 000222234467
   47     6 | 0000112235789
   34     7 | 12233368
   26     8 | 04579
   21     9 | 0008
   17    10 | 035
   14    11 | 07
   12    12 | 047
    9    13 | 0
HI: 1450 1459 1770 1885 2315 2348 2533 3710
</pre>



<p>
The stemplot shows a right-skewed shape to the <code>rivers</code> data distribution. Notice that the last digit of each of the data values were dropped from the display. Notice also that there were eight extreme observations identified by the computer, and their exact values are listed at the bottom of the stemplot. Look at the scale on the left of the stemplot and try to imagine how ridiculous the graph would have looked had we tried to include enough stems to include these other eight observations; the stemplot would have stretched over several pages. Notice finally that we can use the depths to approximate the sample median for these data. The median lies in the row identified by <code>(18)</code>, which means that the median is the average of the ninth and tenth observation on that row. Those two values correspond to <code>43</code> and <code>43</code>, so a good guess for the median would be 430. (For the record, the sample median is \(\widetilde{x}=425\). Recall that stemplots round the data to the nearest stem-leaf pair.) 
</p>
<p>
Next let us see what the <code>precip</code> data look like.
</p>



<pre class="example">stem.leaf(precip)

</pre>





<pre class="example">1 | 2: represents 12
 leaf unit: 1
            n: 70
LO: 7 7.2 7.8 7.8
    8    1* | 1344
   13    1. | 55677
   16    2* | 024
   18    2. | 59
   28    3* | 0000111234
  (15)   3. | 555566677788899
   27    4* | 0000122222334
   14    4. | 56688899
    6    5* | 44
    4    5. | 699
HI: 67
</pre>



<p>
Here is an example of split stems, with two lines per stem. The final digit of each datum has been dropped for the display. The data appear to be left skewed with four extreme values to the left and one extreme value to the right. The sample median is approximately 37 (it turns out to be 36.6).
</p>

</div>

</div>

<div id="outline-container-2-4-2" class="outline-4">
<h4 id="sec-2-4-2"><span class="section-number-4">2.4.2</span> Hinges and the Five Number Summary </h4>
<div class="outline-text-4" id="text-2-4-2">

<p>\label{sub:hinges-and-5NS}
</p>
<p>
Given a data set \(x_{1}\), \(x_{2}\), &hellip;, \(x_{n}\), the hinges are found by the following method:  
</p>
<ul>
<li>Find the order statistics \(x_{(1)}\), \(x_{(2)}\), &hellip;, \(x_{(n)}\). 

</li>
<li>The <i>lower hinge</i> \(h_{L}\) is in position \(L=\left\lfloor (n+3)/2\right\rfloor /2\), where the symbol $\left&lfloor; x\right&rfloor; $ denotes the largest integer less than or equal to \(x\). If the position \(L\) is not an integer, then the hinge \(h_{L}\) is the average of the adjacent order statistics. 

</li>
<li>The <i>upper hinge</i> \(h_{U}\) is in position \(n+1-L\).
</li>
</ul>

<p>Given the hinges, the <i>five number summary</i> (\(5NS\)) is 
</p>


\begin{equation} 
5NS=(x_{(1)},\ h_{L},\ \tilde{x},\ h_{U},\ x_{(n)}).
\end{equation}

<p>
An advantage of the \(5NS\) is that it reduces a potentially large data set to a shorter list of only five numbers, and further, these numbers give insight regarding the shape of the data distribution similar to the sample quantiles in Section <a href="#sub-Order-Statistics-and">Order-Statistics-and</a>.
</p>


<p>
If the data are stored in a vector <code>x</code>, then you can compute the \(5NS\) with the <code>fivenum</code> function.
</p>
</div>

</div>

<div id="outline-container-2-4-3" class="outline-4">
<h4 id="sec-2-4-3"><span class="section-number-4">2.4.3</span> Boxplots </h4>
<div class="outline-text-4" id="text-2-4-3">

<p>\label{sub:boxplots}
</p>
<p>
A boxplot is essentially a graphical representation of the \(5NS\). It can be a handy alternative to a stripchart when the sample size is large.
</p>
<p>
A boxplot is constructed by drawing a box alongside the data axis with sides located at the upper and lower hinges. A line is drawn parallel to the sides to denote the sample median. Lastly, whiskers are extended from the sides of the box to the maximum and minimum data values (more precisely, to the most extreme values that are not potential outliers, defined below).
</p>
<p>
Boxplots are good for quick visual summaries of data sets, and the relative positions of the values in the \(5NS\) are good at indicating the underlying shape of the data distribution, although perhaps not as effectively as a histogram. Perhaps the greatest advantage of a boxplot is that it can help to objectively identify extreme observations in the data set as described in the next section.
</p>
<p>
Boxplots are also good because one can visually assess multiple features of the data set simultaneously:
</p>
<dl>
<dt>Center</dt><dd>can be estimated by the sample median, \(\tilde{x}\).

</dd>
<dt>Spread</dt><dd>can be judged by the width of the box, \(h_{U}-h_{L}\). We know that this will be close to the \(IQR\), which can be compared to \(s\) and the \(MAD\), perhaps after rescaling if appropriate.

</dd>
<dt>Shape</dt><dd>is indicated by the relative lengths of the whiskers, and the position of the median inside the box. Boxes with unbalanced whiskers indicate skewness in the direction of the long whisker. Skewed distributions often have the median tending in the opposite direction of skewness. Kurtosis can be assessed using the box and whiskers. A wide box with short whiskers will tend to be platykurtic, while a skinny box with wide whiskers indicates leptokurtic distributions.

</dd>
<dt>Extreme observations</dt><dd>are identified with open circles (see below).
</dd>
</dl>


</div>

</div>

<div id="outline-container-2-4-4" class="outline-4">
<h4 id="sec-2-4-4"><span class="section-number-4">2.4.4</span> Outliers </h4>
<div class="outline-text-4" id="text-2-4-4">

<p>\label{sub:Outliers}
</p>
<p>
A <i>potential outlier</i> is any observation that falls beyond 1.5 times the width of the box on either side, that is, any observation less than \(h_{L}-1.5(h_{U}-h_{L})\) or greater than \(h_{U}+1.5(h_{U}-h_{L})\). A <i>suspected outlier</i> is any observation that falls beyond 3 times the width of the box on either side. In \textsf{R}, both potential and suspected outliers (if present) are denoted by open circles; there is no distinction between the two. 
</p>
<p>
When potential outliers are present, the whiskers of the boxplot are then shortened to extend to the most extreme observation that is not a potential outlier. If an outlier is displayed in a boxplot, the index of the observation may be identified in a subsequent plot in <code>Rcmdr</code> by clicking the \textsf{Identify outliers with mouse} option in the \textsf{Boxplot} dialog.
</p>
<p>
What do we do about outliers? They merit further investigation. The primary goal is to determine why the observation is outlying, if possible. If the observation is a typographical error, then it should be corrected before continuing. If the observation is from a subject that does not belong to the population of interest, then perhaps the datum should be removed. Otherwise, perhaps the value is hinting at some hidden structure to the data.
</p>

<p>
The quickest way to visually identify outliers is with a boxplot, described above. Another way is with the <code>boxplot.stats</code> function.
</p>
<p>
The <code>rivers</code> data. We will look for potential outliers in the <code>rivers</code> data.
</p>



<pre class="example">boxplot.stats(rivers)$out

</pre>




<pre class="example">
 [1] 1459 1450 1243 2348 3710 2315 2533 1306 1270 1885 1770
</pre>


<p>
We may change the <code>coef</code> argument to 3 (it is 1.5 by default) to identify suspected outliers.
</p>



<pre class="example">boxplot.stats(rivers, coef = 3)$out

</pre>




<pre class="example">
[1] 2348 3710 2315 2533 1885
</pre>



</div>

</div>

<div id="outline-container-2-4-5" class="outline-4">
<h4 id="sec-2-4-5"><span class="section-number-4">2.4.5</span> Standardizing variables </h4>
<div class="outline-text-4" id="text-2-4-5">


<p>
It is sometimes useful to compare data sets with each other on a scale that is independent of the measurement units. Given a set of observed data \(x_{1}\), \(x_{2}\), &hellip;, \(x_{n}\) we get \(z\) scores, denoted \(z_{1}\), \(z_{2}\), &hellip;, \(z_{n}\), by means of the following formula
</p>


\[
z_{i}=\frac{x_{i}-\xbar}{s},\quad i=1,\,2,\,\ldots,\, n.
\]



<p>
The <code>scale</code> function will rescale a numeric vector (or data frame) by subtracting the sample mean from each value (column) and/or by dividing each observation by the sample standard deviation.
</p>

</div>
</div>

</div>

<div id="outline-container-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> Multivariate Data and Data Frames </h3>
<div class="outline-text-3" id="text-2-5">

<p>\label{sec:multivariate-data}
</p>
<p>
We have had experience with vectors of data, which are long lists of numbers. Typically, each entry in the vector is a single measurement on a subject or experimental unit in the study. We saw in Section <a href="#sub-Vectors">Vectors</a> how to form vectors with the <code>c</code> function or the <code>scan</code> function. 
</p>
<p>
However, statistical studies often involve experiments where there are two (or more) measurements associated with each subject. We display the measured information in a rectangular array in which each row corresponds to a subject, and the columns contain the measurements for each respective variable. For instance, if one were to measure the height and weight and hair color of each of 11 persons in a research study, the information could be represented with a rectangular array. There would be 11 rows. Each row would have the person's height in the first column and hair color in the second column.
</p>
<p>
The corresponding objects in \textsf{R} are called <i>data frames</i>, and they can be constructed with the <code>data.frame</code> function. Each row is an observation, and each column is a variable.
</p>
<p>
Suppose we have two vectors <code>x</code> and <code>y</code> and we want to make a data frame out of them.
</p>



<pre class="example">x &lt;- 5:8
y &lt;- letters[3:6]
A &lt;- data.frame(v1 = x, v2 = y)

</pre>






<p>
Notice that <code>x</code> and <code>y</code> are the same length. This is <i>necessary</i>. Also notice that <code>x</code> is a numeric vector and <code>y</code> is a character vector. We may choose numeric and character vectors (or even factors) for the columns of the data frame, but each column must be of exactly one type. That is, we can have a column for <code>height</code> and a column for <code>gender</code>, but we will get an error if we try to mix function <code>height</code> (numeric) and <code>gender</code> (character or factor) information in the same column.
</p>
<p>
Indexing of data frames is similar to indexing of vectors. To get the entry in row \(i\) and column \(j\) do <code>A[i,j]</code>. We can get entire rows and columns by omitting the other index. 
</p>



<pre class="example">A[3,]
A[1, ]
A[ ,2]

</pre>




<pre class="example">
  v1 v2
3  7  e
  v1 v2
1  5  c
[1] c d e f
Levels: c d e f
</pre>


<p>
There are several things happening above. Notice that <code>A[3,]</code> gave a data frame (with the same entries as the third row of <code>A</code>) yet <code>A[1, ]</code> is a numeric vector. <code>A[ ,2]</code> is a factor vector because the default setting for <code>data.frame</code> is <code>stringsAsFactors = TRUE</code>.
</p>
<p>
Data frames have a <code>names</code> attribute and the names may be extracted with the <code>names</code> function. Once we have the names we may extract given columns by way of the dollar sign.
</p>



<pre class="example">names(A)
A$v1

</pre>




<pre class="example">
[1] "v1" "v2"
[1] 5 6 7 8
</pre>


<p>
The above is identical to <code>A[ ,1]</code>. 
</p>

</div>

<div id="outline-container-2-5-1" class="outline-4">
<h4 id="sec-2-5-1"><span class="section-number-4">2.5.1</span> Bivariate Data </h4>
<div class="outline-text-4" id="text-2-5-1">

<p>\label{sub:Bivariate-Data}
</p>
<ul>
<li>Stacked bar charts
</li>
<li>odds ratio and relative risk
</li>
<li>Introduce the sample correlation coefficient.
</li>
</ul>


<p>
The <b>sample Pearson product-moment correlation coefficient</b>:
</p>


\[
r=\frac{\sum_{i=1}^{n}(x_{i}-\xbar)(y_{i}-\ybar)}{\sqrt{\sum_{i=1}^{n}(x_{i}-\xbar)}\sqrt{\sum_{i=1}^{n}(y_{i}-\ybar)}}
\]

<ul>
<li>independent of scale
</li>
<li>\(-1&lt;r&lt;1\)
</li>
<li>measures <i>strength</i> and <i>direction</i> of linear association
</li>
<li>Two-Way Tables. Done with <code>table</code>, or in the \textsf{R} Commander by following \textsf{Statistics \(\triangleright\) Contingency Tables $\triangleright$} \textsf{Two-way Tables}. You can also enter and analyze a two-way table.
</li>
</ul>

<ul>
<li>table
</li>
<li>prop.table
</li>
<li>addmargins
</li>
<li>rowPercents (Rcmdr)
</li>
<li>colPercents (Rcmdr)
</li>
<li>totPercents(Rcmdr)
</li>
<li>A &lt;- xtabs(~ gender + race, data = RcmdrTestDrive)
</li>
<li>xtabs( Freq ~ Class + Sex, data = Titanic) from built in table
</li>
<li>barplot(A, legend.text=TRUE) 
</li>
<li>barplot(t(A), legend.text=TRUE) 
</li>
<li>barplot(A, legend.text=TRUE, beside = TRUE)
</li>
<li>spineplot(gender ~ race, data = RcmdrTestDrive)
</li>
<li>Spine plot: plots categorical versus categorical
</li>
</ul>

<ul>
<li>Scatterplot: look for linear association and correlation. 
<ul>
<li>carb ~ optden, data = Formaldehyde (boring)
</li>
<li>conc ~ rate, data = Puromycin
</li>
<li>xyplot(accel ~ dist, data = attenu) nonlinear association
</li>
<li>xyplot(eruptions ~ waiting, data = faithful) (linear, two groups)
</li>
<li>xyplot(Petal.Width ~ Petal.Length, data = iris)
</li>
<li>xyplot(pressure ~ temperature, data = pressure) (exponential growth)
</li>
<li>xyplot(weight ~ height, data = women) (strong positive linear)
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-2-5-2" class="outline-4">
<h4 id="sec-2-5-2"><span class="section-number-4">2.5.2</span> Multivariate Data </h4>
<div class="outline-text-4" id="text-2-5-2">

<p>\label{sub:Multivariate-Data}
</p>
<p>
Multivariate Data Display
</p>
<ul>
<li>Multi-Way Tables. You can do this with <code>table</code>,
</li>
</ul>

<p>or in \textsf{R} Commander by following \textsf{Statistics} \textsf{$\triangleright$} \textsf{Contingency Tables} \textsf{$\triangleright$} \textsf{Multi-way Tables}.
</p><ul>
<li>Scatterplot matrix. used for displaying pairwise scatterplots simultaneously. Again, look for linear association and correlation.
</li>
<li>3D Scatterplot. See Figure \pageref{fig:3D-scatterplot-trees}
</li>
<li><code>plot(state.region, state.division)</code> 
</li>
<li><code>barplot(table(state.division,state.region), legend.text=TRUE)</code>
</li>
</ul>

</div>
</div>

</div>

<div id="outline-container-2-6" class="outline-3">
<h3 id="sec-2-6"><span class="section-number-3">2.6</span> Comparing Populations </h3>
<div class="outline-text-3" id="text-2-6">

<p>\label{sec:Comparing-Data-Sets}
</p>
<p>
Sometimes we have data from two or more groups (or populations) and we would like to compare them and draw conclusions. Some issues that we would like to address:
</p>
<ul>
<li>Comparing centers and spreads: variation within versus between groups
</li>
<li>Comparing clusters and gaps
</li>
<li>Comparing outliers and unusual features
</li>
<li>Comparing shapes.
</li>
</ul>



</div>

<div id="outline-container-2-6-1" class="outline-4">
<h4 id="sec-2-6-1"><span class="section-number-4">2.6.1</span> Numerically </h4>
<div class="outline-text-4" id="text-2-6-1">


<p>
I am thinking here about the \textsf{Statistics} \textsf{$\triangleright$} \textsf{Numerical Summaries} \textsf{\(\triangleright\) Summarize by groups} option or the \textsf{Statistics} \textsf{$\triangleright$} \textsf{Summaries} \textsf{$\triangleright$Table of Statistics} option. 
</p>
</div>

</div>

<div id="outline-container-2-6-2" class="outline-4">
<h4 id="sec-2-6-2"><span class="section-number-4">2.6.2</span> Graphically </h4>
<div class="outline-text-4" id="text-2-6-2">


<ul>
<li>Boxplots
<ul>
<li>Variable width: the width of the drawn boxplots are proportional to \(\sqrt{n_{i}}\), where \(n_{i}\) is the size of the \(i^{\mathrm{th}}\) group. Why? Because many statistics have variability proportional to the reciprocal of the square root of the sample size. \item Notches: extend to \(1.58\cdot(h_{U}-h_{L})/\sqrt{n}\). The idea is to give roughly a 95% confidence interval for the difference in two medians. See Chapter <a href="#cha-Hypothesis-Testing">Hypothesis-Testing</a>.

</li>
</ul>

</li>
<li>Stripcharts
<ul>
<li>stripchart(weight ~ feed, method=\textquotedbl{}stack\textquotedbl{}, data=chickwts) 

</li>
</ul>

</li>
<li>Bar Graphs
<ul>
<li>barplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions)) stacked bar chart
</li>
<li>barplot(xtabs(Freq ~ Admit, data = UCBAdmissions))
</li>
<li>barplot(xtabs(Freq ~ Gender + Admit, data = UCBAdmissions, legend = TRUE, beside = TRUE)  oops, discrimination.
</li>
<li>barplot(xtabs(Freq ~ Admit+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) different departments have different standards
</li>
<li>barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE) men mostly applied to easy departments, women mostly applied to difficult departments
</li>
<li>barplot(xtabs(Freq ~ Gender+Dept, data = UCBAdmissions), legend = TRUE, beside = TRUE)
</li>
<li>barchart(Admit ~ Freq, data = C)
</li>
<li>barchart(Admit ~ Freq|Gender, data = C)
</li>
<li>barchart(Admit ~ Freq | Dept, groups = Gender, data = C)
</li>
<li>barchart(Admit ~ Freq | Dept, groups = Gender, data = C, auto.key = TRUE)

</li>
</ul>

</li>
<li>Histograms
<ul>
<li>~ breaks | wool{*}tension, data = warpbreaks
</li>
<li>~ weight | feed, data = chickwts
</li>
<li>~ weight | group, data = PlantGrowth 
</li>
<li>~ count | spray, data = InsectSprays
</li>
<li>~ len | dose, data = ToothGrowth
</li>
<li>~ decrease | treatment, data = OrchardSprays (or rowpos or colpos)

</li>
</ul>

</li>
<li>Scatterplots
</li>
</ul>





<pre class="example">library(lattice)
xyplot(Petal.Width ~ Petal.Length, data = iris, group = Species)

</pre>









<ul>
<li>Scatterplot matrices
<ul>
<li>splom( ~ cbind(GNP.deflator,GNP,Unemployed,Armed.Forces,Population,Year,Employed),  data = longley) 
</li>
<li>splom( ~ cbind(pop15,pop75,dpi), data = LifeCycleSavings)
</li>
<li>splom( ~ cbind(Murder, Assault, Rape), data = USArrests)
</li>
<li>splom( ~ cbind(CONT, INTG, DMNR), data = USJudgeRatings)
</li>
<li>splom( ~ cbind(area,peri,shape,perm), data = rock)
</li>
<li>splom( ~ cbind(Air.Flow, Water.Temp, Acid.Conc., stack.loss), data = stackloss)
</li>
<li>splom( ~ cbind(Fertility,Agriculture,Examination,Education,Catholic,Infant.Mortality), data = swiss)
</li>
<li>splom(~ cbind(Fertility,Agriculture,Examination), data = swiss) (positive and negative)

</li>
</ul>

</li>
<li>Dot charts
<ul>
<li>dotchart(USPersonalExpenditure)
</li>
<li>dotchart(t(USPersonalExpenditure))
</li>
<li>dotchart(WorldPhones) (transpose is no good)
</li>
<li>freeny.x is no good, neither is volcano
</li>
<li>dotchart(UCBAdmissions{[},,1{]})
</li>
<li>dotplot(Survived ~ Freq | Class, groups = Sex, data = B)
</li>
<li>dotplot(Admit ~ Freq | Dept, groups = Gender, data = C)

</li>
</ul>

</li>
<li>Mosaic plot
<ul>
<li>mosaic( ~ Survived + Class + Age + Sex, data = Titanic) (or just mosaic(Titanic))
</li>
<li>mosaic( ~ Admit + Dept + Gender, data = UCBAdmissions)

</li>
</ul>

</li>
<li>Spine plots
<ul>
<li>spineplot(xtabs(Freq ~ Admit + Gender, data = UCBAdmissions))
</li>
<li># rescaled barplot

</li>
</ul>

</li>
<li>Quantile-quantile plots: There are two ways to do this. One way is to compare two independent samples (of the same size). qqplot(x,y). Another way is to compare the sample quantiles of one variable to the theoretical quantiles of another distribution. 
</li>
</ul>

<p>Given two samples \(x_{1}\), \(x_{2}\), &hellip;, \(x_{n}\), and \(y_{1}\), \(y_{2}\), &hellip;, \(y_{n}\), we may find the order statistics \(x_{(1)}\leq x_{(2)}\leq\cdots\leq x_{(n)}\) and \(y_{(1)}\leq y_{(2)}\leq\cdots\leq y_{(n)}\). Next, plot the \(n\) points \((x_{(1)},y_{(1)})\), \((x_{(2)},y_{(2)})\), &hellip;, \((x_{(n)},y_{(n)})\).
</p>
<p>
It is clear that if \(x_{(k)}=y_{(k)}\) for all \(k=1,2,\ldots,n\), then we will have a straight line. It is also clear that in the real world, a straight line is NEVER observed, and instead we have a scatterplot that hopefully had a general linear trend. What do the rules tell us? 
</p>
<ul>
<li>If the \(y\)-intercept of the line is greater (less) than zero, then the center of the \(Y\) data is greater (less) than the center of the \(X\) data.

</li>
<li>If the slope of the line is greater (less) than one, then the spread of the \(Y\) data is greater (less) than the spread of the \(X\) data.
</li>
</ul>


</div>

</div>

<div id="outline-container-2-6-3" class="outline-4">
<h4 id="sec-2-6-3"><span class="section-number-4">2.6.3</span> Lattice Graphics </h4>
<div class="outline-text-4" id="text-2-6-3">

<p>\label{sub:Lattice-Graphics}
</p>
<p>
The following types of plots are useful when there is one variable of interest and there is a factor in the data set by which the variable is categorized. 
</p>
<p>
It is sometimes nice to set <code>lattice.options(default.theme = "col.whitebg")</code>
</p>





<pre class="example">library(lattice)
bwplot(~weight | feed, data = chickwts)

</pre>












<pre class="example">histogram(~age | education, data = infert)

</pre>













<pre class="example">xyplot(Petal.Length ~ Petal.Width | Species, data = iris)

</pre>












<pre class="example">coplot(conc ~ uptake | Type * Treatment, data = CO2)

</pre>












<p>
Open \textsf{R} and issue the following commands at the command line to get started. Note that you need to have the <code>RcmdrPlugin.IPSUR</code> package installed, and for some exercises you need the <code>e1071</code> package.
</p>



<pre class="example">library(RcmdrPlugin.IPSUR)
data(RcmdrTestDrive)
attach(RcmdrTestDrive)
names(RcmdrTestDrive)

</pre>




<p>
To load the data in the \textsf{R} Commander (<code>Rcmdr</code>), click the \textsf{Data Set} button, and select <code>RcmdrTestDrive</code> as the active data set. To learn more about the data set and where it comes from, type <code>?RcmdrTestDrive</code> at the command line.
</p>
<p>
\label{xca:summary-RcmdrTestDrive}
</p>
<p>
Perform a summary of all variables in <code>RcmdrTestDrive</code>. You can do this with the command <code>summary(RcmdrTestDrive)</code>.
</p>
<p>
Alternatively, you can do this in the <code>Rcmdr</code> with the sequence \textsf{Statistics} \textsf{\(\triangleright\) Summaries} \textsf{\(\triangleright\) Active Data Set}. Report the values of the summary statistics for each variable.
</p>





<pre class="example">summary(RcmdrTestDrive)

</pre>





<p>
Make a table of the <code>race</code> variable. Do this with \textsf{Statistics} \textsf{\(\triangleright\) Summaries} \textsf{\(\triangleright\) IPSUR - Frequency Distributions}&hellip;
</p>
<ol>
<li>Which ethnicity has the highest frequency?
</li>
<li>Which ethnicity has the lowest frequency?
</li>
<li>Include a bar graph of <code>race</code>. Do this with \textsf{Graphs} \textsf{$\triangleright$} \textsf{IPSUR - Bar Graph}&hellip;
</li>
</ol>





<p>
First we will make a table of the <code>race</code> variable with the <code>table</code> function.
</p>



<pre class="example">table(race)

</pre>




<pre class="example">
race
AfricanAmer       Asian   Caucasian    Hispanic       Other 
         46          13          73          34           2
</pre>


<ol>
<li>For these data,  [1] "Caucasian" has the highest frequency.

</li>
<li>For these data,  [1] "Other" has the lowest frequency.

</li>
<li>The graph is shown below.
</li>
</ol>




<pre class="example">barplot(table(RcmdrTest#+BEGIN_SRC R 
barplot(table(RcmdrTestDrive$race), main = "", xlab = "race", 
  ylab = "Frequency", legend.text = FALSE, col = NULL)

</pre>


<p>
RTDrace.pdf}
    \caption[A \texttt{barplot} of \texttt{race}]{A \texttt{barplot} of \texttt{race} in the \texttt{RcmdrTestDrive} data}
    \label{fig:barplotRTDrace}
  \end{figure}
</p>

<p>
Calculate the average <code>salary</code> by the factor <code>gender</code>. Do this with \textsf{Statistics} \textsf{\(\triangleright\) Summaries} \textsf{\(\triangleright\) Table of Statistics}&hellip; 
</p>
<ol>
<li>Which <code>gender</code> has the highest mean <code>salary</code>? 
</li>
<li>Report the highest mean <code>salary</code>.
</li>
<li>Compare the spreads for the genders by calculating the standard deviation of <code>salary</code> by <code>gender</code>. Which <code>gender</code> has the biggest standard deviation?
</li>
<li>Make boxplots of <code>salary</code> by <code>gender</code> with the following method:
</li>
</ol>


<p>
On the <code>Rcmdr</code>, click \textsf{Graphs} \textsf{$\triangleright$} \textsf{IPSUR - Boxplot}&hellip;<br/>
In the \textsf{Variable} box, select <code>salary</code>.<br/>
Click the \textsf{Plot by groups}&hellip; box and select <code>gender</code>. Click \textsf{OK}.<br/>
Click \textsf{OK} to graph the boxplot.
</p>
<p>
How does the boxplot compare to your answers to (1) and (3)?
</p>




<p>
We can generate a table listing the average salaries by gender with two methods. The first uses <code>tapply</code>:
</p>



<pre class="example">x &lt;- tapply(salary, list(gender = gender), mean)
x

</pre>




<pre class="example">
gender
  Female     Male 
698.0911 743.3915
</pre>


<p>
The second method uses the <code>by</code> function:
</p>



<pre class="example">by(salary, gender, mean, na.rm = TRUE)

</pre>




<pre class="example">
gender: Female
[1] 698.0911
-------------------------------------------------------- 
gender: Male
[1] 743.3915
</pre>


<p>
Now to answer the questions:
</p>

<ol>
<li>Which gender has the highest mean salary? 
</li>
</ol>


<p>
We can answer this by looking above. For these data, the gender with the highest mean salary is  [1] "Male".
</p>
<ol>
<li>Report the highest mean salary.
</li>
</ol>


<p>
Depending on our answer above, we would do something like 
</p>
<pre class="example">
mean(salary[gender == Male])
</pre>


<p>
for example. For these data, the highest mean salary is
</p>



<pre class="example">x[which(x==max(x))]

</pre>


<p>
43.3915
</p>
<ol>
<li>Compare the spreads for the genders by calculating the standard deviation of <code>salary</code> by <code>gender</code>. Which gender has the biggest standard deviation?
</li>
</ol>





<pre class="example">y &lt;- tapply(salary, list(gender = gender), sd)
y

</pre>




<pre class="example">
gender
  Female     Male 
130.7053 158.5423
</pre>


<p>
For these data, the the largest standard deviation is approximately    Male 
158.54 which was attained by the  [1] "Male" gender.
</p>
<ol>
<li>Make boxplots of <code>salary</code> by <code>gender</code>. How does the boxplot compare to your answers to (1) and (3)?
</li>
</ol>


<p>
The graph is shown below.
</p>



<pre class="example">boxplot(salary ~ gender, xlab = "salary", ylab = "gender", 
  main = "", notch = FALSE, varwidth = TRUE, horizontal = TRUE, 
  data = RcmdrTestDrive)

</pre>



<p>
    \includegraphics[width=5in, height=4in]{img/boxplotRTDsalgen.pdf}
    \caption[Side-by-side \texttt{boxplot}s of \texttt{salary} by \texttt{gender}]{Side-by-side \texttt{boxplot}s of \texttt{salary} by \texttt{gender} in the \texttt{RcmdrTestDrive} data}
    \label{fig:boxplotRTDsalgen}
  \end{figure}
</p>
<p>
Answers will vary. There should be some remarks that the center of the box is farther to the right for the  [1] "Male" gender, and some recognition that the box is wider for the  [1] "Male" gender.
</p>

<p>
For this problem we will study the variable <code>reduction</code>.
</p>
<ol>
<li>Find the order statistics and store them in a vector <code>x</code>. <i>Hint:</i> <code>x &lt;- sort(reduction)</code>
</li>
<li>Find \(x_{(137)}\), the 137\(^{\mathrm{th}}\) order statistic.
</li>
<li>Find the IQR.
</li>
<li>Find the Five Number Summary (5NS).
</li>
<li>Use the 5NS to calculate what the width of a boxplot of <code>reduction</code> would be.
</li>
<li>Compare your answers (3) and (5). Are they the same? If not, are they close?
</li>
<li>Make a boxplot of <code>reduction</code>, and include the boxplot in your report. You can do this with the <code>boxplot</code> function, or in <code>Rcmdr</code> with \textsf{Graphs} \textsf{$\triangleright$} \textsf{IPSUR - Boxplot}&hellip;
</li>
<li>Are there any potential/suspected outliers? If so, list their values. <i>Hint:</i> use your answer to (a).
</li>
<li>Using the rules discussed in the text, classify answers to (8), if any, as <i>potential</i> or <i>suspected</i> outliers.
</li>
</ol>







<pre class="example">x &lt;- sort(reduction)

</pre>


<p>
esults output pp
x[137]
IQR(x)
fivenum(x)
fivenum(x)[4] - fivenum(x)[2]
</p>
<p>
\noindent Compare your answers (3) and (5). Are they the same? If not, are they close?
</p>
<p>
Yes, they are close, within  [1] 0.75 of each other.
</p>
<p>
\noindent The boxplot of <code>reduction</code> is below.
</p>



<pre class="example">boxplot(reduction, xlab = "reduction", main = "", notch = FALSE, 
  horizontal = TRUE, data = RcmdrTestDrive)

</pre>


<p>
    \includegraphics[width=5in, height=4in]{img/boxplotRTDreduc.pdf}
    \caption[A \texttt{boxplot} of \texttt{reduction}]{A \texttt{boxplot} of \texttt{reduction} in the \texttt{RcmdrTestDrive} data}
    \label{fig:boxplotRTDreduc}
  \end{figure}
</p>




<pre class="example">temp &lt;- fivenum(x)
inF &lt;- 1.5 * (temp[4] - temp[2]) + temp[4]
outF &lt;- 3 * (temp[4] - temp[2]) + temp[4]
which(x &gt; inF)
which(x &gt; outF)

</pre>




<pre class="example">
[1] 166 167 168
[1] 168
</pre>


<p>
Observations  [1] 166 167 168 would be considered potential outliers, while observation(s)  [1] 168 would be considered a suspected outlier.
</p>
<p>
In this problem we will compare the variables <code>before</code> and <code>after</code>. Don't forget <code>library(e1071)</code>.
</p><ol>
<li>Examine the two measures of center for both variables. Judging from these measures, which variable has a higher center?
</li>
<li>Which measure of center is more appropriate for <code>before</code>? (You may want to look at a boxplot.) Which measure of center is more appropriate for <code>after</code>?
</li>
<li>Based on your answer to (2), choose an appropriate measure of spread for each variable, calculate it, and report its value. Which variable has the biggest spread? (Note that you need to make sure that your measures are on the same scale.) 
</li>
<li>Calculate and report the skewness and kurtosis for <code>before</code>. Based on these values, how would you describe the shape of <code>before</code>?
</li>
<li>Calculate and report the skewness and kurtosis for <code>after</code>. Based on these values, how would you describe the shape of <code>after</code>?
</li>
<li>Plot histograms of <code>before</code> and <code>after</code> and compare them to your answers to (4) and (5).
</li>
</ol>




<ol>
<li>Examine the two measures of center for both variables that you found in problem 1. Judging from these measures, which variable has a higher center?
</li>
</ol>


<p>
We may take a look at the <code>summary(RcmdrTestDrive)</code> output from Exercise <a href="#xca-summary-RcmdrTestDrive">summary-RcmdrTestDrive</a>. Here we will repeat the relevant summary statistics.
</p>



<pre class="example">c(mean(before), median(before))
c(mean(after), median(after))

</pre>


<p>
000
</p><pre class="example">
[1] 73.26726 73.70000
</pre>


<p>
The idea is to look at the two measures and compare them to make a decision. In a nice world, both the mean and median of one variable will be larger than the other which sends a nice message. If We get a mixed message, then we should look for other information, such as extreme values in one of the variables, which is one of the reasons for the next part of the problem.
</p>
<ol>
<li>Which measure of center is more appropriate for <code>before</code>? (You may want to look at a boxplot.) Which measure of center is more appropriate for <code>after</code>?
</li>
</ol>


<p>
The boxplot of <code>before</code> is shown below.
</p>



<pre class="example">boxplot(before, xlab = "before", main = "", notch = FALSE, 
  horizontal = TRUE, data = RcmdrTestDrive)

</pre>







<p>
We want to watch out for extreme values (shown as circles separated from the box) or large departures from symmetry. If the distribution is fairly symmetric then the mean and median should be approximately the same. But if the distribution is highly skewed with extreme values then we should be skeptical of the sample mean, and fall back to the median which is resistant to extremes. By design, the before variable is set up to have a fairly symmetric distribution.
</p>
<p>
A boxplot of <code>after</code> is shown next.
</p>



<pre class="example">boxplot(after, xlab = "after", main = "", notch = FALSE, 
  horizontal = TRUE, data = RcmdrTestDrive)

</pre>







<p>
The same remarks apply to the <code>after</code> variable. The <code>after</code>  variable has been designed to be left-skewed&hellip; thus, the median would likely be a good choice for this variable.
</p>
<ol>
<li>Based on your answer to (2), choose an appropriate measure of spread for each variable, calculate it, and report its value. Which variable has the biggest spread? (Note that you need to make sure that your measures are on the same scale.) 
</li>
</ol>


<p>
Since <code>before</code> has a symmetric, mound shaped distribution, an excellent measure of center would be the sample standard deviation.  And since <code>after</code> is left-skewed, we should use the median absolute deviation. It is also acceptable to use the IQR, but we should rescale it appropriately, namely, by dividing by 1.349. The exact values are shown below.
</p>



<pre class="example">sd(before)
mad(after)
IQR(after)/1.349

</pre>




<pre class="example">
[1] 1.076412
[1] 1.55673
[1] 1.556709
</pre>


<p>
Judging from the values above, we would decide which variable has the higher spread. Look at how close the <code>mad</code> and the <code>IQR</code> (after suitable rescaling) are; it goes to show why the rescaling is important.
</p>
<ol>
<li>Calculate and report the skewness and kurtosis for <code>before</code>. Based on these values, how would you describe the shape of <code>before</code>? 
</li>
</ol>


<p>
The values of these descriptive measures are shown below.
</p>



<pre class="example">library(e1071)
skewness(before)
kurtosis(before)

</pre>




<pre class="example">
[1] -0.7576277
</pre>


<p>
We should take the sample skewness value and compare it to \(2\sqrt{6/n}\approx\)  [1] 0.378 in absolute value to see if it is substantially different from zero. The direction of skewness is decided by the sign (positive or negative) of the skewness value. 
</p>

<p>
We should take the sample kurtosis value and compare it to \(2\cdot\sqrt{24/168}\approx\)  [1] 0.756), in absolute value to see if the excess kurtosis is substantially different from zero. And take a look at the sign to see whether the distribution is platykurtic or leptokurtic.
</p>
<ol>
<li>Calculate and report the skewness and kurtosis for <code>after</code>. Based on these values, how would you describe the shape of <code>after</code>?
</li>
</ol>


<p>
The values of these descriptive measures are shown below.
</p>



<pre class="example">skewness(after)
kurt#+BEGIN_SRC R 
skewness(after)
kurtosis(after)

</pre>


<p>
 did previously. We would again compare the sample skewness and kurtosis values (in absolute value) to  [1] 0.378 and  [1] 0.756, respectively.
</p>
<ol>
<li>Plot histograms of <code>before</code> and <code>after</code> and compare them to your answers to (4) and (5).
</li>
</ol>

<p>The graphs are shown below.
</p>





<pre class="example">hist(before, xlab = "before", data = RcmdrTestDrive)

</pre>


<p>
phics[width=5in, height=4in]{img/histRTDbefore.pdf}
    \caption[A histogram of \texttt{before}]{A histogram of \texttt{before} in the \texttt{RcmdrTestDrive} data}
    \label{fig:histRTDbefore}
  \end{figure}
</p>




<pre class="example">hist(after, xlab = "after", data = RcmdrTestDrive)

</pre>







<p>
Answers will vary. We are looking for visual consistency in the histograms
to our statements above.
</p>
<p>
Describe the following data sets just as if you were communicating with an alien, but one who has had a statistics class. Mention the salient features (data type, important properties, anything special). Support your answers with the appropriate visual displays and descriptive statistics.
</p>
<ol>
<li>Conversion rates of Euro currencies stored in <code>euro</code>.
</li>
<li>State abbreviations stored in <code>state.abb</code>.
</li>
</ol>





</div>
</div>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Probability </h2>
<div class="outline-text-2" id="text-3">

<p>\label{cha:Probability}
</p>
<p>
\noindent In this chapter we define the basic terminology associated with probability and derive some of its properties. We discuss three interpretations of probability. We discuss conditional probability and independent events, along with Bayes' Theorem. We finish the chapter with an introduction to random variables, which paves the way for the next two chapters.
</p>
<p>
In this book we distinguish between two types of experiments: <i>deterministic</i> and <i>random</i>. A <i>deterministic</i> experiment is one whose outcome may be predicted with certainty beforehand, such as combining Hydrogen and Oxygen, or adding two numbers such as \(2+3\). A <i>random</i> experiment is one whose outcome is determined by chance. We posit that the outcome of a random experiment may not be predicted with certainty beforehand, even in principle. Examples of random experiments include tossing a coin, rolling a die, and throwing a dart on a board, how many red lights you encounter on the drive home, how many ants traverse a certain patch of sidewalk over a short period, <i>etc</i>.
</p>

<ul>
<li>that there are multiple interpretations of probability, and the methods used depend somewhat on the philosophy chosen 
</li>
<li>nuts and bolts of basic probability jargon: sample spaces, events, probability functions, <i>etc</i>.
</li>
<li>how to count
</li>
<li>conditional probability and its relationship with independence
</li>
<li>Bayes' Rule and how it relates to the subjective view of probability
</li>
<li>what we mean by 'random variables', and where they come from
</li>
</ul>








</div>

<div id="outline-container-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Sample Spaces </h3>
<div class="outline-text-3" id="text-3-1">

<p>\label{sec:Sample-Spaces}
</p>
<p>
For a random experiment \(E\), the set of all possible outcomes of \(E\) is called the <i>sample space</i>\index{sample space} and is denoted by the letter \(S\). For a coin-toss experiment, \(S\) would be the results ``Head'' and ``Tail'', which we may represent by \( S = \{H,T \} \). Formally, the performance of a random experiment is the unpredictable selection of an outcome in \(S\).
</p>

<p>
Most of the probability work in this book is done with the <code>prob</code> package \cite{Kernsprob}. A sample space is (usually) represented by a <i>data frame</i>, that is, a rectangular collection of variables (see Section <a href="#sub-Multivariate-Data">Multivariate-Data</a>). Each row of the data frame corresponds to an outcome of the experiment. The data frame choice is convenient both for its simplicity and its compatibility with the \textsf{R} Commander. Data frames alone are, however, not sufficient to describe some of the more interesting probabilistic applications we will study later; to handle those we will need to consider a more general <i>list</i> data structure. See Section <a href="#sub-howto-ps-objects">howto-ps-objects</a> for details.
</p>
<p>
Consider the random experiment of dropping a Styrofoam cup onto the floor from a height of four feet. The cup hits the ground and eventually comes to rest. It could land upside down, right side up, or it could land on its side. We represent these possible outcomes of the random experiment by the following.
</p>



<pre class="example">S &lt;- data.frame(lands = c("down","up","side"))
S

</pre>




<pre class="example">
  lands
1  down
2    up
3  side
</pre>



<p>
The sample space <code>S</code> contains the column <code>lands</code> which stores the outcomes <code>down</code>, <code>up</code>, and <code>side</code>. 
</p>

<p>
Some sample spaces are so common that convenience wrappers were written to set them up with minimal effort. The underlying machinery that does the work includes the <code>expand.grid</code> function in the <code>base</code> package, <code>combn</code> in the <code>combinat</code> package \cite{combinat}, and <code>permsn</code> in the <code>prob</code> package[fn:countwrap]. Consider the random experiment of tossing a coin. The outcomes are \(H\) and \(T\). We can set up the sample space quickly with the <code>tosscoin</code> function:
</p>
<p>
[fn:countwrap] The seasoned \textsf{R} user can get the job done without the convenience wrappers. I encourage the beginner to use them to get started, but I also recommend that introductory students wean themselves as soon as possible. The wrappers were designed for ease and intuitive use, not for speed or efficiency.
</p>




<pre class="example">library(prob)
tosscoin(1)

</pre>




<pre class="example">
  toss1
1     H
2     T
</pre>



<p>
The number <code>1</code> tells <code>tosscoin</code> that we only want to toss the coin once. We could toss it three times: 
</p>



<pre class="example">tosscoin(3)

</pre>




<pre class="example">
  toss1 toss2 toss3
1     H     H     H
2     T     H     H
3     H     T     H
4     T     T     H
5     H     H     T
6     T     H     T
7     H     T     T
8     T     T     T
</pre>



<p>
Alternatively we could roll a fair die: 
</p>



<pre class="example">rolldie(1) 

</pre>




<pre class="example">
  X1
1  1
2  2
3  3
4  4
5  5
6  6
</pre>



<p>
The <code>rolldie</code> function defaults to a 6-sided die, but we can specify others with the <code>nsides</code> argument. The command <code>rolldie(3, nsides = 4)</code> would be used to roll a 4-sided die three times.
</p>
<p>
Perhaps we would like to draw one card from a standard set of playing cards (it is a long data frame):
</p>



<pre class="example">head(cards()) 

</pre>




<pre class="example">
  rank suit
1    2 Club
2    3 Club
3    4 Club
4    5 Club
5    6 Club
6    7 Club
</pre>



<p>
The <code>cards</code> function that we just used has optional arguments <code>jokers</code> (if you would like Jokers to be in the deck) and <code>makespace</code> which we will discuss later. There is also a <code>roulette</code> function which returns the sample space associated with one spin on a roulette wheel. There are EU and USA versions available. Interested readers may contribute any other game or sample spaces that may be of general interest.
</p>

</div>

<div id="outline-container-3-1-1" class="outline-4">
<h4 id="sec-3-1-1"><span class="section-number-4">3.1.1</span> Sampling from Urns </h4>
<div class="outline-text-4" id="text-3-1-1">

<p>\label{sub:sampling-from-urns}
</p>
<p>
This is perhaps the most fundamental type of random experiment. We have an urn that contains a bunch of distinguishable objects (balls) inside. We shake up the urn, reach inside, grab a ball, and take a look. That's all.
</p>
<p>
But there are all sorts of variations on this theme. Maybe we would like to grab more than one ball &ndash; say, two balls. What are all of the possible outcomes of the experiment now? It depends on how we sample. We could select a ball, take a look, put it back, and sample again. Another way would be to select a ball, take a look &ndash; but do not put it back &ndash; and sample again (equivalently, just reach in and grab two balls). There are certainly more possible outcomes of the experiment in the former case than in the latter. In the first (second) case we say that sampling is done <i>with (without) replacement</i>.
</p>
<p>
There is more. Suppose we do not actually keep track of which ball came first. All we observe are the two balls, and we have no idea about the order in which they were selected. We call this <i>unordered sampling</i> (in contrast to <i>ordered</i>) because the order of the selections does not matter with respect to what we observe. We might as well have selected the balls and put them in a bag before looking.
</p>
<p>
Note that this one general class of random experiments contains as a special case all of the common elementary random experiments. Tossing a coin twice is equivalent to selecting two balls labeled \(H\) and \(T\) from an urn, with replacement. The die-roll experiment is equivalent to selecting a ball from an urn with six elements, labeled 1 through 6.
</p>

<p>
The <code>prob</code> package accomplishes sampling from urns with the <code>urnsamples</code>\index{urnsamples@\texttt{urnsamples}} function, which has arguments <code>x</code>, <code>size</code>, <code>replace</code>, and <code>ordered</code>. The argument <code>x</code> represents the urn from which sampling is to be done. The <code>size</code> argument tells how large the sample will be. The <code>ordered</code> and <code>replace</code> arguments are logical and specify how sampling will be performed. We will discuss each in turn.
</p>
<p>
Let our urn simply contain three balls, labeled 1, 2, and 3, respectively. We are going to take a sample of size 2 from the urn. 
</p>

<p>
If sampling is with replacement, then we can get any outcome 1, 2, or 3 on any draw. Further, by ``ordered'' we mean that we shall keep track of the order of the draws that we observe. We can accomplish this in \textsf{R} with
</p>



<pre class="example">urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)

</pre>





<pre class="example">  X1 X2
1  1  1
2  2  1
3  3  1
4  1  2
5  2  2
6  3  2
7  1  3
8  2  3
9  3  3
</pre>





<p>
Notice that rows 2 and 4 are identical, save for the order in which the numbers are shown. Further, note that every possible pair of the numbers 1 through 3 are listed. This experiment is equivalent to rolling a 3-sided die twice, which we could have accomplished with <code>rolldie(2, nsides = 3)</code>.
</p>

<p>
Here sampling is without replacement, so we may not observe the same number twice in any row. Order is still important, however, so we expect to see the outcomes <code>1,2</code> and <code>2,1</code> somewhere in our data frame. 
</p>



<pre class="example">urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)

</pre>




<pre class="example">
  X1 X2
1  1  2
2  2  1
3  1  3
4  3  1
5  2  3
6  3  2
</pre>


<p>
This is just as we expected. Notice that there are less rows in this answer due to the more restrictive sampling procedure. If the numbers 1, 2, and 3 represented ``Fred'', ``Mary'', and ``Sue'', respectively, then this experiment would be equivalent to selecting two people of the three to serve as president and vice-president of a company, respectively, and the sample space shown above lists all possible ways that this could be done.
</p>

<p>
Again, we may not observe the same outcome twice, but in this case, we will only retain those outcomes which (when jumbled) would not duplicate earlier ones. 
</p>



<pre class="example">urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE) 

</pre>




<pre class="example">
  X1 X2
1  1  2
2  1  3
3  2  3
</pre>


<p>
This experiment is equivalent to reaching in the urn, picking a pair, and looking to see what they are. This is the default setting of <code>urnsamples</code>, so we would have received the same output by simply typing <code>urnsamples(1:3, 2)</code>.
</p>

<p>
The last possibility is perhaps the most interesting. We replace the balls after every draw, but we do not remember the order in which the draws came. 
</p>



<pre class="example">urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE) 

</pre>




<pre class="example">
  X1 X2
1  1  1
2  1  2
3  1  3
4  2  2
5  2  3
6  3  3
</pre>


<p>
We may interpret this experiment in a number of alternative ways. One way is to consider this as simply putting two 3-sided dice in a cup, shaking the cup, and looking inside &ndash; as in a game of <i>Liar's Dice</i>, for instance. Each row of the sample space is a potential pair we could observe. Another way is to view each outcome as a separate method to distribute two identical golf balls into three boxes labeled 1, 2, and 3. Regardless of the interpretation, <code>urnsamples</code> lists every possible way that the experiment can conclude.
</p>
<p>
Note that the urn does not need to contain numbers; we could have just as easily taken our urn to be <code>x = c("Red","Blue","Green")</code>. But, there is an \textbf{important} point to mention before proceeding. Astute readers will notice that in our example, the balls in the urn were \textit{distinguishable} in the sense that each had a unique label to distinguish it from the others in the urn. A natural question would be, ``What happens if your urn has indistinguishable elements, for example, what if <code>x = c("Red","Red","Blue")</code>?'' The answer is that <code>urnsamples</code> behaves as if each ball in the urn is distinguishable, regardless of its actual contents. We may thus imagine that while there are two red balls in the urn, the balls are such that we can tell them apart (in principle) by looking closely enough at the imperfections on their
surface.
</p>
<p>
In this way, when the <code>x</code> argument of <code>urnsamples</code> has repeated elements, the resulting sample space may appear to be <code>ordered = TRUE</code> even when, in fact, the call to the function was <code>urnsamples(..., ordered = FALSE)</code>. Similar remarks apply for the <code>replace</code> argument. 
</p>
</div>
</div>

</div>

<div id="outline-container-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Events </h3>
<div class="outline-text-3" id="text-3-2">

<p>\label{sec:Events}
</p>
<p>
An <i>event</i>\index{event} \(A\) is merely a collection of outcomes, or in other words, a subset of the sample space[fn:events]. After the performance of a random experiment \(E\) we say that the event \(A\) <i>occurred</i> if the experiment's outcome belongs to \(A\). We say that a bunch of events \(A_{1}\), \(A_{2}\), \(A_{3}\), &hellip; are <i>mutually exclusive</i>\index{mutually exclusive} or <i>disjoint</i> if \(A_{i}\cap A_{j}=\emptyset\) for any distinct pair \(A_{i}\neq A_{j}\). For instance, in the coin-toss experiment the events \( A = \{ \mbox{Heads} \}\) and \( B = \{ \mbox{Tails} \} \) would be mutually exclusive. Now would be a good time to review the algebra of sets in Appendix <a href="#sec-The-Algebra-of">The-Algebra-of</a>. 
</p>
<p>
[fn:events] This naive definition works for finite or countably infinite sample spaces, but is inadequate for sample spaces in general. In this book, we will not address the subtleties that arise, but will refer the interested reader to any text on advanced probability or measure theory.
</p>

<p>
Given a data frame sample/probability space <code>S</code>, we may extract rows using the <code>[]</code> operator: 
</p>



<pre class="example">S &lt;- tosscoin(2, makespace = TRUE) 
S[1:3, ] 

</pre>




<pre class="example">
  toss1 toss2 probs
1     H     H  0.25
2     T     H  0.25
3     H     T  0.25
</pre>





<pre class="example">S[c(2,4), ] 

</pre>




<pre class="example">
  toss1 toss2 probs
2     T     H  0.25
4     T     T  0.25
</pre>


<p>
and so forth. We may also extract rows that satisfy a logical expression using the <code>subset</code> function, for instance 
</p>



<pre class="example">S &lt;- cards() 

</pre>







<pre class="example">subset(S, suit == "Heart") 

</pre>







<pre class="example">   rank  suit
27    2 Heart
28    3 Heart
29    4 Heart
30    5 Heart
31    6 Heart
32    7 Heart
33    8 Heart
34    9 Heart
35   10 Heart
36    J Heart
37    Q Heart
38    K Heart
39    A Heart
</pre>




<pre class="example">subset(S, rank %in% 7:9)

</pre>





<pre class="example">   rank    suit
6     7    Club
7     8    Club
8     9    Club
19    7 Diamond
20    8 Diamond
21    9 Diamond
32    7   Heart
33    8   Heart
34    9   Heart
45    7   Spade
46    8   Spade
47    9   Spade
</pre>



<p>
We could continue indefinitely. Also note that mathematical expressions are allowed: 
</p>



<pre class="example">subset(rolldie(3), X1+X2+X3 &gt; 16) 

</pre>




<pre class="example">
    X1 X2 X3
180  6  6  5
210  6  5  6
215  5  6  6
216  6  6  6
</pre>



</div>

<div id="outline-container-3-2-1" class="outline-4">
<h4 id="sec-3-2-1"><span class="section-number-4">3.2.1</span> Functions for Finding Subsets </h4>
<div class="outline-text-4" id="text-3-2-1">


<p>
It does not take long before the subsets of interest become complicated to specify. Yet the main idea remains: we have a particular logical condition to apply to each row. If the row satisfies the condition, then it should be in the subset. It should not be in the subset otherwise. The ease with which the condition may be coded depends of course on the question being asked. Here are a few functions to get started.
</p>

<p>
The function <code>%in%</code> helps to learn whether each value of one vector lies somewhere inside another vector. 
</p>



<pre class="example">x &lt;- 1:10 
y &lt;- 8:12 
y %in% x

</pre>




<pre class="example">
[1]  TRUE  TRUE  TRUE FALSE FALSE
</pre>


<p>
Notice that the returned value is a vector of length 5 which tests whether each element of <code>y</code> is in <code>x</code>, in turn.
</p>

<p>
It is more common to want to know whether the <i>whole</i> vector <code>y</code> is in <code>x</code>. We can do this with the <code>isin</code> function. 
</p>



<pre class="example">isin(x,y) 

</pre>




<pre class="example">
[1] FALSE
</pre>


<p>
Of course, one may ask why we did not try something like <code>all(y %in% x)</code>, which would give a single result, <code>TRUE</code>. The reason is that the answers are different in the case that <code>y</code> has repeated values. Compare: 
</p>



<pre class="example">x &lt;- 1:10 
y &lt;- c(3,3,7) 

</pre>







<pre class="example">all(y %in% x)
isin(x,y) 

</pre>




<pre class="example">
[1] TRUE
[1] FALSE
</pre>


<p>
The reason for the above is of course that <code>x</code> contains the value 3, but <code>x</code> does not have <i>two</i> 3's. The difference is important when rolling multiple dice, playing cards, <i>etc</i>. Note that there is an optional argument <code>ordered</code> which tests whether the elements of <code>y</code> appear in <code>x</code> in the order in which they are appear in <code>y</code>. The consequences are 
</p>



<pre class="example">isin(x, c(3,4,5), ordered = TRUE) 
isin(x, c(3,5,4), ordered = TRUE) 

</pre>




<pre class="example">
[1] TRUE
[1] FALSE
</pre>


<p>
The connection to probability is that have a data frame sample space and we would like to find a subset of that space. A <code>data.frame</code> method was written for <code>isin</code> that simply applies the function to each row of the data frame. We can see the method in action with the following: 
</p>



<pre class="example">S &lt;- rolldie(4) 
subset(S, isin(S, c(2,2,6), ordered = TRUE)) 

</pre>





<pre class="example">     X1 X2 X3 X4
188   2  2  6  1
404   2  2  6  2
620   2  2  6  3
836   2  2  6  4
1052  2  2  6  5
1088  2  2  1  6
1118  2  1  2  6
1123  1  2  2  6
1124  2  2  2  6
1125  3  2  2  6
1126  4  2  2  6
1127  5  2  2  6
1128  6  2  2  6
1130  2  3  2  6
1136  2  4  2  6
1142  2  5  2  6
1148  2  6  2  6
1160  2  2  3  6
1196  2  2  4  6
1232  2  2  5  6
1268  2  2  6  6
</pre>



<p>
There are a few other functions written to find useful subsets, namely, <code>countrep</code> and <code>isrep</code>. Essentially these were written to test for (or count) a specific number of designated values in outcomes. See the documentation for details.
</p>
</div>

</div>

<div id="outline-container-3-2-2" class="outline-4">
<h4 id="sec-3-2-2"><span class="section-number-4">3.2.2</span> Set Union, Intersection, and Difference </h4>
<div class="outline-text-4" id="text-3-2-2">


<p>
Given subsets \(A\) and \(B\), it is often useful to manipulate them in an algebraic fashion. To this end, we have three set operations at our disposal: union, intersection, and difference. Below is a table that summarizes the pertinent information about these operations.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">Name</th><th scope="col" class="left">Denoted</th><th scope="col" class="left">Defined by elements</th><th scope="col" class="left">Code</th></tr>
</thead>
<tbody>
<tr><td class="left">Union</td><td class="left">\(A\cup B\)</td><td class="left">in \(A\) or \(B\) or both</td><td class="left"><code>union(A,B)</code></td></tr>
<tr><td class="left">Intersection</td><td class="left">\(A\cap B\)</td><td class="left">in both \(A\) and \(B\)</td><td class="left"><code>intersect(A,B)</code></td></tr>
<tr><td class="left">Difference</td><td class="left">\(A\textbackslash B\)</td><td class="left">in \(A\) but not in \(B\)</td><td class="left"><code>setdiff(A,B)</code></td></tr>
</tbody>
</table>



<p>
Some examples follow. 
</p>



<pre class="example">S &lt;- cards() 
A &lt;- subset(S, suit == "Heart") 
B &lt;- subset(S, rank %in% 7:9)

</pre>




<p>
We can now do some set algebra: 
</p>



<pre class="example">union(A,B)

</pre>







<pre class="example">   rank    suit
6     7    Club
7     8    Club
8     9    Club
19    7 Diamond
20    8 Diamond
21    9 Diamond
27    2   Heart
28    3   Heart
29    4   Heart
30    5   Heart
31    6   Heart
32    7   Heart
33    8   Heart
34    9   Heart
35   10   Heart
36    J   Heart
37    Q   Heart
38    K   Heart
39    A   Heart
45    7   Spade
46    8   Spade
47    9   Spade
</pre>




<pre class="example">intersect(A,B)

</pre>




<pre class="example">
   rank  suit
32    7 Heart
33    8 Heart
34    9 Heart
</pre>





<pre class="example">setdiff(A,B)

</pre>





<pre class="example">   rank  suit
27    2 Heart
28    3 Heart
29    4 Heart
30    5 Heart
31    6 Heart
35   10 Heart
36    J Heart
37    Q Heart
38    K Heart
39    A Heart
</pre>




<pre class="example">setdiff(B,A) 

</pre>







<pre class="example">   rank    suit
6     7    Club
7     8    Club
8     9    Club
19    7 Diamond
20    8 Diamond
21    9 Diamond
45    7   Spade
46    8   Spade
47    9   Spade
</pre>



<p>
Notice that <code>setdiff</code> is not symmetric. Further, note that we can calculate the <i>complement</i> of a set \(A\), denoted \(A^{c}\) and defined to be the elements of \(S\) that are not in \(A\) simply with <code>setdiff(S,A)</code>. There have been methods written for <code>intersect</code>, <code>setdiff</code>, <code>subset</code>, and <code>union</code> in the case that the input objects are of class <code>ps</code>. See Section <a href="#sub-howto-ps-objects">howto-ps-objects</a>.
</p>

<p>
When the <code>prob</code> package loads you will notice a message: ``\texttt{The following object(s) are masked from package:base: intersect, setdiff,}''. The reason for this message is that there already exist methods for the functions <code>intersect</code>, <code>setdiff</code>, <code>subset</code>, and <code>union</code> in the <code>base</code> package which ships with \textsf{R}. However, these methods were designed for when the arguments are vectors of the same mode. Since we are manipulating sample spaces which are data frames and lists, it was necessary to write methods to handle those cases as well. When the <code>prob</code> package is loaded, \textsf{R} recognizes that there are multiple versions of the same function in the search path and acts to shield the new definitions from the existing ones. But there is no cause for alarm, thankfully, because the <code>prob</code> functions have been carefully defined to match the usual <code>base</code> package definition in the case that the arguments are vectors. 
</p>

</div>
</div>

</div>

<div id="outline-container-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Model Assignment </h3>
<div class="outline-text-3" id="text-3-3">

<p>\label{sec:Interpreting-Probabilities}
</p>
<p>
Let us take a look at the coin-toss experiment more closely. What do we mean when we say ``the probability of Heads'' or write \(\P(\mbox{Heads})\)? Given a coin and an itchy thumb, how do we go about finding what \(\P(\mbox{Heads})\) should be?
</p>

</div>

<div id="outline-container-3-3-1" class="outline-4">
<h4 id="sec-3-3-1"><span class="section-number-4">3.3.1</span> The Measure Theory Approach </h4>
<div class="outline-text-4" id="text-3-3-1">


<p>
This approach states that the way to handle \(\P(\mbox{Heads})\) is to define a mathematical function, called a <i>probability measure</i>, on the sample space. Probability measures satisfy certain axioms (to be introduced later) and have special mathematical properties, so not just any mathematical function will do. But in any given physical circumstance there are typically all sorts of probability measures from which to choose, and it is left to the experimenter to make a reasonable choice &ndash; one usually based on considerations of objectivity. For the tossing coin example, a valid probability measure assigns probability \(p\) to the event \( \{ \mbox{Heads} \} \), where \(p\) is some number \(0\leq p\leq1\). An experimenter that wishes to incorporate the symmetry of the coin would choose \(p=1/2\) to balance the likelihood of \( \{\mbox{Heads} \} \) and \( \{ \mbox{Tails} \} \).
</p>
<p>
Once the probability measure is chosen (or determined), there is not much left to do. All assignments of probability are made by the probability function, and the experimenter needs only to plug the event \(\{ \mbox{Heads} \}\) into to the probability function to find \(\P(\mbox{Heads})\). In this way, the probability of an event is simply a calculated value, nothing more, nothing less. Of course this is not the whole story; there are many theorems and consequences associated with this approach that will keep us occupied for the remainder of this book. The approach is called <i>measure theory</i> because the measure (probability) of a set (event) is associated with how big it is (how likely it is to occur).
</p>
<p>
The measure theory approach is well suited for situations where there is symmetry to the experiment, such as flipping a balanced coin or spinning an arrow around a circle with well-defined pie slices. It is also handy because of its mathematical simplicity, elegance, and flexibility. There are literally volumes of information that one can prove about probability measures, and the cold rules of mathematics allow us to analyze intricate probabilistic problems with vigor. 
</p>
<p>
The large degree of flexibility is also a disadvantage, however. When symmetry fails it is not always obvious what an ``objective'' choice of probability measure should be; for instance, what probability should we assign to \( \{ \mbox{Heads} \} \) if we spin the coin rather than flip it? (It is not \(1/2\).) Furthermore, the mathematical rules are restrictive when we wish to incorporate subjective knowledge into the model, knowledge which changes over time and depends on the experimenter, such as personal knowledge about the properties of the specific coin being flipped, or of the person doing the flipping.
</p>
<p>
The mathematician who revolutionized this way to do probability theory was Andrey Kolmogorov, who published a landmark monograph in 1933. See <a href="http://www-history.mcs.st-andrews.ac.uk/Mathematicians/Kolmogorov.html">here</a> for more information.
</p>
</div>

</div>

<div id="outline-container-3-3-2" class="outline-4">
<h4 id="sec-3-3-2"><span class="section-number-4">3.3.2</span> Relative Frequency Approach </h4>
<div class="outline-text-4" id="text-3-3-2">


<p>
This approach states that the way to determine \(\P(\mbox{Heads})\) is to flip the coin repeatedly, in exactly the same way each time. Keep a tally of the number of flips and the number of Heads observed. Then a good approximation to \(\P(\mbox{Heads})\) will be
</p>


\begin{equation} 
\P(\mbox{Heads})\approx\frac{\mbox{number of observed Heads}}{\mbox{total number of flips}}.
\end{equation}


<p>
The mathematical underpinning of this approach is the celebrated <b>Law of Large Numbers</b> which may be loosely described as follows. Let \(E\) be a random experiment in which the event \(A\) either does or does not occur. Perform the experiment repeatedly, in an identical manner, in such a way that the successive experiments do not influence each other. After each experiment, keep a running tally of whether or not the event \(A\) occurred. Let \(S_{n}\) count the number of times that \(A\) occurred in the \(n\) experiments. Then the law of large numbers says that 
</p>


\begin{equation}
\frac{S_{n}}{n}\to\P(A)\mbox{ as }n\to\infty.
\end{equation}


<p>
As the reasoning goes, to learn about the probability of an event \(A\) we need only repeat the random experiment to get a reasonable estimate of the probability's value, and if we are not satisfied with our estimate then we may simply repeat the experiment more times all the while confident that with more and more experiments our estimate will stabilize to the true value. 
</p>
<p>
The frequentist approach is good because it is relatively light on assumptions and does not worry about symmetry or claims of objectivity like the measure-theoretic approach does. It is perfect for the spinning coin experiment. One drawback to the method is that one can never know the exact value of a probability, only a long-run approximation. It also does not work well with experiments that can not be repeated indefinitely, say, the probability that it will rain today, the chances that you get will get an A in your Statistics class, or the probability that the world is destroyed by nuclear war.
</p>
<p>
This approach was espoused by Richard von Mises in the early twentieth century, and some of his main ideas were incorporated into the measure theory approach. See <a href="http://www-history.mcs.st-andrews.ac.uk/Biographies/Mises.html">here</a> for more.
</p>
</div>

</div>

<div id="outline-container-3-3-3" class="outline-4">
<h4 id="sec-3-3-3"><span class="section-number-4">3.3.3</span> The Subjective Approach </h4>
<div class="outline-text-4" id="text-3-3-3">


<p>
The subjective approach interprets probability as the experimenter's <i>degree of belief</i> that the event will occur. The estimate of the probability of an event is based on the totality of the individual's knowledge at the time. As new information becomes available, the estimate is modified accordingly to best reflect his/her current knowledge. The method by which the probabilities are updated is commonly done with Bayes' Rule, discussed in Section <a href="#sec-Bayes--Rule">Bayes'-Rule</a>. 
</p>
<p>
So for the coin toss example, a person may have \(\P(\mbox{Heads})=1/2\) in the absence of additional information. But perhaps the observer knows additional information about the coin or the thrower that would shift the probability in a certain direction. For instance, parlor magicians may be trained to be quite skilled at tossing coins, and some are so skilled that they may toss a fair coin and get nothing but Heads, indefinitely. I have <i>seen</i> this. It was similarly claimed in <i>Bringing Down the House</i> \cite{Mezrich2003} that MIT students were accomplished enough with cards to be able to cut a deck to the same location, every single time. In such cases, one clearly should use the additional information to assign \(\P(\mbox{Heads})\) away from the symmetry value of \(1/2\).
</p>
<p>
This approach works well in situations that cannot be repeated indefinitely, for example, to assign your probability that you will get an A in this class, the chances of a devastating nuclear war, or the likelihood that a cure for the common cold will be discovered.
</p>
<p>
The roots of subjective probability reach back a long time. See <a href="http://en.wikipedia.org/wiki/Subjective_probability">here</a> for a short discussion and links to references about the subjective approach.
</p>
</div>

</div>

<div id="outline-container-3-3-4" class="outline-4">
<h4 id="sec-3-3-4"><span class="section-number-4">3.3.4</span> Equally Likely Model (ELM) </h4>
<div class="outline-text-4" id="text-3-3-4">


<p>
We have seen several approaches to the assignment of a probability model to a given random experiment and they are very different in their underlying interpretation. But they all cross paths when it comes to the equally likely model which assigns equal probability to all elementary outcomes of the experiment.
</p>
<p>
The ELM appears in the measure theory approach when the experiment boasts symmetry of some kind. If symmetry guarantees that all outcomes have equal ``size'', and if outcomes with equal ``size'' should get the same probability, then the ELM is a logical objective choice for the experimenter. Consider the balanced 6-sided die, the fair coin, or the dart board with equal-sized wedges.
</p>
<p>
The ELM appears in the subjective approach when the experimenter resorts to indifference or ignorance with respect to his/her knowledge of the outcome of the experiment. If the experimenter has no prior knowledge to suggest that (s)he prefer Heads over Tails, then it is reasonable for the him/her to assign equal subjective probability to both possible outcomes.
</p>
<p>
The ELM appears in the relative frequency approach as a fascinating fact of Nature: when we flip balanced coins over and over again, we observe that the proportion of times that the coin comes up Heads tends to \(1/2\). Of course if we assume that the measure theory applies then we can prove that the sample proportion must tend to 1/2 as expected, but that is putting the cart before the horse, in a manner of speaking.
</p>
<p>
The ELM is only available when there are finitely many elements in the sample space.
</p>

<p>
In the <code>prob</code> package, a probability space is an object of outcomes <code>S</code> and a vector of probabilities (called <code>probs</code>) with entries that correspond to each outcome in <code>S</code>. When <code>S</code> is a data frame, we may simply add a column called <code>probs</code> to <code>S</code> and we will be finished; the probability space will simply be a data frame which we may call <code>S</code>. In the case that S is a list, we may combine the <code>outcomes</code> and <code>probs</code> into a larger list, <code>space</code>; it will have two components: <code>outcomes</code> and <code>probs</code>. The only requirements we need are for the entries of <code>probs</code> to be nonnegative and <code>sum(probs)</code> to be one.
</p>
<p>
To accomplish this in \textsf{R}, we may use the <code>probspace</code> function. The general syntax is <code>probspace(x, probs)</code>, where <code>x</code> is a sample space of outcomes and <code>probs</code> is a vector (of the same length as the number of outcomes in <code>x</code>). The specific choice of <code>probs</code> depends on the context of the problem, and some examples follow to demonstrate some of the more common choices. 
</p>
<p>
The Equally Likely Model asserts that every outcome of the sample space has the same probability, thus, if a sample space has \(n\) outcomes, then <code>probs</code> would be a vector of length \(n\) with identical entries \(1/n\). The quickest way to generate <code>probs</code> is with the <code>rep</code> function. We will start with the experiment of rolling a die, so that \(n=6\). We will construct the sample space, generate the <code>probs</code> vector, and put them together with <code>probspace</code>. 
</p>



<pre class="example">outcomes &lt;- rolldie(1) 
p &lt;- rep(1/6, times = 6) 
probspace(outcomes, probs = p) 

</pre>




<pre class="example">
  X1     probs
1  1 0.1666667
2  2 0.1666667
3  3 0.1666667
4  4 0.1666667
5  5 0.1666667
6  6 0.1666667
</pre>


<p>
The <code>probspace</code> function is designed to save us some time in many of the most common situations. For example, due to the especial simplicity of the sample space in this case, we could have achieved the same result with only (note the name change for the first column) 
</p>



<pre class="example">probspace(1:6, probs = p) 

</pre>




<pre class="example">
  x     probs
1 1 0.1666667
2 2 0.1666667
3 3 0.1666667
4 4 0.1666667
5 5 0.1666667
6 6 0.1666667
</pre>


<p>
Further, since the equally likely model plays such a fundamental role in the study of probability the <code>probspace</code> function will assume that the equally model is desired if no <code>probs</code> are specified. Thus, we get the same answer with only 
</p>



<pre class="example">probspace(1:6) 

</pre>




<pre class="example">
  x     probs
1 1 0.1666667
2 2 0.1666667
3 3 0.1666667
4 4 0.1666667
5 5 0.1666667
6 6 0.1666667
</pre>


<p>
And finally, since rolling dice is such a common experiment in probability classes, the <code>rolldie</code> function has an additional logical argument <code>makespace</code> that will add a column of equally likely <code>probs</code> to the generated sample space: 
</p>



<pre class="example">rolldie(1, makespace = TRUE)

</pre>




<pre class="example">
  X1     probs
1  1 0.1666667
2  2 0.1666667
3  3 0.1666667
4  4 0.1666667
5  5 0.1666667
6  6 0.1666667
</pre>


<p>
\noindent or just <code>rolldie(1, TRUE)</code>. Many of the other sample space functions (<code>tosscoin</code>, <code>cards</code>, <code>roulette</code>, \textit{etc}.) have similar <code>makespace</code> arguments. Check the documentation for details.
</p>

<p>
One sample space function that does NOT have a <code>makespace</code> option is the <code>urnsamples</code> function. This was intentional. The reason is that under the varied sampling assumptions the outcomes in the respective sample spaces are NOT, in general, equally likely. It is important for the user to carefully consider the experiment to decide whether or not the outcomes are equally likely and then use <code>probspace</code> to assign the model.
</p>
<p>
\label{exa:unbalanced-coin}\textbf{An unbalanced coin.} While the <code>makespace</code> argument to <code>tosscoin</code> is useful to represent the tossing of a <i>fair</i> coin, it is not always appropriate. For example, suppose our coin is not perfectly balanced, for instance, maybe the ``\(H\)'' side is somewhat heavier such that the chances of a \(H\) appearing in a single toss is 0.70 instead of 0.5. We may set up the probability space with 
</p>



<pre class="example">probspace(tosscoin(1), probs = c(0.70, 0.30)) 

</pre>




<pre class="example">
  toss1 probs
1     H   0.7
2     T   0.3
</pre>


<p>
The same procedure can be used to represent an unbalanced die, roulette wheel, \textit{etc}.
</p>

</div>

</div>

<div id="outline-container-3-3-5" class="outline-4">
<h4 id="sec-3-3-5"><span class="section-number-4">3.3.5</span> Words of Warning </h4>
<div class="outline-text-4" id="text-3-3-5">


<p>
It should be mentioned that while the splendour of \textsf{R} is uncontested,  it, like everything else, has limits both with respect to the sample/probability spaces it can manage and with respect to the finite accuracy of the representation of most numbers (see the \textsf{R} FAQ 7.31). When playing around with probability, one may be tempted to set up a probability space for tossing 100 coins or rolling 50 dice in an attempt to answer some scintillating question. (Bear in mind: rolling a die just 9 times has a sample space with over <i>10 million</i> outcomes.)
</p>
<p>
Alas, even if there were enough RAM to barely hold the sample space (and there were enough time to wait for it to be generated), the infinitesimal probabilities that are associated with <i>so many</i> outcomes make it difficult for the underlying machinery to handle reliably. In some cases, special algorithms need to be called just to give something
that holds asymptotically. User beware.
</p>
</div>
</div>

</div>

<div id="outline-container-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> Properties of Probability </h3>
<div class="outline-text-3" id="text-3-4">

<p>\label{sec:Properties-of-Probability}
</p>

</div>

<div id="outline-container-3-4-1" class="outline-4">
<h4 id="sec-3-4-1"><span class="section-number-4">3.4.1</span> Probability Functions </h4>
<div class="outline-text-4" id="text-3-4-1">

<p>\label{sub:Probability-Functions}
</p>
<p>
A <i>probability function</i> is a rule that associates with each event \(A\) of the sample space a unique number \(\P(A)=p\), called the <i>probability of</i> \(A\). Any probability function \(\P\) satisfies the following three Kolmogorov Axioms: 
</p>


\begin{ax}
\label{ax:prob-nonnegative}$\P(A)\geq0$ for any event $A\subset S$.
\end{ax}

\begin{ax}
\label{ax:total-mass-one}$\P(S)=1$.
\end{ax}

\begin{ax}
\label{ax:countable-additivity}If the events $A_{1}$, $A_{2}$,
$A_{3}$... are disjoint then

\begin{equation}
\P\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\P(A_{i})\mbox{ for every }n,
\end{equation}

and furthermore,

\begin{equation}
\P\left(\bigcup_{i=1}^{\infty}A_{i}\right)=\sum_{i=1}^{\infty}\P(A_{i}).
\end{equation}

\end{ax}

<p>
The intuition behind the axioms goes like this: first, the probability of an event should never be negative. Second, since the sample space contains all possible outcomes, its probability should be one, or 100%. The last axiom may look intimidating but it simply means that in a sequence of disjoint events (in other words, sets that do not overlap), the total probability (measure) should equal the sum of its parts. For example, the chance of rolling a 1 or a 2 on a die should be the chance of rolling a 1 plus the chance of rolling a 2.
</p>
</div>

</div>

<div id="outline-container-3-4-2" class="outline-4">
<h4 id="sec-3-4-2"><span class="section-number-4">3.4.2</span> Properties </h4>
<div class="outline-text-4" id="text-3-4-2">


<p>
For any events \(A\) and \(B\),
</p>
<ol>
<li>\(\P(A^{c})=1-\P(A)\).\label{enu:prop-prob-complement} 

<p>
  \begin{proof}
  Since $A\cup A^{c}=S$ and $A\cap A^{c}=\emptyset$, we have
  \[
  1=\P(S)=\P(A\cup A^{c})=\P(A)+\P(A^{c}).
  \]
  \end{proof}

</p></li>
<li>\(\P(\emptyset)=0\).

<p>
  \begin{proof}
  Note that $\emptyset=S^{c}$, and use Property 1.
  \end{proof}

</p></li>
<li>If \(A\subset B\) , then \(\P(A)\leq\P(B)\).

<p>
  \begin{proof}
  Write $B=A\cup\left(B\cap A^{c}\right)$, and notice that $A\cap\left(B\cap A^{c}\right)=\emptyset$; thus
  \[
  \P(B)=\P(A\cup\left(B\cap A^{c}\right))=\P(A)+\P\left(B\cap A^{c}\right)\geq\P(A),
  \]
  since $\P\left(B\cap A^{c}\right)\ge0$. 
  \end{proof}

</p></li>
<li>\(0\leq\P(A)\leq1\).

<p>
  \begin{proof}
  The left inequality is immediate from Axiom \ref{ax:prob-nonnegative}, and the second inequality follows from Property 3 since $A\subset S$.
  \end{proof}

</p></li>
<li><b>The General Addition Rule.</b>

<p>
  \begin{equation}
  \P(A\cup B)=\P(A)+\P(B)-\P(A\cap B).\label{eq:general-addition-rule-1}
  \end{equation}
  More generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),&hellip;, \(A_{n}\),
</p>
<p>
  \begin{equation}
  \P\left(\bigcup_{i=1}^{n}A_{i}\right)=\sum_{i=1}^{n}\P(A_{i})-\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\P(A_{i}\cap A_{j})+\cdots+(-1)^{n-1}\P\left(\bigcap_{i=1}^{n}A_{i}\right)
  \end{equation}

</p></li>
<li><b>The Theorem of Total Probability.</b> 

<p>
  Let \(B_{1}\), \(B_{2}\), &hellip;, \(B_{n}\) be mutually exclusive and exhaustive. Then
  \begin{equation}
  \P(A)=\P(A\cap B_{1})+\P(A\cap B_{2})+\cdots+\P(A\cap B_{n}).\label{eq:theorem-total-probability}
  \end{equation}
</p></li>
</ol>


</div>

</div>

<div id="outline-container-3-4-3" class="outline-4">
<h4 id="sec-3-4-3"><span class="section-number-4">3.4.3</span> Assigning Probabilities </h4>
<div class="outline-text-4" id="text-3-4-3">


<p>
A model of particular interest is the <i>equally likely model</i>. The idea is to divide the sample space \(S\) into a finite collection of elementary events \( \{ a_{1},\ a_{2}, \ldots, a_{N} \} \) that are equally likely in the sense that each \(a_{i}\) has equal chances of occurring. The probability function associated with this model must satisfy \(\P(S)=1\), by Axiom 2. On the other hand, it must also satisfy
</p>


\[
\P(S)=\P( \{ a_{1},\ a_{2},\ldots,a_{N} \} )=\P(a_{1}\cup a_{2}\cup\cdots\cup a_{N})=\sum_{i=1}^{N}\P(a_{i}),
\]

<p>
by Axiom 3. Since \(\P(a_{i})\) is the same for all \(i\), each one necessarily equals \(1/N\). 
</p>
<p>
For an event \(A\subset S\), we write \(A\) as a collection of elementary outcomes: if \( A = \{ a_{i_{1}}, a_{i_{2}}, \ldots, a_{i_{k}} \} \) then \(A\) has \(k\) elements and 
</p>


\begin{align*}
\P(A) & =\P(a_{i_{1}})+\P(a_{i_{2}})+\cdots+\P(a_{i_{k}}),\\
 & =\frac{1}{N}+\frac{1}{N}+\cdots+\frac{1}{N},\\  & =\frac{k}{N}=\frac{\#(A)}{\#(S)}.
\end{align*}


<p>
In other words, under the equally likely model, the probability of an event \(A\) is determined by the number of elementary events that \(A\) contains. 
</p>
<p>
Consider the random experiment \(E\) of tossing a coin. Then the sample space is \(S=\{H,T\}\), and under the equally likely model, these two outcomes have \(\P(H)=\P(T)=1/2\). This model is taken when it is reasonable to assume that the coin is fair.
</p>
<p>
Suppose the experiment \(E\) consists of tossing a fair coin twice. The sample space may be represented by \(S=\{HH,\, HT,\, TH,\, TT\}\). Given that the coin is fair and that the coin is tossed in an independent and identical manner, it is reasonable to apply the equally likely model. 
</p>
<p>
What is \(\P(\mbox{at least 1 Head})\)? Looking at the sample space we see the elements \(HH\), \(HT\), and \(TH\) have at least one Head; thus, \(\P(\mbox{at least 1 Head})=3/4\). 
</p>
<p>
What is \(\P(\mbox{no Heads})\)? Notice that the event \(\{ \mbox{no Heads} \} = \{ \mbox{at least one Head} \} ^{c}\), which by Property <a href="#enu-prop-prob-complement">prop-prob-complement</a> means \(\P(\mbox{no Heads})=1-\P(\mbox{at least one head})=1-3/4=1/4\). It is obvious in this simple example that the only outcome with no Heads is \(TT\), however, this complementation trick can be handy in more complicated problems.
</p>
<p>
\label{exa:three-child-family}
Imagine a three child family, each child being either Boy (\(B\)) or Girl (\(G\)). An example sequence of siblings would be \(BGB\). The sample space may be written
</p>


\[
S=\left\{ 
\begin{array}{cccc}
BBB, & BGB, & GBB, & GGB,\\
BBG, & BGG, & GBG, & GGG
\end{array}
\right\}.
\]

<p>
Note that for many reasons (for instance, it turns out that girls are slightly more likely to be born than boys), this sample space is <i>not</i> equally likely. For the sake of argument, however, we will assume that the elementary outcomes each have probability \(1/8\).
</p>
<p>
What is \(\P(\mbox{exactly 2 Boys})\)? Inspecting the sample space reveals three outcomes with exactly two boys: \( \{ BBG,\, BGB,\, GBB \} \).  Therefore \(\P(\mbox{exactly 2 Boys})=3/8\). 
</p>
<p>
What is \(\P(\mbox{at most 2 Boys})\)? One way to solve the problem would be to count the outcomes that have 2 or less Boys, but a quicker way would be to recognize that the only way that the event \(\{ \mbox{at most 2 Boys} \}\) does <i>not</i> occur is the event \(\{ \mbox{all Girls} \}\).
</p>
<p>
Thus
\[
\P(\mbox{at most 2 Boys})=1-\P(GGG)=1-1/8=7/8.
\]


Consider the experiment of rolling a six-sided die, and let the outcome be the face showing up when the die comes to rest. Then \( S = \{ 1,\,2,\,3,\,4,\,5,\,6 \} \). It is usually reasonable to suppose that the die is fair, so that the six outcomes are equally likely.
</p>
<p>
Consider a standard deck of 52 cards. These are usually labeled with the four <i>suits</i>: Clubs, Diamonds, Hearts, and Spades, and the 13 <i>ranks</i>: 2, 3, 4, &hellip;, 10, Jack (J), Queen (Q), King (K), and Ace (A). Depending on the game played, the Ace may be ranked below 2 or above King. 
</p>
<p>
Let the random experiment \(E\) consist of drawing exactly one card from a well-shuffled deck, and let the outcome be the face of the card. Define the events \( A = \{ \mbox{draw an Ace} \} \) and \( B = \{ \mbox{draw a Club} \} \). Bear in mind: we are only drawing one card.
</p>
<p>
Immediately we have \(\P(A)=4/52\) since there are four Aces in the deck; similarly, there are \(13\) Clubs which implies \(\P(B)=13/52\).
</p>
<p>
What is \(\P(A\cap B)\)? We realize that there is only one card of the 52 which is an Ace and a Club at the same time, namely, the Ace of Clubs. Therefore \(\P(A\cap B)=1/52\).
</p>
<p>
To find \(\P(A\cup B)\) we may use the above with the General Addition Rule to get
</p>


\begin{eqnarray*}
\P(A\cup B) & = & \P(A)+\P(B)-\P(A\cap B),\\
 & = & 4/52+13/52-1/52,\\
 & = & 16/52.
\end{eqnarray*}


<p>
Staying with the deck of cards, let another random experiment be the selection of a five card stud poker hand, where ``five card stud'' means that we draw exactly five cards from the deck without replacement, no more, and no less. It turns out that the sample space \(S\) is so large and complicated that we will be obliged to settle for the trivial description \( S = \{ \mbox{all possible 5 card hands} \} \) for the time being. We will have a more precise description later.
</p>
<p>
What is \(\P(\mbox{Royal Flush})\), or in other words, \(\P(\mbox{A, K, Q, J, 10 all in the same suit})\)? 
</p>
<p>
It should be clear that there are only four possible royal flushes. Thus, if we could only count the number of outcomes in \(S\) then we could simply divide four by that number and we would have our answer under the equally likely model. This is the subject of Section <a href="#sec-Methods-of-Counting">Methods-of-Counting</a>.
</p>


<p>
Probabilities are calculated in the <code>prob</code> package with the <code>prob</code> function.
</p>
<p>
Consider the experiment of drawing a card from a standard deck of playing cards. Let's denote the probability space associated with the experiment as <code>S</code>, and let the subsets <code>A</code> and <code>B</code> be defined by the following: 
</p>



<pre class="example">S &lt;- cards(makespace = TRUE) 
A &lt;- subset(S, suit == "Heart") 
B &lt;- subset(S, rank %in% 7:9)

</pre>




<p>
Now it is easy to calculate 
</p>



<pre class="example">prob(A) 

</pre>





<p>
Note that we can get the same answer with 
</p>



<pre class="example">prob(S, suit == "Heart") 

</pre>





<p>
We also find <code>prob(B) = 0.23</code> (listed here approximately, but 12/52 actually) and <code>prob(S) = 1</code>. Internally, the <code>prob</code> function operates by summing the <code>probs</code> column of its argument. It will find subsets on-the-fly if desired.
</p>
<p>
We have as yet glossed over the details. More specifically, <code>prob</code> has three arguments: <code>x</code>, which is a probability space (or a subset of one), <code>event</code>, which is a logical expression used to define a subset, and <code>given</code>, which is described in Section <a href="#sec-Conditional-Probability">Conditional-Probability</a>.
</p>
<p>
<i>WARNING</i>. The <code>event</code> argument is used to define a subset of <code>x</code>, that is, the only outcomes used in the probability calculation will be those that are elements of <code>x</code> and satisfy <code>event</code> simultaneously. In other words, <code>prob(x, event)</code> calculates 
</p>
<pre class="example">
prob(intersect(x, subset(x, event)))
</pre>


<p>
Consequently, <code>x</code> should be the entire probability space in the case that <code>event</code> is non-null.
</p>
</div>
</div>

</div>

<div id="outline-container-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> Counting Methods </h3>
<div class="outline-text-3" id="text-3-5">

<p>\label{sec:Methods-of-Counting}
</p>
<p>
The equally-likely model is a convenient and popular way to analyze random experiments. And when the equally likely model applies, finding the probability of an event \(A\) amounts to nothing more than counting the number of outcomes that \(A\) contains (together with the number of events in \(S\)). Hence, to be a master of probability one must be skilled at counting outcomes in events of all kinds.
</p>
<p>
The Multiplication Principle. Suppose that an experiment is composed of two successive steps. Further suppose that the first step may be performed in \(n_{1}\) distinct ways while the second step may be performed in \(n_{2}\) distinct ways. Then the experiment may be performed in \(n_{1}n_{2}\) distinct ways.
</p>
<p>
More generally, if the experiment is composed of \(k\) successive steps which may be performed in \(n_{1}\), \(n_{2}\), &hellip;, \(n_{k}\) distinct ways, respectively, then the experiment may be performed in \(n_{1}n_{2}\cdots n_{k}\) distinct ways.
</p>
<p>
We would like to order a pizza. It will be sure to have cheese (and marinara sauce), but we may elect to add one or more of the following five (5) available toppings:
\[
\mbox{pepperoni, sausage, anchovies, olives, and green peppers.}
\]
How many distinct pizzas are possible?
</p>
<p>
There are many ways to approach the problem, but the quickest avenue employs the Multiplication Principle directly. We will separate the action of ordering the pizza into a series of stages. At the first stage, we will decide whether or not to include pepperoni on the pizza (two possibilities). At the next stage, we will decide whether or not to include sausage on the pizza (again, two possibilities). We will continue in this fashion until at last we will decide whether or not to include green peppers on the pizza.
</p>
<p>
At each stage we will have had two options, or ways, to select a pizza to be made. The Multiplication Principle says that we should multiply the 2's to find the total number of possible pizzas: \(2\cdot2\cdot2\cdot2\cdot2=2^{5}=32\).
</p>

<p>
We would like to buy a desktop computer to study statistics. We go to a website to build our computer our way. Given a line of products we have many options to customize our computer. In particular, there are 2 choices for a processor, 3 different operating systems, 4 levels of memory, 4 hard drives of differing sizes, and 10 choices for a monitor. How many possible types of computer must the company be prepared to build? <b>Answer:</b> \(2\cdot3\cdot4\cdot4\cdot10=960\)
</p>



</div>

<div id="outline-container-3-5-1" class="outline-4">
<h4 id="sec-3-5-1"><span class="section-number-4">3.5.1</span> Ordered Samples </h4>
<div class="outline-text-4" id="text-3-5-1">


<p>
Imagine a bag with \(n\) distinguishable balls inside. Now shake up the bag and select \(k\) balls at random. How many possible sequences might we observe?
</p>
<p>
The number of ways in which one may select an ordered sample of \(k\) subjects from a population that has \(n\) distinguishable members is
</p>
<ul>
<li>\(n^{k}\) if sampling is done with replacement,
</li>
<li>\(n(n-1)(n-2)\cdots(n-k+1)\) if sampling is done without replacement.
</li>
</ul>



<p>
Recall from calculus the notation for <i>factorials</i>: 
</p>


\begin{eqnarray*}
1! & = & 1,\\
2! & = & 2\cdot1=2,\\
3! & = & 3\cdot2\cdot1=6,\\
 & \vdots\\
n! & = & n(n-1)(n-2)\cdots3\cdot2\cdot1.
\end{eqnarray*}

<p>
The number of permutations of \(n\) elements is \(n!\).
</p>
<p>
Take a coin and flip it 7 times. How many sequences of Heads and Tails are possible? <b>Answer:</b> \(2^{7}=128\).
</p>
<p>
In a class of 20 students, we randomly select a class president, a class vice-president, and a treasurer. How many ways can this be done? <b>Answer:</b> \(20\cdot19\cdot18=6840\).
</p>
<p>
We rent five movies to watch over the span of two nights. We wish to watch 3 movies on the first night. How many distinct sequences of 3 movies could we possibly watch? <b>Answer:</b> \(5\cdot4\cdot3=60\).
</p>


</div>

</div>

<div id="outline-container-3-5-2" class="outline-4">
<h4 id="sec-3-5-2"><span class="section-number-4">3.5.2</span> Unordered Samples </h4>
<div class="outline-text-4" id="text-3-5-2">


<p>
The number of ways in which one may select an unordered sample of \(k\) subjects from a population that has \(n\) distinguishable members is
</p><ul>
<li>\((n-1+k)!/[(n-1)!k!]\) if sampling is done with replacement,
</li>
<li>\(n!/[k!(n-k)!]\) if sampling is done without replacement.
</li>
</ul>


<p>
The quantity \(n!/[k!(n-k)!]\) is called a <i>binomial coefficient</i> and plays a special role in mathematics; it is denoted
\begin{equation}
{n \choose k}=\frac{n!}{k!(n-k)!}\label{eq:binomial-coefficient}
\end{equation}
and is read ``\(n\) choose \(k\)''.
</p>
<p>
You rent five movies to watch over the span of two nights, but only wish to watch 3 movies the first night. Your friend, Fred, wishes to borrow some movies to watch at his house on the first night. You owe Fred a favor, and allow him to select 2 movies from the set of 5. How many choices does Fred have? \textbf{Answer:} \({5 \choose 2}=10\).
</p>
<p>
Place 3 six-sided dice into a cup. Next, shake the cup well and pour out the dice. How many distinct rolls are possible? \textbf{Answer:} \((6-1+3)!/[(6-1)!3!]={8 \choose 5}=56\). 
</p>


<p>
The factorial \(n!\) is computed with the command <code>factorial(n)</code> and the binomial coefficient \({n \choose k}\) with the command <code>choose(n,k)</code>.
</p>
<p>
The sample spaces we have computed so far have been relatively small, and we can visually study them without much trouble. However, it is <i>very</i> easy to generate sample spaces that are prohibitively large. And while \textsf{R} is wonderful and powerful and does almost everything except wash windows, even \textsf{R} has limits of which we should be mindful.
</p>
<p>
But we often do not need to actually generate the sample space; it suffices to count the number of outcomes. The <code>nsamp</code> function will calculate the number of rows in a sample space made by <code>urnsamples</code> without actually devoting the memory resources necessary to generate the space. The arguments are <code>n</code>, the number of (distinguishable) objects in the urn, <code>k</code>, the sample size, and <code>replace</code>, <code>ordered</code>, as above.
</p>

<table  border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides" id="tab-Sampling-k-from-n">
<caption>Sampling $k$ from $n$ objects with \texttt{urnsamples}</caption>
<colgroup><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left"></th><th scope="col" class="left"><code>ordered = TRUE</code></th><th scope="col" class="left"><code>ordered = FALSE</code></th></tr>
</thead>
<tbody>
<tr><td class="left"><code>replace = TRUE</code></td><td class="left">\(n^{k}\)</td><td class="left">\((n-1+k)! / [(n-1)!k!]\)</td></tr>
<tr><td class="left"><code>replace = FALSE</code></td><td class="left">\( n! / (n-k)! \)</td><td class="left">\( {n \choose k} \)</td></tr>
</tbody>
</table>



<p>
We will compute the number of outcomes for each of the four <code>urnsamples</code> examples that we saw in Example <a href="#exa-sample-urn-two-from-three">sample-urn-two-from-three</a>. Recall that we took a sample of size two from an urn with three distinguishable elements.
</p>




<pre class="example">nsamp(n=3, k=2, replace = TRUE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = TRUE) 
nsamp(n=3, k=2, replace = FALSE, ordered = FALSE) 
nsamp(n=3, k=2, replace = TRUE, ordered = FALSE) 

</pre>




<pre class="example">
[1] 9
[1] 6
[1] 3
[1] 6
</pre>


<p>
Compare these answers with the length of the data frames generated above.
</p>


<p>
A benefit of <code>nsamp</code> is that it is <i>vectorized</i> so that entering vectors instead of numbers for <code>n</code>, <code>k</code>, <code>replace</code>, and <code>ordered</code> results in a vector of corresponding answers. This becomes particularly convenient for combinatorics problems.
</p>
<p>
There are 11 artists who each submit a portfolio containing 7 paintings for competition in an art exhibition. Unfortunately, the gallery director only has space in the winners' section to accommodate 12 paintings in a row equally spread over three consecutive walls. The director decides to give the first, second, and third place winners each a wall to display the work of their choice. The walls boast 31 separate lighting options apiece. How many displays are possible?
</p>
<p>
<b>Answer:</b> The judges will pick 3 (ranked) winners out of 11 (with <code>rep = FALSE</code>, <code>ord = TRUE</code>). Each artist will select 4 of his/her paintings from 7 for display in a row (<code>rep = FALSE</code>, <code>ord = TRUE</code>), and lastly, each of the 3 walls has 31 lighting possibilities (<code>rep = TRUE</code>, <code>ord = TRUE</code>). These three numbers can be calculated quickly with 
</p>



<pre class="example">n &lt;- c(11,7,31) 
k &lt;- c(3,4,3) 
r &lt;- c(FALSE,FALSE,TRUE) 

</pre>







<pre class="example">x &lt;- nsamp(n, k, rep = r, ord = TRUE) 

</pre>




<p>
(Notice that <code>ordered</code> is always <code>TRUE</code>; <code>nsamp</code> will recycle <code>ordered</code> and <code>replace</code> to the appropriate length.) By the Multiplication Principle, the number of ways to complete the experiment is the product of the entries of <code>x</code>: 
</p>



<pre class="example">prod(x) 

</pre>




<pre class="example">
[1] 24774195600
</pre>


<p>
Compare this with the some other ways to compute the same thing: 
</p>



<pre class="example">(11*10*9)*(7*6*5*4)*313 

</pre>




<pre class="example">
[1] 260290800
</pre>


<p>
or alternatively 
</p>



<pre class="example">prod(9:11)*prod(4:7)*313 

</pre>




<pre class="example">
[1] 260290800
</pre>


<p>
or even 
</p>



<pre class="example">prod(factorial(c(11,7))/factorial(c(8,3)))*313 

</pre>




<pre class="example">
[1] 260290800
</pre>



<p>
As one can guess, in many of the standard counting problems there aren't substantial savings in the amount of typing; it is about the same using <code>nsamp</code> versus <code>factorial</code> and <code>choose</code>. But the virtue of <code>nsamp</code> lies in its collecting the relevant counting formulas in a one-stop shop. Ultimately, it is up to the user to choose the method that works best for him/herself. 
</p>
<p>
<b>The Birthday Problem.</b> Suppose that there are \(n\) people together in a room. Each person announces the date of his/her birthday in turn. The question is: what is the probability of at least one match? If we let the event \(A\) represent \(\{ \mbox{there is at least one match} \}\), then would like to know \(\P(A)\), but as we will see, it is more convenient to calculate \(\P(A^{c})\).
</p>
<p>
For starters we will ignore leap years and assume that there are only 365 days in a year. Second, we will assume that births are equally distributed over the course of a year (which is not true due to all sorts of complications such as hospital delivery schedules). See <a href="http://en.wikipedia.org/wiki/Birthday_problem">here</a> for more.
</p>
<p>
Let us next think about the sample space. There are 365 possibilities for the first person's birthday, 365 possibilities for the second, and so forth. The total number of possible birthday sequences is therefore \(\#(S)=365^{n}\).
</p>
<p>
Now we will use the complementation trick we saw in Example <a href="#exa-three-child-family">three-child-family</a>. We realize that the only situation in which \(A\) does <i>not</i> occur is if there are <i>no</i> matches among all people in the room, that is, only when everybody's birthday is different, so
</p>


\[
\P(A)=1-\P(A^{c})=1-\frac{\#(A^{c})}{\#(S)},
\]

<p>
since the outcomes are equally likely. Let us then suppose that there are no matches. The first person has one of 365 possible birthdays. The second person must not match the first, thus, the second person has only 364 available birthdays from which to choose. Similarly, the third person has only 363 possible birthdays, and so forth, until we reach the \(n^{\mathrm{th}}\) person, who has only \(365-n+1\) remaining possible days for a birthday. By the Multiplication Principle, we have \(\#(A^{c})=365\cdot364\cdots(365-n+1)\), and
</p>


\begin{equation}
\P(A)=1-\frac{365\cdot364\cdots(365-n+1)}{365^{n}}=1-\frac{364}{365}\cdot\frac{363}{365}\cdots\frac{(365-n+1)}{365}.
\end{equation}

<p>
As a surprising consequence, consider this: how many people does it take to be in the room so that the probability of at least one match is at least 0.50? Clearly, if there is only \(n=1\) person in the room then the probability of a match is zero, and when there are \(n=366\) people in the room there is a 100% chance of a match (recall that we are ignoring leap years). So how many people does it take so that there is an equal chance of a match and no match?
</p>
<p>
When I have asked this question to students, the usual response is ``somewhere around \(n=180\) people'' in the room. The reasoning seems to be that in order to get a 50\% chance of a match, there should be 50% of the available days to be occupied. The number of students in a typical classroom is 25, so as a companion question I ask students to estimate the probability of a match when there are \(n=25\) students in the room. Common estimates are a 1%, or 0.5%, or even 0.1% chance of a match. After they have given their estimates, we go around the room and each student announces their birthday. More often than not, we observe a match in the class, to the students' disbelief.
</p>
<p>
Students are usually surprised to hear that, using the formula above, one needs only \(n=23\) students to have a greater than 50\% chance of at least one match. Figure <a href="#fig-birthday">birthday</a> shows a graph of the birthday probabilities:
</p>







<p>
We can make the plot in Figure <a href="#fig-The-Birthday-Problem">The-Birthday-Problem</a> with the following sequence of commands.
</p>



<pre class="example">g &lt;- Vectorize(pbirthday.ipsur)
plot(1:50, g(1:50), xlab = "Number of people in room", 
  ylab = "Prob(at least one match)" )
abline(h = 0.5)
abline(v = 23, lty = 2)
remove(g)

</pre>




<p>
There is a \textsf{Birthday problem} item in the \textsf{Probability} menu of <code>RcmdrPlugin.IPSUR</code>. In the base \textsf{R} version, one can compute approximate probabilities for the more general case of probabilities other than 1/2, for differing total number of days in the year, and even for more than two matches.
</p>

</div>
</div>

</div>

<div id="outline-container-3-6" class="outline-3">
<h3 id="sec-3-6"><span class="section-number-3">3.6</span> Conditional Probability </h3>
<div class="outline-text-3" id="text-3-6">

<p>\label{sec:Conditional-Probability}
</p>
<p>
Consider a full deck of 52 standard playing cards. Now select two cards from the deck, in succession. Let \( A = \{ \mbox{first card drawn is an Ace} \} \) and \( B = \{ \mbox{second card drawn is an Ace} \} \). Since there are four Aces in the deck, it is natural to assign \( \P(A) = 4/52 \). Suppose we look at the first card. What now is the probability of \(B\)? Of course, the answer depends on the value of the first card. If the first card is an Ace, then the probability that the second also is an Ace should be \( 3/51 \), but if the first card is not an Ace, then the probability that the second is an Ace should be \( 4/51 \). As notation for these two situations we write
\[
\P(B|A)=3/51,\quad\P(B|A^{c})=4/51.
\]

The conditional probability of \(B\) given \(A\), denoted \(\P(B|A)\), is defined by
\begin{equation}
\P(B|A)=\frac{\P(A\cap B)}{\P(A)},\quad\mbox{if }\P(A)>0.
\end{equation}
We will not be discussing a conditional probability of \(B\) given \(A\) when \(\P(A)=0\), even though this theory exists, is well developed, and forms the foundation for the study of stochastic processes[fn:condexp]. 
</p>
<p>
[fn:condexp] Conditional probability in this case is defined by means of <i>conditional expectation</i>, a topic that is well beyond the scope of this text. The interested reader should consult an advanced text on probability theory, such as Billingsley, Resnick, or Ash Dooleans-Dade.
</p>

<p>
Toss a coin twice. The sample space is given by $S=\{ HH,\ HT,\ TH,\ TT \} $. Let $A= \{ \mbox{a head occurs} \} $ and $B= \{ \mbox{a head and tail occur} \} $. It should be clear that \(\P(A)=3/4\), \(\P(B)=2/4\), and \(\P(A\cap B)=2/4\). What now are the probabilities \(\P(A|B)\) and \(\P(B|A)\)?
\[
\P(A|B)=\frac{\P(A\cap B)}{\P(B)}=\frac{2/4}{2/4}=1,
\]
in other words, once we know that a Head and Tail occur, we may be certain that a Head occurs. Next
\[
\P(B|A)=\frac{\P(A\cap B)}{\P(A)}=\frac{2/4}{3/4}=\frac{2}{3},
\]
which means that given the information that a Head has occurred, we no longer need to account for the outcome \(TT\), and the remaining three outcomes are equally likely with exactly two outcomes lying in the set \(B\). 
</p>
<p>
\label{exa:Toss-a-six-sided-die-twice}
Toss a six-sided die twice. The sample space consists of all ordered pairs \((i,j)\) of the numbers \(1,2,\ldots,6\), that is, \( S = \{ (1,1),\ (1,2),\ldots,(6,6) \} \). We know from Section <a href="#sec-Methods-of-Counting">Methods-of-Counting</a> that \( \# (S) = 6^{2} = 36 \). Let \( A = \{ \mbox{outcomes match} \} \) and \( B = \{ \mbox{sum of outcomes at least 8} \} \). The sample space may be represented by a matrix:
</p>



\begin{table}
\begin{tabular}{c}
\begin{sideways}
First Roll
\end{sideways}\tabularnewline
\end{tabular}\begin{tabular}{c|cccccc|}
\multicolumn{1}{c}{} & \multicolumn{6}{c}{Second Roll}\tabularnewline
\multicolumn{1}{c}{} & 1 & 2 & 3 & 4 & 5 & \multicolumn{1}{c}{6}\tabularnewline
\cline{2-7} 
1 & $\varprod$ &  &  &  &  & \tabularnewline
2 &  & $\varprod$ &  &  &  & {\Large $\bigcirc$}\tabularnewline
3 &  &  & $\varprod$ &  & {\Large $\bigcirc$} & {\Large $\bigcirc$}\tabularnewline
4 &  &  &  & {\huge $\otimes$} & {\Large $\bigcirc$} & {\Large $\bigcirc$}\tabularnewline
5 &  &  & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\huge $\otimes$} & {\Large $\bigcirc$}\tabularnewline
6 &  & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\Large $\bigcirc$} & {\huge $\otimes$}\tabularnewline
\cline{2-7} 
\end{tabular}
\caption{Rolling two dice\label{tab:Rolling-two-dice}}
\end{table}


<p>
The outcomes lying in the event \(A\) are marked with the symbol ``\(\varprod\)'', the outcomes falling in \(B\) are marked with ``\(\bigcirc\)'', and those in both \(A\) and \(B\) are marked ``\(\otimes\)''. Now it is clear that \(\P(A)=6/36\), \(\P(B)=15/36\), and \(\P(A\cap B)=3/36\).  Finally, 
</p>


\[
\P(A|B)=\frac{3/36}{15/36}=\frac{1}{5},\quad\P(B|A)=\frac{3/36}{6/36}=\frac{1}{2}.
\]

<p>
Again, we see that given the knowledge that \(B\) occurred (the 15 outcomes in the lower right triangle), there are 3 of the 15 that fall into the set \(A\), thus the probability is \(3/15\). Similarly, given that \(A\) occurred (we are on the diagonal), there are 3 out of 6 outcomes that also fall in \(B\), thus, the probability of \(B\) given \(A\) is 1/2. 
</p>

<p>
Continuing with Example <a href="#exa-Toss-a-six-sided-die-twice">Toss-a-six-sided-die-twice</a>, the first thing to do is set up the probability space with the <code>rolldie</code> function.
</p>



<pre class="example">library(prob)
S &lt;- rolldie(2, makespace = TRUE)  # assumes ELM
head(S)                            #  first few rows

</pre>




<pre class="example">
  X1 X2      probs
1  1  1 0.02777778
2  2  1 0.02777778
3  3  1 0.02777778
4  4  1 0.02777778
5  5  1 0.02777778
6  6  1 0.02777778
</pre>


<p>
Next we define the events
</p>



<pre class="example">A &lt;- subset(S, X1 == X2)
B &lt;- subset(S, X1 + X2 &gt;= 8)

</pre>




<p>
And now we are ready to calculate probabilities. To do conditional probability, we use the <code>given</code> argument of the <code>prob</code> function:
</p>



<pre class="example">prob(A, given = B)
prob(B, given = A)

</pre>





<p>
Note that we do not actually need to define the events \(A\) and \(B\) separately as long as we reference the original probability space \(S\) as the first argument of the <code>prob</code> calculation:
</p>



<pre class="example">prob(S, X1==X2, given = (X1 + X2 &gt;= 8) )
prob(S, X1+X2 &gt;= 8, given = (X1==X2) )

</pre>






</div>

<div id="outline-container-3-6-1" class="outline-4">
<h4 id="sec-3-6-1"><span class="section-number-4">3.6.1</span> Properties and Rules </h4>
<div class="outline-text-4" id="text-3-6-1">


<p>
The following theorem establishes that conditional probabilities behave just like regular probabilities when the conditioned event is fixed. 
</p>
<p>
For any fixed event \(A\) with \(\P(A)&gt;0\),
</p>
<ol>
<li>\( \P (B|A)\geq 0 \), for all events \( B \subset S\),
</li>
<li>\( \P (S|A) = 1 \), and
</li>
<li>If \(B_{1}\), \(B_{2}\), \(B_{3}\),&hellip; are disjoint events, then
  \begin{equation}
  \P\left(\left.\bigcup_{k=1}^{\infty}B_{k}\:\right|A\right)=\sum_{k=1}^{\infty}\P(B_{k}|A).
  \end{equation}
</li>
</ol>



<p>
In other words, \(\P(\cdot|A)\) is a legitimate probability function. With this fact in mind, the following properties are immediate:
</p>
<p>
For any events \(A\), \(B\), and \(C\) with \(\P(A)&gt;0\),
</p>
<ol>
<li>\( \P ( B^{c} | A ) = 1 - \P (B|A).\)
</li>
<li>If \(B\subset C\) then \(\P(B|A)\leq\P(C|A)\).
</li>
<li>\( \P [ ( B\cup C ) | A ] = \P (B|A) + \P(C|A) - \P [ (B \cap C|A) ].\)
</li>
<li><b>The Multiplication Rule.</b> For any two events \(A\) and \(B\),
  \begin{equation}
  \P(A\cap B)=\P(A)\P(B|A).\label{eq:multiplication-rule-short}
  \end{equation}
  And more generally, for events \(A_{1}\), \(A_{2}\), \(A_{3}\),&hellip;, \(A_{n}\),
  \begin{equation}
  \P(A_{1}\cap A_{2}\cap\cdots\cap A_{n})=\P(A_{1})\P(A_{2}|A_{1})\cdots\P(A_{n}|A_{1}\cap A_{2}\cap\cdots\cap A_{n-1}).\label{eq:multiplication-rule-long}
  \end{equation}
</li>
</ol>



<p>
The Multiplication Rule is very important because it allows us to find probabilities in random experiments that have a sequential structure, as the next example shows. 
</p>
<p>
\label{exa:two-cards-both-aces}
At the beginning of the section we drew two cards from a standard playing deck. Now we may answer our original question, what is \(\P(\mbox{both Aces})\)?
</p>


\[
\P(\mbox{both Aces})=\P(A\cap B)=\P(A)\P(B|A)=\frac{4}{52}\cdot\frac{3}{51}\approx0.00452.
\]


<p>
\label{sub:howto-ps-objects}
</p>
<p>
Continuing Example <a href="#exa-two-cards-both-aces">two-cards-both-aces</a>, we set up the probability space by way of a three step process. First we employ the <code>cards</code> function to get a data frame <code>L</code> with two columns: <code>rank</code> and <code>suit</code>. Both columns are stored internally as factors with 13 and 4 levels, respectively.
</p>
<p>
Next we sample two cards randomly from the <code>L</code> data frame by way of the <code>urnsamples</code> function. It returns a list <code>M</code> which contains all possible pairs of rows from <code>L</code> (there are <code>choose(52,2)</code> of them). The sample space for this experiment is exactly the list <code>M</code>.
</p>
<p>
At long last we associate a probability model with the sample space. This is right down the <code>probspace</code> function's alley. It assumes the equally likely model by default. We call this result <code>N</code> which is an object of class <code>ps</code> &ndash; short for ``probability space''.
</p>
<p>
But do not be intimidated. The object <code>N</code> is nothing more than a list with two elements: <code>outcomes</code> and <code>probs</code>. The <code>outcomes</code> element is itself just another list, with <code>choose(52,2)</code> entries, each one a data frame with two rows which correspond to the pair of cards chosen. The <code>probs</code> element is just a vector with <code>choose(52,2)</code> entries all the same: <code>1/choose(52,2)</code>.
</p>
<p>
Putting all of this together we do 
</p>



<pre class="example">library(prob)
L &lt;- cards()
M &lt;- urnsamples(L, size = 2)
N &lt;- probspace(M)

</pre>





<p>
Now that we have the probability space <code>N</code> we are ready to do some probability. We use the <code>prob</code> function, just like before. The only trick is to specify the event of interest correctly, and recall that we were interested in \(\P(\mbox{both Aces})\). But if the cards are both Aces then the <code>rank</code> of both cards should be <code>A</code>, which sounds like a job for the <code>all</code> function:
</p>



<pre class="example">prob(N, all(rank == "A"))

</pre>





<p>
Note that this value matches what we found in Example <a href="#exa-two-cards-both-aces">two-cards-both-aces</a>, above. We could calculate all sorts of probabilities at this point; we are limited only by the complexity of the event's computer representation. 
</p>

<p>
\label{exa:urn-7-red-3-green}
Consider an urn with 10 balls inside, 7 of which are red and 3 of which are green. Select 3 balls successively from the urn. Let \( A = \{ 1^{\mathrm{st}} \mbox{ ball is red} \} \), \( B = \{ 2^{\mathrm{nd}} \mbox{ ball is red} \right\} \), and \( C = \{ 3^{\mathrm{rd}} \mbox{ ball is red} \} \). Then
</p>


\[
\P(\mbox{all 3 balls are red})=\P(A\cap B\cap C)=\frac{7}{10}\cdot\frac{6}{9}\cdot\frac{5}{8}\approx0.2917.
\]



<p>
Example <a href="#exa-urn-7-red-3-green">urn-7-red-3-green</a> is similar to Example <a href="#exa-two-cards-both-aces">two-cards-both-aces</a>, but it is even easier. We need to set up an urn (vector <code>L</code>) to hold the balls, we sample from <code>L</code> to get the sample space (data frame <code>M</code>), and we associate a probability vector (column <code>probs</code>) with the outcomes (rows of <code>M</code>) of the sample space. The final result is a probability space (an ordinary data frame <code>N</code>).
</p>
<p>
It is easier for us this time because our urn is a vector instead of a <code>cards()</code> data frame. Before there were two dimensions of information associated with the outcomes (rank and suit) but presently we have only one dimension (color).
</p>



<pre class="example">library(prob)
L &lt;- rep(c("red","green"), times = c(7,3))
M &lt;- urnsamples(L, size = 3, replace = FALSE, ordered = TRUE)
N &lt;- probspace(M)

</pre>





<p>
Now let us think about how to set up the event \(\{ \mbox{all 3 balls are red}\} \). Rows of <code>N</code> that satisfy this condition have \texttt{X1=="red" \&amp; X2=="red" \&amp; X3=="red"}, but there must be an easier way. Indeed, there is. The <code>isrep</code> function (short for ``is repeated'') in the <code>prob</code> package was written for this purpose. The command <code>isrep(N,"red",3)</code> will test each row of <code>N</code> to see whether the value \texttt{"red"} appears <code>3</code> times. The result is exactly what we need to define an event with the <code>prob</code> function. Observe
</p>



<pre class="example">prob(N, isrep(N, "red", 3))

</pre>





<p>
Note that this answer matches what we found in Example <a href="#exa-urn-7-red-3-green">urn-7-red-3-green</a>. Now let us try some other probability questions. What is the probability of getting two ="red"=s?
</p>



<pre class="example">prob(N, isrep(N, "red", 2))

</pre>






<p>
Note that the exact value is \(21/40\); we will learn a quick way to compute this in Section <a href="#sec-other-discrete-distributions">other-discrete-distributions</a>. What is the probability of observing \texttt{"red"}, then \texttt{"green"}, then \texttt{"red"}?
</p>



<pre class="example">prob(N, isin(N, c("red","green","red"), ordered = TRUE))

</pre>





<p>
Note that the exact value is \(7/20\) (do it with the Multiplication Rule). What is the probability of observing \texttt{"red"}, \texttt{"green"}, and \texttt{"red"}, in no particular order?
</p>



<pre class="example">prob(N, isin(N, c("red","green","red")))

</pre>





<p>
We already knew this. It is the probability of observing two ="red"=s, above.
</p>

<p>
Consider two urns, the first with 5 red balls and 3 green balls, and the second with 2 red balls and 6 green balls. Your friend randomly selects one ball from the first urn and transfers it to the second urn, without disclosing the color of the ball. You select one ball from the second urn. What is the probability that the selected ball is red? Let \( A = \{ \mbox{transferred ball is red} \} \) and \( B = \{ \mbox{selected ball is red} \} \). Write
\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\P(B) & =\P(A\cap B)+\P(A^{c}\cap B)\\
 & =\P(A)\P(B|A)+\P(A^{c})\P(B|A^{c})\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9}\\
 & =\frac{21}{72}\ 
\end{align*}
(which is 7/24 in lowest terms).
</p>

<p>
We saw the <code>RcmdrTestDrive</code> data set in Chapter <a href="#cha-introduction-to-R">introduction-to-R</a> in which a two-way table of the smoking status versus the gender was 
</p>



<pre class="example">.Table &lt;- xtabs( ~ smoking + gender, data = RcmdrTestDrive)
addmargins(.Table) # Table with Marginal Distributions

</pre>




<pre class="example">
           gender
smoking     Female Male Sum
  Nonsmoker     61   75 136
  Smoker         9   23  32
  Sum           70   98 168
</pre>


<p>
If one person were selected at random from the data set, then we see from the two-way table that \(\P(\mbox{Female})=70/168\) and \(\P(\mbox{Smoker})=32/168\). Now suppose that one of the subjects quits smoking, but we do not know the person's gender. If we select one subject at random, what now is \(\P(\mbox{Female})\)? Let \( A = \{ \mbox{the quitter is a female} \} \) and \( B = \{ \mbox{selected person is a female} \} \). Write
\begin{align*}
B & =S\cap B\\
 & =(A\cup A^{c})\cap B\\
 & =(A\cap B)\cup(A^{c}\cap B)
\end{align*}
and notice that \(A\cap B\) and \(A^{c}\cap B\) are disjoint. Therefore
\begin{align*}
\P(B) & =\P(A\cap B)+\P(A^{c}\cap B),\\
 & =\P(A)\P(B|A)+\P(A^{c})\P(B|A^{c}),\\
 & =\frac{5}{8}\cdot\frac{3}{9}+\frac{3}{8}\cdot\frac{2}{9},\\
 & =\frac{21}{72},
\end{align*}
(which is 7/24 in lowest terms).
</p>
<p>
Using the same reasoning, we can return to the example from the beginning of the section and show that
\[
\P(\{ \mbox{second card is an Ace} \} )=4/52.
\]
</p>

</div>
</div>

</div>

<div id="outline-container-3-7" class="outline-3">
<h3 id="sec-3-7"><span class="section-number-3">3.7</span> Independent Events </h3>
<div class="outline-text-3" id="text-3-7">

<p>\label{sec:Independent-Events}
</p>
<p>
Toss a coin twice. The sample space is $S= \{ HH,\ HT,\ TH,\ TT \} $. We know that \(\P(1^{\mathrm{st}}\mbox{ toss is }H)=2/4\), \(\P(2^{\mathrm{nd}}\mbox{ toss is }H)=2/4\), and \(\P(\mbox{both }H)=1/4\). Then
\begin{align*} \P(2^{\mathrm{nd}}\mbox{ toss is }H\ |\ 1^{\mathrm{st}}\mbox{ toss is }H) & =\frac{\P(\mbox{both }H)}{\P(1^{\mathrm{st}}\mbox{ toss is }H)},\\
 & =\frac{1/4}{2/4},\\
 & =\P(2^{\mathrm{nd}}\mbox{ toss is }H).
\end{align*}

Intuitively, this means that the information that the first toss is \(H\) has no bearing on the probability that the second toss is \(H\). The coin does not remember the result of the first toss. 
</p>
<p>
Events \(A\) and \(B\) are said to be <i>independent</i> if 
\begin{equation}
\P(A\cap B)=\P(A)\P(B).
\end{equation}
Otherwise, the events are said to be <i>dependent</i>. 
</p>
<p>
The connection with the above example stems from the following. We know from Section <a href="#sec-Conditional-Probability">Conditional-Probability</a> that when \(\P(B)&gt;0\) we may write
</p>


\begin{equation}
\P(A|B)=\frac{\P(A\cap B)}{\P(B)}.
\end{equation}

<p>
In the case that \(A\) and \(B\) are independent, the numerator of the fraction factors so that \(\P(B)\) cancels with the result:
</p>


\begin{equation}
\P(A|B)=\P(A)\mbox{ when \mbox{\emph{A},\emph{ B}} are independent.}
\end{equation}

<p>
The interpretation in the case of independence is that the information that the event \(B\) occurred does not influence the probability of the event \(A\) occurring. Similarly, \(\P(B|A)=\P(B)\), and so the occurrence of the event \(A\) likewise does not affect the probability of event \(B\). It may seem more natural to define \(A\) and \(B\) to be independent when \(\P(A|B)=\P(A)\); however, the conditional probability \(\P(A|B)\) is only defined when \(\P(B)&gt;0\). Our definition is not limited by this restriction. It can be shown that when \(\P(A),\ \P(B)&gt;0\) the two notions of independence are equivalent.
</p>
<p>
If the events \(A\) and \(B\) are independent then
</p><ul>
<li>\(A\) and \(B^{c}\) are independent,
</li>
<li>\(A^{c}\) and \(B\) are independent,
</li>
<li>\(A^{c}\) and \(B^{c}\) are independent.
</li>
</ul>


<p>
Suppose that \(A\) and \(B\) are independent. We will show the second one; the others are similar. We need to show that
\[
\P(A^{c}\cap B)=\P(A^{c})\P(B).
\]

To this end, note that the Multiplication Rule, Equation <a href="#eq-multiplication-rule-short">multiplication-rule-short</a> implies 
\begin{eqnarray*}
\P(A^{c}\cap B) & = & \P(B)\P(A^{c}|B),\\
 & = & \P(B)[1-\P(A|B)],\\
 & = & \P(B)\P(A^{c}).
\end{eqnarray*}

The events \(A\), \(B\), and \(C\) are <i>mutually independent</i> if the following four conditions are met: 
\begin{eqnarray*}
\P(A\cap B) & = & \P(A)\P(B),\\
\P(A\cap C) & = & \P(A)\P(C),\\
\P(B\cap C) & = & \P(B)\P(C),
\end{eqnarray*}
and
\[
\P(A\cap B\cap C)=\P(A)\P(B)\P(C).
\]
If only the first three conditions hold then \(A\), \(B\), and \(C\) are said to be independent <i>pairwise</i>. Note that pairwise independence is not the same as mutual independence when the number of events is larger than two.
</p>
<p>
We can now deduce the pattern for \(n\) events, \(n&gt;3\). The events will be mutually independent only if they satisfy the product equality pairwise, then in groups of three, in groups of four, and so forth, up to all \(n\) events at once. For \(n\) events, there will be \(2^{n}-n-1\) equations that must be satisfied (see Exercise <a href="#xca-numb-cond-indep">numb-cond-indep</a>). Although these requirements for a set of events to be mutually independent may seem stringent, the good news is that for most of the situations considered in this book the conditions will all be met (or at least we will suppose that they are).
</p>
<p>
\label{exa:toss-ten-coins}
Toss ten coins. What is the probability of observing at least one Head? Answer: Let \(A_{i}= \{ \mbox{the }i^{\mathrm{th}}\mbox{ coin shows }H \} ,\ i=1,2,\ldots,10\). Supposing that we toss the coins in such a way that they do not interfere with each other, this is one of the situations where all of the \(A_{i}\) may be considered mutually independent due to the nature of the tossing. Of course, the only way that there will not be at least one Head showing is if all tosses are Tails. Therefore,
\begin{align*}
\P(\mbox{at least one }H) & =1-\P(\mbox{all }T),\\
 & =1-\P(A_{1}^{c}\cap A_{2}^{c}\cap\cdots\cap A_{10}^{c}),\\
 & =1-\P(A_{1}^{c})\P(A_{2}^{c})\cdots\P(A_{10}^{c}),\\
 & =1-\left(\frac{1}{2}\right)^{10},
\end{align*}
which is approximately \(0.9990234\).
</p>


<p>
Toss ten coins. What is the probability of observing at least one Head?
</p>



<pre class="example">S &lt;- tosscoin(10, makespace = TRUE)
A &lt;- subset(S, isrep(S, vals = "T", nrep = 10))
1 - prob(A)

</pre>





<p>
Compare this answer to what we got in Example <a href="#exa-toss-ten-coins">toss-ten-coins</a>.
</p>


</div>

<div id="outline-container-3-7-1" class="outline-4">
<h4 id="sec-3-7-1"><span class="section-number-4">3.7.1</span> Independent, Repeated Experiments </h4>
<div class="outline-text-4" id="text-3-7-1">


<p>
Generalizing from above it is common to repeat a certain experiment multiple times under identical conditions and in an independent manner. We have seen many examples of this already: tossing a coin repeatedly, rolling a die or dice, <i>etc</i>.
</p>
<p>
The <code>iidspace</code> function was designed specifically for this situation. It has three arguments: <code>x</code>, which is a vector of outcomes, <code>ntrials</code>, which is an integer telling how many times to repeat the experiment, and <code>probs</code> to specify the probabilities of the outcomes of <code>x</code> in a single trial. 
</p>
<p>
\textbf{An unbalanced coin} (continued, see Example <a href="#exa-unbalanced-coin">unbalanced-coin</a>). It was easy enough to set up the probability space for one unbalanced toss, however, the situation becomes more complicated when there are many tosses involved. Clearly, the outcome \(HHH\) should not have the same probability as \(TTT\), which should again not have the same probability as \(HTH\). At the same time, there is symmetry in the experiment in that the coin does not remember the face it shows from toss to toss, and it is easy enough to toss the coin in a similar way repeatedly.
</p>
<p>
We may represent tossing our unbalanced coin three times with the following: 
</p>



<pre class="example">iidspace(c("H","T"), ntrials = 3, probs = c(0.7, 0.3)) 

</pre>




<pre class="example">
  X1 X2 X3 probs
1  H  H  H 0.343
2  T  H  H 0.147
3  H  T  H 0.147
4  T  T  H 0.063
5  H  H  T 0.147
6  T  H  T 0.063
7  H  T  T 0.063
8  T  T  T 0.027
</pre>


<p>
As expected, the outcome \(HHH\) has the largest probability, while \(TTT\) has the smallest. (Since the trials are independent, \(\P(HHH)=0.7^{3}\) and \(\P(TTT)=0.3^{3}\), \textit{etc}.) Note that the result of the function call is a probability space, not a sample space (which we could construct already with the <code>tosscoin</code> or <code>urnsamples</code> functions). The same procedure could be used to model an unbalanced die or any other experiment that may be represented with a vector of possible outcomes.
</p>

<p>
Note that <code>iidspace</code> will assume <code>x</code> has equally likely outcomes if no <code>probs</code> argument is specified. Also note that the argument <code>x</code> is a <i>vector</i>, not a data frame. Something like <code>iidspace(tosscoin(1),...)</code> would give an error.
</p>
</div>
</div>

</div>

<div id="outline-container-3-8" class="outline-3">
<h3 id="sec-3-8"><span class="section-number-3">3.8</span> Bayes' Rule </h3>
<div class="outline-text-3" id="text-3-8">

<p>\label{sec:Bayes'-Rule}
</p>
<p>
We mentioned the subjective view of probability in Section <a href="#sec-Interpreting-Probabilities">Interpreting-Probabilities</a>. In this section we introduce a rule that allows us to update our probabilities when new information becomes available. 
</p>
<p>
\textbf{\emph{(Bayes' Rule).}} Let \(B_{1}\), \(B_{2}\), &hellip;, \(B_{n}\) be mutually exclusive and exhaustive and let \(A\) be an event with \(\P(A)&gt;0\). Then 
\begin{equation}
\P(B_{k}|A)=\frac{\P(B_{k})\P(A|B_{k})}{\sum_{i=1}^{n}\P(B_{i})\P(A|B_{i})},\quad k=1,2,\ldots,n.\label{eq:bayes-rule}
\end{equation}

The proof follows from looking at \(\P(B_{k}\cap A)\) in two different ways. For simplicity, suppose that \(P(B_{k})&gt;0\) for all \(k\). Then
\[
\P(A)\P(B_{k}|A)=\P(B_{k}\cap A)=\P(B_{k})\P(A|B_{k}).
\]
Since \(\P(A)&gt;0\) we may divide through to obtain 
\[
\P(B_{k}|A)=\frac{\P(B_{k})\P(A|B_{k})}{\P(A)}.
\]

Now remembering that \(\{ B_{k} \}\) is a partition, the Theorem of Total Probability (Equation <a href="#eq-theorem-total-probability">theorem-total-probability</a>) gives the denominator of the last expression to be
\[
\P(A)=\sum_{k=1}^{n}\P(B_{k}\cap A)=\sum_{k=1}^{n}\P(B_{k})\P(A|B_{k}).
\]

What does it mean? Usually in applications we are given (or know) <i>a priori</i> probabilities \(\P(B_{k})\). We go out and collect some data, which we represent by the event \(A\). We want to know: how do we <b>update</b> \(\P(B_{k})\) to \(\P(B_{k}|A)\)? The answer: Bayes' Rule.
</p>
<p>
\label{exa:misfiling-assistants}
<b>Misfiling Assistants.</b> In this problem, there are three assistants working at a company: Moe, Larry, and Curly. Their primary job duty is to file paperwork in the filing cabinet when papers become available. The three assistants have different work schedules:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left"></th><th scope="col" class="left">Moe</th><th scope="col" class="left">Larry</th><th scope="col" class="left">Curly</th></tr>
</thead>
<tbody>
<tr><td class="left">Workload</td><td class="left">60%</td><td class="left">30%</td><td class="left">10%</td></tr>
</tbody>
</table>


<p>
That is, Moe works 60% of the time, Larry works 30% of the time, and Curly does the remaining 10%, and they file documents at approximately the same speed. Suppose a person were to select one of the documents from the cabinet at random. Let \(M\) be the event
\[
M= \{ \mbox{Moe filed the document} \}
\]
and let \(L\) and \(C\) be the events that Larry and Curly, respectively, filed the document. What are these events' respective probabilities? In the absence of additional information, reasonable prior probabilities would just be
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left"></th><th scope="col" class="left">Moe</th><th scope="col" class="left">Larry</th><th scope="col" class="left">Curly</th></tr>
</thead>
<tbody>
<tr><td class="left">Prior Probability</td><td class="left">0.6</td><td class="left">0.3</td><td class="left">0.1</td></tr>
</tbody>
</table>


<p>
Now, the boss comes in one day, opens up the file cabinet, and selects a file at random. The boss discovers that the file has been misplaced. The boss is so angry at the mistake that (s)he threatens to fire the one who erred. The question is: who misplaced the file?
</p>
<p>
The boss decides to use probability to decide, and walks straight to the workload schedule. (S)he reasons that, since the three employees work at the same speed, the probability that a randomly selected file would have been filed by each one would be proportional to his workload. The boss notifies <b>Moe</b> that he has until the end of the day to empty his desk.
</p>
<p>
But Moe argues in his defense that the boss has ignored additional information. Moe's likelihood of having misfiled a document is smaller than Larry's and Curly's, since he is a diligent worker who pays close attention to his work. Moe admits that he works longer than the others, but he doesn't make as many mistakes as they do. Thus, Moe recommends that &ndash; before making a decision &ndash; the boss should update the probability (initially based on workload alone) to incorporate the likelihood of having observed a misfiled document.
</p>
<p>
And, as it turns out, the boss has information about Moe, Larry, and Curly's filing accuracy in the past (due to historical performance evaluations). The performance information may be represented by the following table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left"></th><th scope="col" class="left">Moe</th><th scope="col" class="left">Larry</th><th scope="col" class="left">Curly</th></tr>
</thead>
<tbody>
<tr><td class="left">Misfile Rate</td><td class="left">0.003</td><td class="left">0.007</td><td class="left">0.010</td></tr>
</tbody>
</table>


<p>
In other words, on the average, Moe misfiles 0.3% of the documents he is supposed to file. Notice that Moe was correct: he is the most accurate filer, followed by Larry, and lastly Curly. If the boss were to make a decision based only on the worker's overall accuracy, then <b>Curly</b> should get the axe. But Curly hears this and interjects that he only works a short period during the day, and consequently makes mistakes only very rarely; there is only the tiniest chance that he misfiled this particular document.
</p>
<p>
The boss would like to use this updated information to update the probabilities for the three assistants, that is, (s)he wants to use the additional likelihood that the document was misfiled to update his/her beliefs about the likely culprit. Let \(A\) be the event that a document is misfiled. What the boss would like to know are the three probabilities
\[
\P(M|A),\mbox{ }\P(L|A),\mbox{ and }\P(C|A).
\]

We will show the calculation for \(\P(M|A)\), the other two cases being similar. We use Bayes' Rule in the form
\[
\P(M|A)=\frac{\P(M\cap A)}{\P(A)}.
\]

Let's try to find \(\P(M\cap A)\), which is just \(\P(M)\cdot\P(A|M)\) by the Multiplication Rule. We already know \(\P(M)=0.6\) and \(\P(A|M)\) is nothing more than Moe's misfile rate, given above to be \(\P(A|M)=0.003\). Thus, we compute
\[
\P(M\cap A)=(0.6)(0.003)=0.0018.
\]
Using the same procedure we may calculate
\[
\P(L|A)=0.0021\mbox{ and }\P(C|A)=0.0010.
\]

Now let's find the denominator, \(\P(A)\). The key here is the notion that if a file is misplaced, then either Moe or Larry or Curly must have filed it; there is no one else around to do the misfiling. Further, these possibilities are mutually exclusive. We may use the Theorem of Total Probability <a href="#eq-theorem-total-probability">theorem-total-probability</a> to write
\[ 
\P(A)=\P(A\cap M)+\P(A\cap L)+\P(A\cap C).
\]

Luckily, we have computed these above. Thus
\[
\P(A)=0.0018+0.0021+0.0010=0.0049.
\]
Therefore, Bayes' Rule yields
\[
\P(M|A)=\frac{0.0018}{0.0049}\approx0.37.
\]

This last quantity is called the posterior probability that Moe misfiled the document, since it incorporates the observed data that a randomly selected file was misplaced (which is governed by the misfile rate). We can use the same argument to calculate
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left"></th><th scope="col" class="left">Moe</th><th scope="col" class="left">Larry</th><th scope="col" class="left">Curly</th></tr>
</thead>
<tbody>
<tr><td class="left">Posterior Probability</td><td class="left">\(\approx0.37\)</td><td class="left">\(\approx0.43\)</td><td class="left">\(\approx0.20\)</td></tr>
</tbody>
</table>


<p>
The conclusion: <b>Larry</b> gets the axe. What is happening is an intricate interplay between the time on the job and the misfile rate. It is not obvious who the winner (or in this case, loser) will be, and the statistician needs to consult Bayes' Rule to determine the best course of action.
</p>
<p>
\label{exa:misfiling-assistants-multiple}
Suppose the boss gets a change of heart and does not fire anybody. But the next day (s)he randomly selects another file and again finds it to be misplaced. To decide whom to fire now, the boss would use the same procedure, with one small change. (S)he would not use the prior probabilities 60%, 30%, and 10%; those are old news. Instead, she would replace the prior probabilities with the posterior probabilities just calculated. After the math she will have new posterior probabilities, updated even more from the day before.
</p>
<p>
In this way, probabilities found by Bayes' rule are always on the cutting edge, always updated with respect to the best information available at the time.
</p>

<p>
There are not any special functions for Bayes' Rule in the <code>prob</code> package, but problems like the ones above are easy enough to do by hand.
</p>
<p>
<b>Misfiling assistants</b> (continued from Example <a href="#exa-misfiling-assistants">misfiling-assistants</a>). We store the prior probabilities and the likelihoods in vectors and go to town.
</p>



<pre class="example">prior &lt;- c(0.6, 0.3, 0.1)
like &lt;- c(0.003, 0.007, 0.010)
post &lt;- prior * like
post / sum(post)

</pre>




<pre class="example">
[1] 0.3673469 0.4285714 0.2040816
</pre>




<p>
Compare these answers with what we got in Example <a href="#exa-misfiling-assistants">misfiling-assistants</a>. We would replace <code>prior</code> with <code>post</code> in a future calculation. We could raise <code>like</code> to a power to see how the posterior is affected by future document mistakes. (Do you see why? Think back to Section <a href="#sec-Independent-Events">Independent-Events</a>.)
</p>

<p>
Let us incorporate the posterior probability (<code>post</code>) information from the last example and suppose that the assistants misfile seven more documents. Using Bayes' Rule, what would the new posterior probabilities be?
</p>



<pre class="example">newprior &lt;- post
post &lt;- newprior * like^7
post / sum(post)

</pre>




<pre class="example">
[1] 0.0003355044 0.1473949328 0.8522695627
</pre>


<p>
We see that the individual with the highest probability of having misfiled all eight documents given the observed data is no longer Larry, but Curly. 
</p>
<p>
There are two important points. First, we did not divide <code>post</code> by the sum of its entries until the very last step; we do not need to calculate it, and it will save us computing time to postpone normalization until absolutely necessary, namely, until we finally want to interpret them as probabilities.
</p>
<p>
Second, the reader might be wondering what the boss would get if (s)he skipped the intermediate step of calculating the posterior after only one misfiled document. What if she started from the <i>original</i> prior, then observed eight misfiled documents, and calculated the posterior? What would she get? It must be the same answer, of course.
</p>



<pre class="example">fastpost &lt;- prior * like^8
fastpost / sum(fastpost)

</pre>




<pre class="example">
[1] 0.0003355044 0.1473949328 0.8522695627
</pre>


<p>
Compare this to what we got in Example <a href="#exa-misfiling-assistants-multiple">misfiling-assistants-multiple</a>.
</p>


</div>

</div>

<div id="outline-container-3-9" class="outline-3">
<h3 id="sec-3-9"><span class="section-number-3">3.9</span> Random Variables </h3>
<div class="outline-text-3" id="text-3-9">

<p>\label{sec:Random-Variables}
</p>
<p>
We already know about experiments, sample spaces, and events. In this section, we are interested in a <i>number</i> that is associated with the experiment. We conduct a random experiment \(E\) and after learning the outcome \(\omega\) in \(S\) we calculate a number \(X\). That is, to each outcome \(\omega\) in the sample space we associate a number \(X(\omega)=x\). 
</p>
<p>
A <i>random variable</i> \(X\) is a function \(X:S\to\R\) that associates to each outcome \(\omega\in S\) exactly one number \(X(\omega)=x\). 
</p>
<p>
We usually denote random variables by uppercase letters such as \(X\), \(Y\), and \(Z\), and we denote their observed values by lowercase letters \(x\), \(y\), and \(z\). Just as \(S\) is the set of all possible outcomes of \(E\), we call the set of all possible values of \(X\) the <i>support</i> of \(X\) and denote it by \(S_{X}\).
</p>
<p>
Let \(E\) be the experiment of flipping a coin twice. We have seen that the sample space is \( S = \{ HH,\ HT,\ TH,\ TT \} \). Now define the random variable \(X = \mbox{the number of heads}\). That is, for example, \(X(HH)=2\), while \(X(HT)=1\). We may make a table of the possibilities:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">\(\omega\in S\)</th><th scope="col" class="left">\(HH\)</th><th scope="col" class="left">\(HT\)</th><th scope="col" class="left">\(TH\)</th><th scope="col" class="left">\(TT\)</th></tr>
</thead>
<tbody>
<tr><td class="left">\(X(\omega)=x\)</td><td class="left">2</td><td class="left">1</td><td class="left">1</td><td class="left">0</td></tr>
</tbody>
</table>


<p>
Taking a look at the second row of the table, we see that the support of \(X\) &ndash; the set of all numbers that \(X\) assumes &ndash; would be \( S_{X}= \{ 0,1,2 \} \).
</p>
<p>
Let \(E\) be the experiment of flipping a coin repeatedly until observing a Head. The sample space would be $S= \{ H,\ TH,\ TTH,\ TTTH,\ \ldots \} $. Now define the random variable \(Y=\mbox{the number of Tails before the first head}\). Then the support of \(Y\) would be \( S_{Y}= \{ 0,1,2,\ldots \} \).
</p>
<p>
Let \(E\) be the experiment of tossing a coin in the air, and define the random variable \( Z = \mbox{the time (in seconds) until the coin hits the ground} \). In this case, the sample space is inconvenient to describe. Yet the support of \(Z\) would be \((0,\infty)\). Of course, it is reasonable to suppose that the coin will return to Earth in a short amount of time; in practice, the set \((0,\infty)\) is admittedly too large. However, we will find that in many circumstances it is mathematically convenient to study the extended set rather than a restricted one. 
</p>
<p>
There are important differences between the supports of \(X\), \(Y\), and \(Z\). The support of \(X\) is a finite collection of elements that can be inspected all at once. And while the support of \(Y\) cannot be exhaustively written down, its elements can nevertheless be listed in a naturally ordered sequence. Random variables with supports similar to those of \(X\) and \(Y\) are called <i>discrete random variables</i>. We study these in Chapter <a href="#cha-Discrete-Distributions">Discrete-Distributions</a>.
</p>
<p>
In contrast, the support of \(Z\) is a continuous interval, containing all rational and irrational positive real numbers. For this reason[fn:contrv], random variables with supports like \(Z\) are called <i>continuous random variables</i>, to be studied in Chapter <a href="#cha-Continuous-Distributions">Continuous-Distributions</a>.
</p>
<p>
[fn:contrv] This isn't really the reason, but it serves as an effective litmus test at the introductory level. See Billingsley or Resnick.
</p>

<p>
The primary vessel for this task is the <code>addrv</code> function. There are two ways to use it, and we will describe both.
</p>

<p>
The first method is based on the <code>transform</code> function. See <code>?transform</code>. The idea is to write a formula defining the random variable inside the function, and it will be added as a column to the data frame. As an example, let us roll a 4-sided die three times, and let us define the random variable \(U=X1-X2+X3\). 
</p>



<pre class="example">S &lt;- rolldie(3, nsides = 4, makespace = TRUE) 
S &lt;- addrv(S, U = X1-X2+X3) 

</pre>




<p>
Now let's take a look at the values of \(U\). In the interest of space, we will only reproduce the first few rows of \(S\) (there are \(4^{3}=64\) rows in total). 
</p>



<pre class="example">head(S)

</pre>




<pre class="example">
  X1 X2 X3 U    probs
1  1  1  1 1 0.015625
2  2  1  1 2 0.015625
3  3  1  1 3 0.015625
4  4  1  1 4 0.015625
5  1  2  1 0 0.015625
6  2  2  1 1 0.015625
</pre>


<p>
We see from the \(U\) column it is operating just like it should. We can now answer questions like
</p>



<pre class="example">prob(S, U &gt; 6) 

</pre>






<p>
Sometimes we have a function laying around that we would like to apply to some of the outcome variables, but it is unfortunately tedious to write out the formula defining what the new variable would be. The <code>addrv</code> function has an argument <code>FUN</code> specifically for this case. Its value should be a legitimate function from \textsf{R}, such as <code>sum</code>, <code>mean</code>, <code>median</code>, and so forth. Or, you can define your own function. Continuing the previous example, let's define \(V=\max(X1,X2,X3)\) and \(W=X1+X2+X3\). 
</p>



<pre class="example">S &lt;- addrv(S, FUN = max, invars = c("X1","X2","X3"), name = "V") 
S &lt;- addrv(S, FUN = sum, invars = c("X1","X2","X3"), name = "W") 
head(S) 

</pre>




<pre class="example">
  X1 X2 X3 U V W    probs
1  1  1  1 1 1 3 0.015625
2  2  1  1 2 2 4 0.015625
3  3  1  1 3 3 5 0.015625
4  4  1  1 4 4 6 0.015625
5  1  2  1 0 2 4 0.015625
6  2  2  1 1 2 5 0.015625
</pre>


<p>
Notice that <code>addrv</code> has an <code>invars</code> argument to specify exactly to which columns one would like to apply the function <code>FUN</code>. If no input variables are specified, then <code>addrv</code> will apply <code>FUN</code> to all non-=probs= columns. Further, <code>addrv</code> has an optional argument <code>name</code> to give the new variable; this can be useful when adding several random variables to a probability space (as above). If not specified, the default name is <code>X</code>.
</p>

<p>
As we can see above, often after adding a random variable \(V\) to a probability space one will find that \(V\) has values that are repeated, so that it becomes difficult to understand what the ultimate behavior of \(V\) actually is. We can use the <code>marginal</code> function to aggregate the rows of the sample space by values of \(V\), all the while accumulating the probability associated with \(V\)'s distinct values. Continuing our example from above, suppose we would like to focus entirely on the values and probabilities of \(V=\max(X1,X2,X3)\). 
</p>



<pre class="example">marginal(S, vars = "V") 

</pre>




<pre class="example">
  V    probs
1 1 0.015625
2 2 0.109375
3 3 0.296875
4 4 0.578125
</pre>


<p>
We could save the probability space of \(V\) in a data frame and study it further, if we wish. As a final remark, we can calculate the marginal distributions of multiple variables desired using the <code>vars</code> argument. For example, suppose we would like to examine the joint distribution of \(V\) and \(W\). 
</p>



<pre class="example">marginal(S, vars = c("V", "W")) 

</pre>







<pre class="example">   V  W    probs
1  1  3 0.015625
2  2  4 0.046875
3  2  5 0.046875
4  3  5 0.046875
5  2  6 0.015625
6  3  6 0.093750
7  4  6 0.046875
8  3  7 0.093750
9  4  7 0.093750
10 3  8 0.046875
11 4  8 0.140625
12 3  9 0.015625
13 4  9 0.140625
14 4 10 0.093750
15 4 11 0.046875
16 4 12 0.015625
</pre>



<p>
Note that the default value of <code>vars</code> is the names of all columns except <code>probs</code>. This can be useful if there are duplicated rows in the probability space.
</p>
<p>
\newpage{}
</p>
</div>

</div>

<div id="outline-container-3-10" class="outline-3">
<h3 id="sec-3-10"><span class="section-number-3">3.10</span> Chapter Exercises </h3>
<div class="outline-text-3" id="text-3-10">





\begin{xca}
\label{xca:numb-cond-indep}
Prove the assertion given in the text: the number of conditions that the events $A_{1}$, $A_{2}$, ..., $A_{n}$ must satisfy in order to be mutually independent is $2^{n}-n-1$. (/Hint/: think about Pascal's triangle.)
\end{xca}


<p>
The events must satisfy the product equalities two at a time, of which there are \({n \choose 2}\), then they must satisfy an additional \({n \choose 3}\) conditions three at a time, and so on, until they satisfy the \({n \choose n}=1\) condition including all \(n\) events. In total, there are 
</p>


\[
{n \choose 2}+{n \choose 3}+\cdots+{n \choose n}=\sum_{k=0}^{n}{n \choose k}-\left[{n \choose 0}+{n \choose 1}\right]
\]

<p>
conditions to be satisfied, but the binomial series in the expression on the right is the sum of the entries of the \(n^{\mathrm{th}}\) row of Pascal's triangle, which is \(2^{n}\).
</p>

</div>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Discrete Distributions </h2>
<div class="outline-text-2" id="text-4">

<p>\label{cha:Discrete-Distributions}
</p>
<p>
\noindent In this chapter we introduce discrete random variables, those who take values in a finite or countably infinite support set. We discuss probability mass functions and some special expectations, namely, the mean, variance and standard deviation. Some of the more important discrete distributions are explored in detail, and the more general concept of expectation is defined, which paves the way for moment generating functions. 
</p>
<p>
We give special attention to the empirical distribution since it plays such a fundamental role with respect to resampling and Chapter <a href="#cha-resampling-methods">resampling-methods</a>; it will also be needed in Section <a href="#sub-Kolmogorov-Smirnov-Goodness-of-Fit-Test">Kolmogorov-Smirnov-Goodness-of-Fit-Test</a> where we discuss the Kolmogorov-Smirnov test. Following this is a section in which we introduce a catalogue of discrete random variables that can be used to model experiments.
</p>
<p>
There are some comments on simulation, and we mention transformations of random variables in the discrete case. The interested reader who would like to learn more about any of the assorted discrete distributions mentioned here should take a look at <i>Univariate Discrete Distributions</i> by Johnson <i>et al</i>\cite{Johnson1993}.
</p>


<ul>
<li>how to choose a reasonable discrete model under a variety of physical circumstances
</li>
<li>item the notion of mathematical expectation, how to calculate it, and basic properties- moment generating functions (yes, I want them to hear about those)
</li>
<li>the general tools of the trade for manipulation of continuous random variables, integration, <i>etc</i>.
</li>
<li>some details on a couple of discrete models, and exposure to a bunch of other ones
</li>
<li>how to make new discrete random variables from old ones
</li>
</ul>


</div>

<div id="outline-container-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> Discrete Random Variables </h3>
<div class="outline-text-3" id="text-4-1">

<p>\label{sec:discrete-random-variables}
</p>

</div>

<div id="outline-container-4-1-1" class="outline-4">
<h4 id="sec-4-1-1"><span class="section-number-4">4.1.1</span> Probability Mass Functions </h4>
<div class="outline-text-4" id="text-4-1-1">

<p>\label{sub:probability-mass-functions}
</p>
<p>
Discrete random variables are characterized by their supports which take the form
\begin{equation}
S_{X}=\{u_{1},u_{2},\ldots,u_{k}\}\mbox{ or }S_{X}=\{u_{1},u_{2},u_{3}\ldots\}.
\end{equation}

Every discrete random variable \(X\) has associated with it a probability mass function (PMF) \(f_{X}:S_{X}\to[0,1]\) defined by
\begin{equation}
f_{X}(x)=\P(X=x),\quad x\in S_{X}.
\end{equation}

Since values of the PMF represent probabilities, we know from Chapter <a href="#cha-Probability">Probability</a> that PMFs enjoy certain properties. In particular, all PMFs satisfy
</p>
<ol>
<li>\(f_{X}(x)&gt;0\) for \(x\in S\),
</li>
<li>\(\sum_{x\in S}f_{X}(x)=1\), and
</li>
<li>\(\P(X\in A)=\sum_{x\in A}f_{X}(x)\), for any event \(A\subset S\).\end{enumerate}
</li>
</ol>


<p>
\label{exa:Toss-a-coin}
</p>
<p>
Toss a coin 3 times. The sample space would be
\[
S=\{ HHH,\ HTH,\ THH,\ TTH,\ HHT,\ HTT,\ THT,\ TTT\}.
\]

Now let \(X\) be the number of Heads observed. Then \(X\) has support $S<sub>X</sub>=\{ 0,1,2,3\} $. Assuming that the coin is fair and was tossed in exactly the same way each time, it is not unreasonable to suppose that the outcomes in the sample space are all equally likely. 
</p>
<p>
What is the PMF of \(X\)? Notice that \(X\) is zero exactly when the outcome \(TTT\) occurs, and this event has probability \(1/8\). Therefore, \(f_{X}(0)=1/8\), and the same reasoning shows that \(f_{X}(3)=1/8\). Exactly three outcomes result in \(X=1\), thus, \(f_{X}(1)=3/8\) and \(f_{X}(3)\) holds the remaining \(3/8\) probability (the total is 1). We can represent the PMF with a table:
</p>


\begin{table}[H]
\begin{tabular}{c|cccc|c}
$x\in S_{X}$ & 0 & 1 & 2 & 3 & Total\tabularnewline
\hline
$f_{X}(x)=\P(X=x)$ & 1/8 & 3/8 & 3/8 & 1/8 & 1\tabularnewline
\end{tabular}
\end{table}


</div>

</div>

<div id="outline-container-4-1-2" class="outline-4">
<h4 id="sec-4-1-2"><span class="section-number-4">4.1.2</span> Mean, Variance, and Standard Deviation </h4>
<div class="outline-text-4" id="text-4-1-2">

<p>\label{sub:mean-variance-sd}
</p>
<p>
There are numbers associated with PMFs. One important example is the mean \(\mu\), also known as \(\E X\) (which we will discuss later):
\begin{equation}
\mu=\E X=\sum_{x\in S}xf_{X}(x),
\end{equation}
provided the (potentially infinite) series \(\sum|x|f_{X}(x)\) is convergent. Another important number is the variance:
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x),
\end{equation}
which can be computed (see Exercise <a href="#xca-variance-shortcut">variance-shortcut</a>) with the alternate formula \(\sigma^{2}=\sum x{}^{2}f_{X}(x)-\mu^{2}\). Directly defined from the variance is the standard deviation \(\sigma=\sqrt{\sigma^{2}}\).
</p>
<p> 
\label{exa:disc-pmf-mean}
We will calculate the mean of \(X\) in Example <a href="#exa-Toss-a-coin">Toss-a-coin</a>.
\[
\mu=\sum_{x=0}^{3}xf_{X}(x)=0\cdot\frac{1}{8}+1\cdot\frac{3}{8}+2\cdot\frac{3}{8}+3\cdot\frac{1}{8}=3.5.
\]

We interpret \(\mu=3.5\) by reasoning that if we were to repeat the random experiment many times, independently each time, observe many corresponding outcomes of the random variable \(X\), and take the sample mean of the observations, then the calculated value would fall close to 3.5. The approximation would get better as we observe more and more values of \(X\) (another form of the Law of Large Numbers; see Section <a href="#sec-Interpreting-Probabilities">Interpreting-Probabilities</a>). Another way it is commonly stated is that \(X\) is 3.5 ``on the average'' or ``in the long run''.
</p>



\begin{rem}
Note that although we say $X$ is 3.5 on the average, we must keep in mind that our $X$ never actually equals 3.5 (in fact, it is impossible for $X$ to equal 3.5).
\end{rem}

<p>
Related to the probability mass function \(f_{X}(x)=\P(X=x)\) is another important function called the cumulative distribution function (CDF), \(F_{X}\). It is defined by the formula
\begin{equation}
F_{X}(t)=\P(X\leq t),\quad-\infty<t<\infty.
\end{equation}


We know that all PMFs satisfy certain properties, and a similar statement may be made for CDFs. In particular, any CDF \(F_{X}\) satisfies
</p>
<ul>
<li>\(F_{X}\) is nondecreasing (\(t_{1}\leq t_{2}\) implies \(F_{X}(t_{1})\leq F_{X}(t_{2})\)).

</li>
<li>\(F_{X}\) is right-continuous (\(\lim_{t\to a^{+}}F_{X}(t)=F_{X}(a)\) for all \(a\in\R\)).

</li>
<li>\(\lim_{t\to-\infty}F_{X}(t)=0\) and \(\lim_{t\to\infty}F_{X}(t)=1\).
</li>
</ul>

<p>We say that \(X\) has the distribution \(F_{X}\) and we write \(X\sim F_{X}\). In an abuse of notation we will also write \(X\sim f_{X}\) and for the named distributions the PMF or CDF will be identified by the family name instead of the defining formula.
</p>
<p>
\label{sub:disc-rv-how-r}
</p>
<p>
The mean and variance of a discrete random variable is easy to compute at the console. Let's return to Example <a href="#exa-disc-pmf-mean">disc-pmf-mean</a>. We will start by defining a vector <code>x</code> containing the support of \(X\), and a vector <code>f</code> to contain the values of \(f_{X}\) at the respective outcomes in <code>x</code>:
</p>



<pre class="example">x &lt;- c(0,1,2,3)
f &lt;- c(1/8, 3/8, 3/8, 1/8)

</pre>





<p>
To calculate the mean \(\mu\), we need to multiply the corresponding values of <code>x</code> and <code>f</code> and add them. This is easily accomplished in \textsf{R} since operations on vectors are performed <i>element-wise</i> (see Section <a href="#sub-Functions-and-Expressions">Functions-and-Expressions</a>): 
</p>



<pre class="example">mu &lt;- sum(x * f)
mu

</pre>




<pre class="example">
[1] 1.5
</pre>


<p>
To compute the variance \(\sigma^{2}\), we subtract the value of <code>mu</code> from each entry in <code>x</code>, square the answers, multiply by <code>f</code>,and <code>sum</code>. The standard deviation \(\sigma\) is simply the square root of \(\sigma^{2}\).
</p>



<pre class="example">sigma2 &lt;- sum((x-mu)^2 * f)
sigma2
sigma &lt;- sqrt(sigma2)
sigma

</pre>




<pre class="example">
[1] 0.75
[1] 0.8660254
</pre>


<p>
Finally, we may find the values of the CDF \(F_{X}\) on the support by accumulating the probabilities in \(f_{X}\) with the <code>cumsum</code> function. 
</p>



<pre class="example">F = cumsum(f)
F

</pre>




<pre class="example">
[1] 0.125 0.500 0.875 1.000
</pre>


<p>
As easy as this is, it is even easier to do with the <code>distrEx</code> package \cite{Ruckdescheldistr}. We define a random variable <code>X</code> as an object, then compute things from the object such as mean, variance, and standard deviation with the functions <code>E</code>, <code>var</code>, and <code>sd</code>:
</p>



<pre class="example">library(distrEx)
X &lt;- DiscreteDistribution(supp = 0:3, prob = c(1,3,3,1)/8)
E(X); var(X); sd(X)

</pre>




<pre class="example">
[1] 1.5
[1] 0.75
[1] 0.8660254
</pre>



</div>
</div>

</div>

<div id="outline-container-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> The Discrete Uniform Distribution </h3>
<div class="outline-text-3" id="text-4-2">

<p>\label{sec:disc-uniform-dist}
</p>
<p>
We have seen the basic building blocks of discrete distributions and we now study particular models that statisticians often encounter in the field. Perhaps the most fundamental of all is the <i>discrete uniform</i> distribution.
</p>
<p>
A random variable \(X\) with the discrete uniform distribution on the integers \(1,2,\ldots,m\) has PMF
\begin{equation}
f_{X}(x)=\frac{1}{m},\quad x=1,2,\ldots,m.
\end{equation}

We write \(X\sim\mathsf{disunif}(m)\). A random experiment where this distribution occurs is the choice of an integer at random between 1 and 100, inclusive. Let \(X\) be the number chosen. Then \(X\sim\mathsf{disunif}(m=100)\) and
\[
\P(X=x)=\frac{1}{100},\quad x=1,\ldots,100.
\]

We find a direct formula for the mean of \(X\sim\mathsf{disunif}(m)\):
\begin{equation}
\mu=\sum_{x=1}^{m}xf_{X}(x)=\sum_{x=1}^{m}x\cdot\frac{1}{m}=\frac{1}{m}(1+2+\cdots+m)=\frac{m+1}{2},
\end{equation}
where we have used the famous identity \(1+2+\cdots+m=m(m+1)/2\). That is, if we repeatedly choose integers at random from 1 to \(m\) then, on the average, we expect to get \((m+1)/2\). To get the variance we first calculate
\[
\sum_{x=1}^{m}x^{2}f_{X}(x)=\frac{1}{m}\sum_{x=1}^{m}x^{2}=\frac{1}{m}\frac{m(m+1)(2m+1)}{6}=\frac{(m+1)(2m+1)}{6},
\]
and finally,
\begin{equation}
\sigma^{2}=\sum_{x=1}^{m}x^{2}f_{X}(x)-\mu^{2}=\frac{(m+1)(2m+1)}{6}-\left(\frac{m+1}{2}\right)^{2}=\cdots=\frac{m^{2}-1}{12}.
\end{equation}

Roll a die and let \(X\) be the upward face showing. Then \(m=6\), \(\mu=7/2=3.5\), and \(\sigma^{2}=(6^{2}-1)/12=35/12\).
</p>




<p>
One can choose an integer at random with the <code>sample</code> function. The general syntax to simulate a discrete uniform random variable is <code>sample(x, size, replace = TRUE)</code>.
</p>
<p>
The argument <code>x</code> identifies the numbers from which to randomly sample. If <code>x</code> is a number, then sampling is done from 1 to <code>x</code>. The argument <code>size</code> tells how big the sample size should be, and <code>replace</code> tells whether or not numbers should be replaced in the urn after having been sampled. The default option is <code>replace = FALSE</code> but for discrete uniforms the sampled values should be replaced. Some examples follow.
</p>


\begin{itemize}
\item To roll a fair die 3000 times, do =sample(6, size = 3000, replace = TRUE)=.
\item To choose 27 random numbers from 30 to 70, do =sample(30:70, size = 27, replace = TRUE)=.
\item To flip a fair coin 1000 times, do =sample(c("H","T"), size = 1000, replace = TRUE)=.
\end{itemize}


<p>
Follow the sequence \textsf{Probability \(\triangleright\) Discrete Distributions \(\triangleright\) Discrete Uniform distribution \(\triangleright\) Simulate Discrete uniform variates.}&hellip;
</p>
<p>
Suppose we would like to roll a fair die 3000 times. In the <code>Number of samples</code> field we enter <code>1</code>. Next, we describe what interval of integers to be sampled. Since there are six faces numbered 1 through 6, we set <code>from = 1</code>, we set <code>to = 6</code>, and set <code>by = 1</code> (to indicate that we travel from 1 to 6 in increments of 1 unit). We will generate a list of 3000 numbers selected from among 1, 2, &hellip;, 6, and we store the results of the simulation. For the time being, we select <code>New Data set</code>. Click \textsf{OK}.
</p>
<p>
Since we are defining a new data set, the \textsf{R} Commander requests a name for the data set. The default name is <code>Simset1</code>, although in principle you could name it whatever you like (according to \textsf{R}'s rules for object names). We wish to have a list that is 3000 long, so we set <code>Sample Size = 3000</code> and click \textsf{OK}.
</p>
<p>
In the \textsf{R} Console window, the \textsf{R} Commander should tell you that <code>Simset1</code> has been initialized, and it should also alert you that <code>There was 1 discrete uniform variate sample stored in Simset 1.</code>. To take a look at the rolls of the die, we click \textsf{View data set} and a window opens.  
</p>
<p>
The default name for the variable is <code>disunif.sim1</code>.
</p>

</div>

</div>

<div id="outline-container-4-3" class="outline-3">
<h3 id="sec-4-3"><span class="section-number-3">4.3</span> The Binomial Distribution </h3>
<div class="outline-text-3" id="text-4-3">

<p>\label{sec:binom-dist}
</p>
<p>
The binomial distribution is based on a <i>Bernoulli trial</i>, which is a random experiment in which there are only two possible outcomes: success (\(S\)) and failure (\(F\)). We conduct the Bernoulli trial and let 
\begin{equation}
X=
\begin{cases}
1 & \mbox{if the outcome is \ensuremath{S}},\\
0 & \mbox{if the outcome is \ensuremath{F}}.
\end{cases}
\end{equation}

If the probability of success is \(p\) then the probability of failure must be \(1-p=q\) and the PMF of \(X\) is
</p>


\begin{equation}
f_{X}(x)=p^{x}(1-p)^{1-x},\quad x=0,1.
\end{equation}

<p>
It is easy to calculate \(\mu=\E X=p\) and \(\E X^{2}=p\) so that \(\sigma^{2}=p-p^{2}=p(1-p)\).
</p>


</div>

<div id="outline-container-4-3-1" class="outline-4">
<h4 id="sec-4-3-1"><span class="section-number-4">4.3.1</span> The Binomial Model </h4>
<div class="outline-text-4" id="text-4-3-1">

<p>\label{sub:The-Binomial-Model}
</p>
<p>
The Binomial model has three defining properties:
</p>
<ul>
<li>Bernoulli trials are conducted \(n\) times,
</li>
<li>the trials are independent,
</li>
<li>the probability of success \(p\) does not change between trials.
</li>
</ul>


<p>
If \(X\) counts the number of successes in the \(n\) independent trials, then the PMF of \(X\) is 
\begin{equation}
f_{X}(x)={n \choose x}p^{x}(1-p)^{n-x},\quad x=0,1,2,\ldots,n.
\end{equation}

We say that \(X\) has a <i>binomial distribution</i> and we write \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\). It is clear that \(f_{X}(x)\geq0\) for all \(x\) in the support because the value is the product of nonnegative numbers. We next check that \(\sum f(x)=1\):
</p>


\[
\sum_{x=0}^{n}{n \choose x}p^{x}(1-p)^{n-x}=[p+(1-p)]^{n}=1^{n}=1.
\]

<p>
We next find the mean:
\begin{alignat*}{1}
\mu= & \sum_{x=0}^{n}x\,{n \choose x}p^{x}(1-p)^{n-x},\\
= & \sum_{x=1}^{n}x\,\frac{n!}{x!(n-x)!}p^{x}q^{n-x},\\
= & n\cdot p\sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}q^{n-x},\\
= & np\,\sum_{x-1=0}^{n-1}{n-1 \choose x-1}p^{(x-1)}(1-p)^{(n-1)-(x-1)},\\
= & np.
\end{alignat*}

A similar argument shows that \(\E X(X-1)=n(n-1)p^{2}\) (see Exercise <a href="#xca-binom-factorial-expectation">binom-factorial-expectation</a>). Therefore
\begin{alignat*}{1}
\sigma^{2}= & \E X(X-1)+\E X-[\E X]^{2},\\
= & n(n-1)p^{2}+np-(np)^{2},\\
= & n^{2}p^{2}-np^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=np(1-p).
\end{alignat*}

A four-child family. Each child may be either a boy (\(B\)) or a girl (\(G\)). For simplicity we suppose that \(\P(B)=\P(G)=1/2\) and that the genders of the children are determined independently. If we let \(X\) count the number of \(B\)'s, then \(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\). Further, \(\P(X=2)\) is
\[
f_{X}(2)={4 \choose 2}(1/2)^{2}(1/2)^{2}=\frac{6}{2^{4}}.
\]

The mean number of boys is \(4(1/2)=2\) and the variance of \(X\) is \(4(1/2)(1/2)=1\).
</p>

<p>
The corresponding \textsf{R} function for the PMF and CDF are <code>dbinom</code> and <code>pbinom</code>, respectively. We demonstrate their use in the following examples.  
</p>
<p>
We can calculate it in \textsf{R} Commander under the \textsf{Binomial Distribution} menu with the \textsf{Binomial probabilities} menu item.
</p>


<pre class="example">
      Pr
0 0.0625
1 0.2500
2 0.3750
3 0.2500
4 0.0625
</pre>



<p>
We know that the \(\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\) distribution is supported on the integers 0, 1, 2, 3, and 4; thus the table is complete. We can read off the answer to be \(\P(X=2)=0.3750\).
</p>

<p>
Roll 12 dice simultaneously, and let \(X\) denote the number of 6's that appear. We wish to find the probability of getting seven, eight, or nine 6's. If we let $S=\{ \mbox{get a 6 on one roll} \} $, then \(\P(S)=1/6\) and the rolls constitute Bernoulli trials; thus \(X\sim\mathsf{binom}(\mathtt{size}=12,\ \mathtt{prob}=1/6)\) and our task is to find \(\P(7\leq X\leq9)\). This is just
\[ 
\P(7\leq X\leq9)=\sum_{x=7}^{9}{12 \choose x}(1/6)^{x}(5/6)^{12-x}.
\]

Again, one method to solve this problem would be to generate a probability mass table and add up the relevant rows. However, an alternative method is to notice that \(\P(7\leq X\leq9)=\P(X\leq9)-\P(X\leq6)=F_{X}(9)-F_{X}(6)\), so we could get the same answer by using the \textsf{Binomial tail probabilities}&hellip; menu in the \textsf{R} Commander or the following from the command line: 
</p>



<pre class="example">pbinom(9, size=12, prob=1/6) - pbinom(6, size=12, prob=1/6)
diff(pbinom(c(6,9), size = 12, prob = 1/6))  # same thing

</pre>




<pre class="example">
[1] 0.001291758
[1] 0.001291758
</pre>



<p>
\label{exa:toss-coin-3-withR}
Toss a coin three times and let \(X\) be the number of Heads observed. We know from before that \(X\sim\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) which implies the following PMF:
</p>



\begin{table}[H]
\begin{tabular}{c|cccc}
$x=\mbox{\#of Heads}$ & 0 & 1 & 2 & 3\tabularnewline
\hline
$f(x)=\P(X=x)$ & 1/8 & 3/8 & 3/8 & 1/8\tabularnewline
\end{tabular}
\end{table}


<p>
Our next goal is to write down the CDF of \(X\) explicitly. The first case is easy: it is impossible for \(X\) to be negative, so if \(x&lt;0\) then we should have \(\P(X\leq x)=0\). Now choose a value \(x\) satisfying \(0\leq x&lt;1\), say, \(x=0.3\). The only way that \(X\leq x\) could happen would be if \(X=0\), therefore, \(\P(X\leq x)\) should equal \(\P(X=0)\), and the same is true for any \(0\leq x&lt;1\). Similarly, for any \(1\leq x&lt;2\), say, \(x=1.73\), the event \(\{ X\leq x \}\) is exactly the event \(\{ X=0\mbox{ or }X=1 \}\). Consequently, \(\P(X\leq x)\) should equal \(\P(X=0\mbox{ or }X=1)=\P(X=0)+\P(X=1)\). Continuing in this fashion, we may figure out the values of \(F_{X}(x)\) for all possible inputs \(-\infty&lt;x&lt;\infty\), and we may summarize our observations with the following piecewise defined function:
\[
F_{X}(x)=\P(X\leq x)=
\begin{cases}
0, & x<0,\\
\frac{1}{8}, & 0\leq x<1,\\
\frac{1}{8}+\frac{3}{8}=\frac{4}{8}, & 1\leq x<2,\\
\frac{4}{8}+\frac{3}{8}=\frac{7}{8}, & 2\leq x<3,\\
1, & x\geq3.
\end{cases}
\]


In particular, the CDF of \(X\) is defined for the entire real line, \(\R\). The CDF is right continuous and nondecreasing. A graph of the \(\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)\) CDF is shown in Figure <a href="#fig-binom-cdf-base">binom-cdf-base</a>.
</p>




\begin{figure}
<<echo = FALSE, fig = true, height = 4.5, width = 6>>=
plot(0, xlim = c(-1.2, 4.2), ylim = c(-0.04, 1.04), type = "n", xlab = "number of successes", ylab = "cumulative probability")
abline(h = c(0,1), lty = 2, col = "grey")
lines(stepfun(0:3, pbinom(-1:3, size = 3, prob = 0.5)), verticals = FALSE, do.p = FALSE)
points(0:3, pbinom(0:3, size = 3, prob = 0.5), pch = 16, cex = 1.2)
points(0:3, pbinom(-1:2, size = 3, prob = 0.5), pch = 1, cex = 1.2)

\caption{Graph of the $\mathsf{binom}(\mathtt{size}=3,\,\mathtt{prob}=1/2)$
CDF\label{fig:binom-cdf-base}}

\end{figure}


<p>
Another way to do Example <a href="#exa-toss-coin-3-withR">toss-coin-3-withR</a> is with the <code>distr</code> family of packages \cite{Ruckdescheldistr}. They use an object oriented approach to random variables, that is, a random variable is stored in an object <code>X</code>, and then questions about the random variable translate to functions on and involving <code>X</code>. Random variables with distributions from the <code>base</code> package are specified by capitalizing the name of the distribution.
</p>



<pre class="example">library(distr)
X &lt;- Binom(size = 3, prob = 1/2)
X

</pre>




<pre class="example">
 Distribution Object of Class: Binom
 size: 3
 prob: 0.5
</pre>


<p>
The analogue of the <code>dbinom</code> function for <code>X</code> is the <code>d(X)</code> function, and the analogue of the <code>pbinom</code> function is the <code>p(X)</code> function. Compare the following:
</p>



<pre class="example">d(X)(1)   # pmf of X evaluated at x = 1
p(X)(2)   # cdf of X evaluated at x = 2

</pre>




<pre class="example">
[1] 0.375
[1] 0.875
</pre>



<p>
Random variables defined via the <code>distr</code> package may be <i>plotted</i>, which will return graphs of the PMF, CDF, and quantile function (introduced in Section <a href="#sub-Normal-Quantiles-QF">Normal-Quantiles-QF</a>). See Figure <a href="#fig-binom-plot-distr">binom-plot-distr</a> for an example.
</p>



\begin{figure}[H]
<<echo = FALSE, fig=true, height = 4, width = 6>>=
plot(X, cex = 0.2)
\caption{The \textsf{binom}(\texttt{size} = 3, \texttt{prob} = 0.5) distribution
from the \texttt{distr} package\label{fig:binom-plot-distr}}
\end{figure}






\begin{table}
\begin{tabular}{lllll}
\multicolumn{5}{l}{Given $X\sim\mathsf{dbinom}(\mathtt{size}=n,\,\mathtt{prob}=p)$.}\tabularnewline
 &  &  &  & \tabularnewline
How to do: &  & with $\mathtt{stats}$ (default)  &  & with $\mathtt{distr}$\tabularnewline
\hline
PMF: $\P(X=x)$ &  & $\mathtt{dbinom(x,size=n,prob=p)}$ &  & $\mathtt{d(X)(x)}$\tabularnewline
CDF: $\P(X\leq x)$ &  & $\mathtt{pbinom(x,size=n,prob=p)}$ &  & $\mathtt{p(X)(x)}$\tabularnewline
Simulate $k$ variates &  & $\mathtt{rbinom(k,size=n,prob=p)}$ &  & $\mathtt{r(X)(k)}$\tabularnewline
\hline
 &  &  &  & \tabularnewline
\multicolumn{5}{r}{For $\mathtt{distr}$ need \texttt{X <-} $\mathtt{Binom(size=}n\mathtt{,\ prob=}p\mathtt{)}$}\tabularnewline
\end{tabular}

\caption{Correspondence between \texttt{stats} and \texttt{distr}}

\end{table}



</div>
</div>

</div>

<div id="outline-container-4-4" class="outline-3">
<h3 id="sec-4-4"><span class="section-number-3">4.4</span> Expectation and Moment Generating Functions </h3>
<div class="outline-text-3" id="text-4-4">

<p>\label{sec:expectation-and-mgfs}
</p>


</div>

<div id="outline-container-4-4-1" class="outline-4">
<h4 id="sec-4-4-1"><span class="section-number-4">4.4.1</span> The Expectation Operator </h4>
<div class="outline-text-4" id="text-4-4-1">

<p>\label{sub:expectation-operator}
</p>
<p>
We next generalize some of the concepts from Section <a href="#sub-mean-variance-sd">mean-variance-sd</a>. There we saw that every[fn:expexists] PMF has two important numbers associated with it:
\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x),\quad\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x).
\end{equation}

[fn:expexists] Not every, only those PMFs for which the (potentially infinite) series converges.
</p>
<p>
Intuitively, for repeated observations of \(X\) we would expect the sample mean to closely approximate \(\mu\) as the sample size increases without bound. For this reason we call \(\mu\) the <i>expected value</i> of \(X\) and we write \(\mu=\E X\), where \(\E\) is an <i>expectation operator</i>.
</p>



\begin{defn}
More generally, given a function $g$ we define the /expected value of/ $g(X)$ by
\begin{equation}
\E\, g(X)=\sum_{x\in S}g(x)f_{X}(x),
\end{equation}
provided the (potentially infinite) series $\sum_{x}|g(x)|f(x)$ is convergent. We say that $\E g(X)$ /exists/.
\end{defn}


<p>
In this notation the variance is \(\sigma^{2}=\E(X-\mu)^{2}\) and we prove the identity
\begin{equation}
\E(X-\mu)^{2}=\E X^{2}-(\E X)^{2}
\end{equation}
in Exercise <a href="#xca-variance-shortcut">variance-shortcut</a>. Intuitively, for repeated observations of \(X\) we would expect the sample mean of the \(g(X)\) values to closely approximate \(\E\, g(X)\) as the sample size increases without bound.
</p>
<p>
Let us take the analogy further. If we expect \(g(X)\) to be close to \(\E g(X)\) on the average, where would we expect \(3g(X)\) to be on the average? It could only be \(3\E g(X)\). The following theorem makes this idea precise.
</p>


\begin{prop}
\label{pro:expectation-properties}
For any functions $g$ and $h$, any random variable $X$, and any constant $c$: 

1. $\E\: c=c$,
2. $\E[c\cdot g(X)]=c\E g(X)$
3. $\E[g(X)+h(X)]=\E g(X)+\E h(X)$,
ORG-LIST-END-MARKER

provided $\E g(X)$ and $\E h(X)$ exist.

\end{prop}

\begin{proof}
Go directly from the definition. For example,

\[
\E[c\cdot g(X)]=\sum_{x\in S}c\cdot g(x)f_{X}(x)=c\cdot\sum_{x\in S}g(x)f_{X}(x)=c\E g(X).
\]

\end{proof}

</div>

</div>

<div id="outline-container-4-4-2" class="outline-4">
<h4 id="sec-4-4-2"><span class="section-number-4">4.4.2</span> Moment Generating Functions </h4>
<div class="outline-text-4" id="text-4-4-2">

<p>\label{sub:MGFs}
</p>


\begin{defn}
Given a random variable $X$, its /moment generating function/ (abbreviated MGF) is defined by the formula
\begin{equation}
M_{X}(t)=\E\me^{tX}=\sum_{x\in S}\me^{tx}f_{X}(x),
\end{equation}
provided the (potentially infinite) series is convergent for all $t$ in a neighborhood of zero (that is, for all $-\epsilon<t<\epsilon$, for some $\epsilon>0$). 
\end{defn}


<p>
Note that for any MGF \(M_{X}\),
\begin{equation}
M_{X}(0)=\E\me^{0\cdot X}=\E1=1.
\end{equation}

We will calculate the MGF for the two distributions introduced above.
</p>
<p>
Find the MGF for \(X\sim\mathsf{disunif}(m)\). 
</p>
<p>
Since \(f(x)=1/m\), the MGF takes the form
\[
M(t)=\sum_{x=1}^{m}\me^{tx}\frac{1}{m}=\frac{1}{m}(\me^{t}+\me^{2t}+\cdots+\me^{mt}),\quad\mbox{for any \ensuremath{t}.}
\]


Find the MGF for \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\).
</p>


\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{n}\me^{tx}\,{n \choose x}\, p^{x}(1-p)^{n-x},\\
= & \sum_{x=0}^{n}{n \choose x}\,(p\me^{t})^{x}q^{n-x},\\
= & (p\me^{t}+q)^{n},\quad\mbox{for any \ensuremath{t}.}
\end{alignat*}



<p>
We will discuss three applications of moment generating functions in this book. The first is the fact that an MGF may be used to accurately identify the probability distribution that generated it, which rests on the following:
</p>


\begin{thm}
\label{thm:mgf-unique}
The moment generating function, if it exists in a neighborhood of zero, determines a probability distribution /uniquely/. 
\end{thm}

\begin{proof}
Unfortunately, the proof of such a theorem is beyond the scope of a text like this one. Interested readers could consult Billingsley \cite{Billingsley1995}.
\end{proof}


<p>
We will see an example of Theorem <a href="#thm-mgf-unique">mgf-unique</a> in action.
</p>
<p>
Suppose we encounter a random variable which has MGF
\[
M_{X}(t)=(0.3+0.7\me^{t})^{13}.
\]

Then \(X\sim\mathsf{binom}(\mathtt{size}=13,\,\mathtt{prob}=0.7)\).
</p>

<p>
An MGF is also known as a ``Laplace Transform'' and is manipulated in that context in many branches of science and engineering.
</p>

<p>
This brings us to the second powerful application of MGFs. Many of the models we study have a simple MGF, indeed, which permits us to determine the mean, variance, and even higher moments very quickly. Let us see why. We already know that 
\begin{alignat*}{1}
M(t)= & \sum_{x\in S}\me^{tx}f(x).
\end{alignat*}

Take the derivative with respect to \(t\) to get
\begin{equation}
M'(t)=\frac{\diff}{\diff t}\left(\sum_{x\in S}\me^{tx}f(x)\right)=\sum_{x\in S}\ \frac{\diff}{\diff t}\left(\me^{tx}f(x)\right)=\sum_{x\in S}x\me^{tx}f(x),
\end{equation}
and so if we plug in zero for \(t\) we see
\begin{equation}
M'(0)=\sum_{x\in S}x\me^{0}f(x)=\sum_{x\in S}xf(x)=\mu=\E X.
\end{equation}

Similarly, \(M''(t)=\sum x^{2}\me^{tx}f(x)\) so that \(M''(0)=\E X^{2}\). And in general, we can see[fn:exchdiffint] that
\begin{equation}
M_{X}^{(r)}(0)=\E X^{r}=\mbox{\ensuremath{r^{\mathrm{th}}} moment of \ensuremath{X} about the origin.}
\end{equation}

[fn:exchdiffint] We are glossing over some significant mathematical details in our derivation. Suffice it to say that when the MGF exists in a neighborhood of \(t=0\), the exchange of differentiation and summation is valid in that neighborhood, and our remarks hold true.
</p>

<p>
These are also known as <i>raw moments</i> and are sometimes denoted \(\mu_{r}'\). In addition to these are the so called <i>central moments</i> \(\mu_{r}\) defined by
\begin{equation}
\mu_{r}=\E(X-\mu)^{r},\quad r=1,2,\ldots
\end{equation}

Let \(X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)\mbox{ with \ensuremath{M(t)=(q+p\me^{t})^{n}}}\).
</p>
<p>
We calculated the mean and variance of a binomial random variable in Section <a href="#sec-binom-dist">binom-dist</a> by means of the binomial series. But look how quickly we find the mean and variance with the moment generating function.
\begin{alignat*}{1}
M'(t)= & n(q+p\me^{t})^{n-1}p\me^{t}\left|_{t=0}\right.,\\
= & n\cdot1^{n-1}p,\\
= & np.
\end{alignat*}

And
\begin{alignat*}{1}
M''(0)= & n(n-1)[q+p\me^{t}]^{n-2}(p\me^{t})^{2}+n[q+p\me^{t}]^{n-1}p\me^{t}\left|_{t=0}\right.,\\
\E X^{2}= & n(n-1)p^{2}+np.
\end{alignat*}

Therefore
</p>


\begin{alignat*}{1}
\sigma^{2}= & \E X^{2}-(\E X)^{2},\\
= & n(n-1)p^{2}+np-n^{2}p^{2},\\
= & np-np^{2}=npq.
\end{alignat*}

<p>
See how much easier that was?
</p>



\begin{rem}
We learned in this section that $M^{(r)}(0)=\E X^{r}$. We remember from Calculus II that certain functions $f$ can be represented by a Taylor series expansion about a point $a$, which takes the form
\begin{equation}
f(x)=\sum_{r=0}^{\infty}\frac{f^{(r)}(a)}{r!}(x-a)^{r},\quad\mbox{for all \ensuremath{|x-a|<R},}
\end{equation}

where $R$ is called the /radius of convergence/ of the series (see Appendix \ref{sec:Sequences-and-Series}). We combine the two to say that if an MGF exists for all $t$ in the interval $(-\epsilon,\epsilon)$, then we can write
\begin{equation}
M_{X}(t)=\sum_{r=0}^{\infty}\frac{\E X^{r}}{r!}t^{r},\quad\mbox{for all \ensuremath{|t|<\epsilon}.}
\end{equation}

\end{rem}


<p>
The <code>distrEx</code> package provides an expectation operator <code>E</code> which can be used on random variables that have been defined in the ordinary <code>distr</code> sense:
</p>



<pre class="example">X &lt;- Binom(size = 3, prob = 0.45)
library(distrEx)
E(X)
E(3*X + 4)

</pre>




<pre class="example">
[1] 1.35
[1] 8.05
</pre>


<p>
For discrete random variables with finite support, the expectation is simply computed with direct summation. In the case that the random variable has infinite support and the function is crazy, then the expectation is not computed directly, rather, it is estimated by first generating a random sample from the underlying model and next computing a sample mean of the function of interest. 
</p>
<p>
There are methods for other population parameters:
</p>



<pre class="example">var(X)
sd(X)

</pre>




<pre class="example">
[1] 0.7425
[1] 0.8616844
</pre>


<p>
There are even methods for <code>IQR</code>, <code>mad</code>, <code>skewness</code>, and <code>kurtosis</code>.
</p>

</div>
</div>

</div>

<div id="outline-container-4-5" class="outline-3">
<h3 id="sec-4-5"><span class="section-number-3">4.5</span> The Empirical Distribution </h3>
<div class="outline-text-3" id="text-4-5">

<p>\label{sec:empirical-distribution}
</p>
<p>
Do an experiment \(n\) times and observe \(n\) values \(x_{1}\), \(x_{2}\), &hellip;, \(x_{n}\) of a random variable \(X\). For simplicity in most of the discussion that follows it will be convenient to imagine that the observed values are distinct, but the remarks are valid even when the observed values are repeated. 
</p>


\begin{defn}
The /empirical cumulative distribution function/ $F_{n}$ (written ECDF)\index{Empirical distribution} is the probability distribution that places probability mass $1/n$ on each of the values $x_{1}$, $x_{2}$, ..., $x_{n}$. The empirical PMF takes the form
\begin{equation} 
f_{X}(x)=\frac{1}{n},\quad x\in \{ x_{1},x_{2},...,x_{n} \}.
\end{equation}

If the value $x_{i}$ is repeated $k$ times, the mass at $x_{i}$ is accumulated to $k/n$.
\end{defn}


<p>
The mean of the empirical distribution is
\begin{equation}
\mu=\sum_{x\in S}xf_{X}(x)=\sum_{i=1}^{n}x_{i}\cdot\frac{1}{n}
\end{equation}
and we recognize this last quantity to be the sample mean, \(\xbar\). The variance of the empirical distribution is
\begin{equation}
\sigma^{2}=\sum_{x\in S}(x-\mu)^{2}f_{X}(x)=\sum_{i=1}^{n}(x_{i}-\xbar)^{2}\cdot\frac{1}{n}
\end{equation}
and this last quantity looks very close to what we already know to be the sample variance.
</p>


\begin{equation}
s^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\xbar)^{2}.
\end{equation}

<p>
The <i>empirical quantile function</i> is the inverse of the ECDF. See Section <a href="#sub-Normal-Quantiles-QF">Normal-Quantiles-QF</a>.
</p>


<p>
The empirical distribution is not directly available as a distribution in the same way that the other base probability distributions are, but there are plenty of resources available for the determined investigator.  Given a data vector of observed values <code>x</code>, we can see the empirical CDF with the <code>ecdf</code>\index{ecdf@\texttt{ecdf}} function:
</p>



<pre class="example">x &lt;- c(4, 7, 9, 11, 12)
ecdf(x)

</pre>




<pre class="example">
Empirical CDF 
Call: ecdf(x)
 x[1:5] =      4,      7,      9,     11,     12
</pre>


<p>
The above shows that the returned value of <code>ecdf(x)</code> is not a <i>number</i> but rather a <i>function</i>. The ECDF is not usually used by itself in this form, by itself. More commonly it is used as an intermediate step in a more complicated calculation, for instance, in hypothesis testing (see Chapter <a href="#cha-Hypothesis-Testing">Hypothesis-Testing</a>) or resampling (see Chapter <a href="#cha-resampling-methods">resampling-methods</a>). It is nevertheless instructive to see what the <code>ecdf</code> looks like, and there is a special plot method for <code>ecdf</code> objects.
</p>


<p>
plot(ecdf(x))
</p>




\begin{figure}[H]
<<echo = FALSE, fig=true, height = 4.5, width = 6>>=
plot(ecdf(x))

\caption{The empirical CDF\label{fig:empirical-CDF}}
\end{figure}

<p>
See Figure <a href="#fig-empirical-CDF">empirical-CDF</a>. The graph is of a right-continuous function with jumps exactly at the locations stored in <code>x</code>. There are no repeated values in <code>x</code> so all of the jumps are equal to \(1/5=0.2\).
</p>
<p>
The empirical PDF is not usually of particular interest in itself, but if we really wanted we could define a function to serve as the empirical PDF:
</p>



<pre class="example">epdf &lt;- function(x) function(t){sum(x %in% t)/length(x)}
x &lt;- c(0,0,1)
epdf(x)(0)       # should be 2/3

</pre>




<pre class="example">
[1] 0.6666667
</pre>


<p>
To simulate from the empirical distribution supported on the vector <code>x</code>, we use the <code>sample</code>\index{sample@\texttt{sample}} function.
</p>



<pre class="example">x &lt;- c(0,0,1)
sample(x, size = 7, replace = TRUE)

</pre>




<pre class="example">
[1] 0 0 1 1 1 0 1
</pre>


<p>
We can get the empirical quantile function in \textsf{R} with <code>quantile(x, probs = p, type = 1)</code>; see Section <a href="#sub-Normal-Quantiles-QF">Normal-Quantiles-QF</a>.
</p>
<p>
As we hinted above, the empirical distribution is significant more because of how and where it appears in more sophisticated applications. We will explore some of these in later chapters &ndash; see, for instance, Chapter <a href="#cha-resampling-methods">resampling-methods</a>.
</p>
</div>

</div>

<div id="outline-container-4-6" class="outline-3">
<h3 id="sec-4-6"><span class="section-number-3">4.6</span> Other Discrete Distributions </h3>
<div class="outline-text-3" id="text-4-6">

<p>\label{sec:other-discrete-distributions}
</p>
<p>
The binomial and discrete uniform distributions are popular, and rightly so; they are simple and form the foundation for many other more complicated distributions. But the particular uniform and binomial models only apply to a limited range of problems. In this section we introduce situations for which we need more than what the uniform and binomial offer.
</p>


</div>

<div id="outline-container-4-6-1" class="outline-4">
<h4 id="sec-4-6-1"><span class="section-number-4">4.6.1</span> Dependent Bernoulli Trials </h4>
<div class="outline-text-4" id="text-4-6-1">

<p>\label{sec:non-bernoulli-trials}
</p>
<p>
\label{sub:hypergeometric-dist}
</p>
<p>
Consider an urn with 7 white balls and 5 black balls. Let our random experiment be to randomly select 4 balls, without replacement, from the urn. Then the probability of observing 3 white balls (and thus 1 black ball) would be
</p>


\begin{equation}
\P(3W,1B)=\frac{{7 \choose 3}{5 \choose 1}}{{12 \choose 4}}.
\end{equation}

<p>
More generally, we sample without replacement \(K\) times from an urn with \(M\) white balls and \(N\) black balls. Let \(X\) be the number of white balls in the sample. The PMF of \(X\) is
</p>


\begin{equation}
f_{X}(x)=\frac{{M \choose x}{N \choose K-x}}{{M+N \choose K}}.
\end{equation}

<p>
We say that \(X\) has a <i>hypergeometric distribution</i> and write \(X\sim\mathsf{hyper}(\mathtt{m}=M,\,\mathtt{n}=N,\,\mathtt{k}=K)\).
</p>
<p>
The support set for the hypergeometric distribution is a little bit tricky. It is tempting to say that \(x\) should go from 0 (no white balls in the sample) to \(K\) (no black balls in the sample), but that does not work if \(K&gt;M\), because it is impossible to have more white balls in the sample than there were white balls originally in the urn. We have the same trouble if \(K&gt;N\). The good news is that the majority of examples we study have \(K\leq M\) and \(K\leq N\) and we will happily take the support to be \(x=0,\ 1,\ \ldots,\ K\). 
</p>
<p>
It is shown in Exercise <a href="#xca-hyper-mean-variance">hyper-mean-variance</a> that
\begin{equation}
\mu=K\frac{M}{M+N},\quad\sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}.
\end{equation}

The associated \textsf{R} functions for the PMF and CDF are <code>dhyper(x, m, n, k)</code> and <code>phyper</code>, respectively. There are two more functions: <code>qhyper</code>, which we will discuss in Section <a href="#sub-Normal-Quantiles-QF">Normal-Quantiles-QF</a>, and <code>rhyper</code>, discussed below.
</p>
<p>
Suppose in a certain shipment of 250 Pentium processors there are 17 defective processors. A quality control consultant randomly collects 5 processors for inspection to determine whether or not they are defective. Let \(X\) denote the number of defectives in the sample.
</p>



\begin{enumerate}
\item Find the probability of exactly 3 defectives in the sample, that is,
find $\P(X=3)$. 

/Solution:/ We know that $X\sim\mathsf{hyper}(\mathtt{m}=17,\,\mathtt{n}=233,\,\mathtt{k}=5)$. So the required probability is just
\[
f_{X}(3)=\frac{{17 \choose 3}{233 \choose 2}}{{250 \choose 5}}.
\]

To calculate it in \textsf{R} we just type 


<pre class="example">dhyper(3, m = 17, n = 233, k = 5)

</pre>




: [1] 0.002351153

To find it with the \textsf{R} Commander we go \textsf{Probability} $\triangleright$ \textsf{Discrete Distributions} $\triangleright$ \textsf{Hypergeometric distribution} $\triangleright$ \textsf{Hypergeometric probabilities}... . We fill in the parameters $m=17$, $n=233$, and $k=5$. Click \textsf{OK}, and the following table is shown in the window.


<pre class="example">A &lt;- data.frame(Pr=dhyper(0:4, m = 17, n = 233, k = 5))
rownames(A) &lt;- 0:4 
A

</pre>




:             Pr
: 0 7.011261e-01
: 1 2.602433e-01
: 2 3.620776e-02
: 3 2.351153e-03
: 4 7.093997e-05

We wanted $\P(X=3)$, and this is found from the table to be approximately 0.0024. The value is rounded to the fourth decimal place.

We know from our above discussion that the sample space should be $x=0,1,2,3,4,5$, yet, in the table the probabilities are only displayed for $x=1,2,3$, and 4. What is happening? As it turns out, the \textsf{R} Commander will only display probabilities that are 0.00005 or greater. Since $x=5$ is not shown, it suggests that the outcome has a tiny probability. To find its exact value we use the =dhyper= function:


<pre class="example">dhyper(5, m = 17, n = 233, k = 5)

</pre>




: [1] 7.916049e-07

In other words, $\P(X=5)\approx0.0000007916049$, a small number indeed.

\item Find the probability that there are at most 2 defectives in the sample, that is, compute $\P(X\leq2)$.

/Solution:/ Since $\P(X\leq2)=\P(X=0,1,2)$, one way to do this would be to add the 0, 1, and 2 entries in the above table. this gives $0.7011+0.2602+0.0362=0.9975$. Our answer should be correct up to the accuracy of 4 decimal places. However, a more precise method is provided by the \textsf{R} Commander. Under the \textsf{Hypergeometric distribution} menu we select \textsf{Hypergeometric tail probabilities}.... We fill in the parameters $m$, $n$, and $k$ as before, but in the \textsf{Variable value(s)} dialog box we enter the value 2. We notice that the =Lower tail= option is checked, and we leave that alone. Click \textsf{OK}.


<pre class="example">phyper(2, m = 17, n = 233, k = 5)

</pre>




: [1] 0.9975771

And thus $\P(X\leq2)\approx 0.9975771$. We have confirmed that the above answer was correct up to four decimal places.

\item Find $\P(X>1)$. 

The table did not give us the explicit probability $\P(X=5)$, so we can not use the table to give us this probability. We need to use another method. Since $\P(X>1)=1-\P(X\leq1)=1-F_{X}(1)$, we can find the probability with \textsf{Hypergeometric tail probabilities}.... We enter 1 for \textsf{Variable Value(s)}, we enter the parameters as before, and in this case we choose the =Upper tail= option. This results in the following output.


<pre class="example">phyper(1, m = 17, n = 233, k = 5, lower.tail = FALSE)

</pre>




: [1] 0.03863065

In general, the =Upper tail= option of a tail probabilities dialog computes $\P(X>x)$ for all given \textsf{Variable Value(s)} $x$.

\item Generate $100,000$ observations of the random variable $X$.


We can randomly simulate as many observations of $X$ as we want in \textsf{R} Commander. Simply choose \textsf{Simulate hypergeometric variates}... in the \textsf{Hypergeometric distribution} dialog. 

In the \textsf{Number of samples} dialog, type 1. Enter the parameters as above. Under the \textsf{Store Values} section, make sure \textsf{New Data set} is selected. Click \textsf{OK}. 

A new dialog should open, with the default name =Simset1=.  We could change this if we like, according to the rules for \textsf{R} object names. In the sample size box, enter 100000. Click \textsf{OK}. 

In the Console Window, \textsf{R} Commander should issue an alert that \texttt{Simset1} has been initialized, and in a few seconds, it should also state that 100,000 hypergeometric variates were stored in =hyper.sim1=. We can view the sample by clicking the \textsf{View Data Set} button on the \textsf{R} Commander interface.

We know from our formulas that $\mu=K\cdot M/(M+N)=5*17/250=0.34$. We can check our formulas using the fact that with repeated observations of $X$ we would expect about 0.34 defectives on the average. To see how our sample reflects the true mean, we can compute the sample mean

:  Rcmdr> mean(Simset2$hyper.sim1, na.rm=TRUE)
:  [1] 0.340344

:  Rcmdr> sd(Simset2$hyper.sim1, na.rm=TRUE)
:  [1] 0.5584982
:  ...

We see that when given many independent observations of $X$, the sample mean is very close to the true mean $\mu$. We can repeat the same idea and use the sample standard deviation to estimate the true standard deviation of $X$. From the output above our estimate is 0.5584982, and from our formulas we get
\[
\sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}\approx0.3117896,
\]
with $\sigma=\sqrt{\sigma^{2}}\approx0.5583811944$. Our estimate was pretty close.

From the console we can generate random hypergeometric variates with the =rhyper= function, as demonstrated below.


<pre class="example">rhyper(10, m = 17, n = 233, k = 5)

</pre>




:  [1] 0 0 1 0 1 0 0 1 0 0

\end{enumerate}

<p>
\label{sub:Sampling-With-and}
</p>
<p>
Suppose that we have a large urn with, say, \(M\) white balls and \(N\) black balls. We take a sample of size \(n\) from the urn, and let \(X\) count the number of white balls in the sample. If we sample
</p>


\begin{description}
\item [{without~replacement,}] then $X\sim\mathsf{hyper}(\mathtt{m=}M,\,\mathtt{n}=N,\,\mathtt{k}=n)$ and has mean and variance

\begin{alignat*}{1}
\mu= & n\frac{M}{M+N},\\
\sigma^{2}= & n\frac{MN}{(M+N)^{2}}\frac{M+N-n}{M+N-1},\\
= & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right)\frac{M+N-n}{M+N-1}.
\end{alignat*}

On the other hand, if we sample
\item [{with~replacement,}] then $X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=M/(M+N))$ with mean and variance

\begin{alignat*}{1}
\mu= & n\frac{M}{M+N},\\
\sigma^{2}= & n\frac{M}{M+N}\left(1-\frac{M}{M+N}\right).
\end{alignat*}

\end{description}

<p>
We see that both sampling procedures have the same mean, and the method with the larger variance is the ``with replacement'' scheme. The factor by which the variances differ,
\begin{equation}
\frac{M+N-n}{M+N-1},
\end{equation}
is called a <i>finite population correction</i>. For a fixed sample size \(n\), as \(M,N\to\infty\) it is clear that the correction goes to 1, that is, for infinite populations the sampling schemes are essentially the same with respect to mean and variance.
</p>

</div>

</div>

<div id="outline-container-4-6-2" class="outline-4">
<h4 id="sec-4-6-2"><span class="section-number-4">4.6.2</span> Waiting Time Distributions </h4>
<div class="outline-text-4" id="text-4-6-2">

<p>\label{sec:Waiting-Time-Distributions}
</p>
<p>
Another important class of problems is associated with the amount of time it takes for a specified event of interest to occur. For example, we could flip a coin repeatedly until we observe Heads. We could toss a piece of paper repeatedly until we make it in the trash can.
</p>

<p>
\label{sub:The-Geometric-Distribution}
</p>
<p>
Suppose that we conduct Bernoulli trials repeatedly, noting the successes and failures. Let \(X\) be the number of failures before a success. If \(\P(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)=p(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}

(Why?) We say that \(X\) has a <i>Geometric distribution</i> and we write \(X\sim\mathsf{geom}(\mathtt{prob}=p)\). The associated \textsf{R} functions are <code>dgeom(x, prob)</code>, <code>pgeom</code>, <code>qgeom</code>, and <code>rhyper</code>, which give the PMF, CDF, quantile function, and simulate random variates, respectively.
</p>
<p>
Again it is clear that \(f(x)\geq0\) and we check that \(\sum f(x)=1\) (see Equation <a href="#eq-geom-series">geom-series</a> in Appendix <a href="#sec-Sequences-and-Series">Sequences-and-Series</a>):
\begin{alignat*}{1}
\sum_{x=0}^{\infty}p(1-p)^{x}= & p\sum_{x=0}^{\infty}q^{x}=p\,\frac{1}{1-q}=1.
\end{alignat*}

We will find in the next section that the mean and variance are
\begin{equation}
\mu=\frac{1-p}{p}=\frac{q}{p}\mbox{ and }\sigma^{2}=\frac{q}{p^{2}}.
\end{equation}


The Pittsburgh Steelers place kicker, Jeff Reed, made 81.2% of his attempted field goals in his career up to 2006. Assuming that his successive field goal attempts are approximately Bernoulli trials, find the probability that Jeff misses at least 5 field goals before his first successful goal.
</p>
<p>
<i>Solution</i>: If \(X=\) the number of missed goals until Jeff's first success, then \(X\sim\mathsf{geom}(\mathtt{prob}=0.812)\) and we want \(\P(X\geq5)=\P(X&gt;4)\). We can find this in \textsf{R} with
</p>



<pre class="example">pgeom(4, prob = 0.812, lower.tail = FALSE)

</pre>




<pre class="example">
[1] 0.0002348493
</pre>






\begin{note}
Some books use a slightly different definition of the geometric distribution. They consider Bernoulli trials and let $Y$ count instead the number of trials until a success, so that $Y$ has PMF
\begin{equation}
f_{Y}(y)=p(1-p)^{y-1},\quad y=1,2,3,\ldots
\end{equation}

When they say ``geometric distribution'', this is what they mean. It is not hard to see that the two definitions are related. In fact, if $X$ denotes our geometric and $Y$ theirs, then $Y=X+1$. Consequently, they have $\mu_{Y}=\mu_{X}+1$ and $\sigma_{Y}^{2}=\sigma_{X}^{2}$.
\end{note}

<p>
\label{sub:The-Negative-Binomial}
</p>
<p>
We may generalize the problem and consider the case where we wait for <i>more</i> than one success. Suppose that we conduct Bernoulli trials repeatedly, noting the respective successes and failures. Let \(X\) count the number of failures before \(r\) successes. If \(\P(S)=p\) then \(X\) has PMF
\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},\quad x=0,1,2,\ldots
\end{equation}

We say that \(X\) has a <i>Negative Binomial distribution</i> and write \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). The associated \textsf{R} functions are <code>dnbinom(x, size, prob)</code>, <code>pnbinom</code>, <code>qnbinom</code>, and <code>rnbinom</code>, which give the PMF, CDF, quantile function, and simulate random variates, respectively.
</p>
<p>
As usual it should be clear that \(f_{X}(x)\geq 0\) and the fact that \(\sum f_{X}(x)=1\) follows from a generalization of the geometric series by means of a Maclaurin's series expansion:
</p>


\begin{alignat}{1}
\frac{1}{1-t}= & \sum_{k=0}^{\infty}t^{k},\quad\mbox{for \ensuremath{-1<t<1}},\mbox{ and}\\
\frac{1}{(1-t)^{r}}= & \sum_{k=0}^{\infty}{r+k-1 \choose r-1}\, t^{k},\quad\mbox{for \ensuremath{-1<t<1}}.
\end{alignat}

<p>
Therefore
\begin{equation}
\sum_{x=0}^{\infty}f_{X}(x)=p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}\, q^{x}=p^{r}(1-q)^{-r}=1,
\end{equation}
since \(|q|=|1-p|&lt;1\). 
</p>
<p>
We flip a coin repeatedly and let \(X\) count the number of Tails until we get seven Heads. What is \(\P(X=5)?\)
</p>
<p>
<i>Solution</i>: We know that \(X\sim\mathsf{nbinom}(\mathtt{size}=7,\,\mathtt{prob}=1/2)\).
\[
\P(X=5)=f_{X}(5)={7+5-1 \choose 7-1}(1/2)^{7}(1/2)^{5}={11 \choose 6}2^{-12}
\]

and we can get this in \textsf{R} with
</p>



<pre class="example">dnbinom(5, size = 7, prob = 0.5)

</pre>




<pre class="example">
[1] 0.112793
</pre>


<p>
Let us next compute the MGF of \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\).
</p>


\begin{alignat*}{1}
M_{X}(t)= & \sum_{x=0}^{\infty}\me^{tx}\ {r+x-1 \choose r-1}p^{r}q^{x}\\
= & p^{r}\sum_{x=0}^{\infty}{r+x-1 \choose r-1}[q\me^{t}]^{x}\\
= & p^{r}(1-qe^{t})^{-r},\quad\mbox{provided \ensuremath{|q\me^{t}|<1,}}
\end{alignat*}

<p>
and so
</p>


\begin{equation}
M_{X}(t)=\left(\frac{p}{1-q\me^{t}}\right)^{r},\quad\mbox{for \ensuremath{q\me^{t}<1}}.
\end{equation}

<p>
We see that \(q\me^{t}&lt;1\) when \(t&lt;-\ln(1-p)\).
</p>
<p>
Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\mathtt{prob}=p)\mbox{ with \ensuremath{M(t)=p^{r}(1-q\me^{t})^{-r}}}\). We proclaimed above the values of the mean and variance. Now we are equipped with the tools to find these directly.
</p>


\begin{alignat*}{1}
M'(t)= & p^{r}(-r)(1-q\me^{t})^{-r-1}(-q\me^{t}),\\
= & rq\me^{t}p^{r}(1-q\me^{t})^{-r-1},\\
= & \frac{rq\me^{t}}{1-q\me^{t}}M(t),\mbox{ and so }\\
M'(0)= & \frac{rq}{1-q}\cdot1=\frac{rq}{p}.
\end{alignat*}


<p>
Thus \(\mu=rq/p\). We next find \(\E X^{2}\).
</p>


\begin{alignat*}{1}
M''(0)= & \left.\frac{rq\me^{t}(1-q\me^{t})-rq\me^{t}(-q\me^{t})}{(1-q\me^{t})^{2}}M(t)+\frac{rq\me^{t}}{1-q\me^{t}}M'(t)\right|_{t=0},\\
= & \frac{rqp+rq^{2}}{p^{2}}\cdot1+\frac{rq}{p}\left(\frac{rq}{p}\right),\\
= & \frac{rq}{p^{2}}+\left(\frac{rq}{p}\right)^{2}.
\end{alignat*}

<p>
Finally we may say $&sigma;<sup>2</sup>=M''(0)-[M'(0)]<sup>2</sup>=rq/p<sup>2</sup>.$
</p>
<p>
A random variable has MGF
</p>


\[
M_{X}(t)=\left(\frac{0.19}{1-0.81\me^{t}}\right)^{31}.
\]

<p>
Then \(X\sim\mathsf{nbinom}(\mathtt{size}=31,\,\mathtt{prob}=0.19)\).
</p>



\begin{note}
As with the Geometric distribution, some books use a slightly different definition of the Negative Binomial distribution. They consider Bernoulli trials and let $Y$ be the number of trials until $r$ successes, so that $Y$ has PMF

\begin{equation}
f_{Y}(y)={y-1 \choose r-1}p^{r}(1-p)^{y-r},\quad y=r,r+1,r+2,\ldots
\end{equation}

It is again not hard to see that if $X$ denotes our Negative Binomial and $Y$ theirs, then $Y=X+r$. Consequently, they have $\mu_{Y}=\mu_{X}+r$ and $\sigma_{Y}^{2}=\sigma_{X}^{2}$.

\end{note}

</div>

</div>

<div id="outline-container-4-6-3" class="outline-4">
<h4 id="sec-4-6-3"><span class="section-number-4">4.6.3</span> Arrival Processes </h4>
<div class="outline-text-4" id="text-4-6-3">

<p>\label{sec:Arrival-Processes}
</p>

<p>
\label{sub:The-Poisson-Distribution}
</p>
<p>
This is a distribution associated with ``rare events'', for reasons which will become clear in a moment. The events might be:
</p><ul>
<li>traffic accidents,
</li>
<li>typing errors, or
</li>
<li>customers arriving in a bank.
</li>
</ul>

<p>Let \(\lambda\) be the average number of events in the time interval \([0,1]\). Let the random variable \(X\) count the number of events occurring in the interval. Then under certain reasonable conditions it can be shown that
</p>


\begin{equation}
f_{X}(x)=\P(X=x)=\me^{-\lambda}\frac{\lambda^{x}}{x!},\quad x=0,1,2,\ldots
\end{equation}

<p>
We use the notation \(X\sim\mathsf{pois}(\mathtt{lambda}=\lambda)\). The associated \textsf{R} functions are <code>dpois(x, lambda)</code>, <code>ppois</code>, <code>qpois</code>, and <code>rpois</code>, which give the PMF, CDF, quantile function, and simulate random variates, respectively.
</p>


<p>
Divide \([0,1]\) into subintervals of length \(1/n\). A <i>Poisson process</i>\index{Poisson process} satisfies the following conditions:
</p>
<ul>
<li>the probability of an event occurring in a particular subinterval is \(\approx\lambda/n\).

</li>
<li>the probability of two or more events occurring in any subinterval is \(\approx 0\).

</li>
<li>occurrences in disjoint subintervals are independent.
</li>
</ul>




\begin{rem}
\label{rem:poisson-process}

If $X$ counts the number of events in the interval $[0,t]$ and $\lambda$ is the average number that occur in unit time, then $X\sim\mathsf{pois}(\mathtt{lambda}=\lambda t)$, that is,
\begin{equation}
\P(X=x)=\me^{-\lambda t}\frac{(\lambda t)^{x}}{x!},\quad x=0,1,2,3\ldots
\end{equation}
\end{rem}

<p>
On the average, five cars arrive at a particular car wash every hour. Let \(X\) count the number of cars that arrive from 10AM to 11AM. Then \(X\sim\mathsf{pois}(\mathtt{lambda}=5)\). Also, \(\mu=\sigma^{2}=5\). What is the probability that no car arrives during this period? 
</p>
<p>
<i>Solution</i>: The probability that no car arrives is
\[
\P(X=0)=\me^{-5}\frac{5^{0}}{0!}=\me^{-5}\approx0.0067.
\]


Suppose the car wash above is in operation from 8AM to 6PM, and we let \(Y\) be the number of customers that appear in this period. Since this period covers a total of 10 hours, from Remark <a href="#rem-poisson-process">poisson-process</a> we get that \(Y\sim\mathsf{pois}(\mathtt{lambda}=5\ast10=50)\). What is the probability that there are between 48 and 50 customers, inclusive? 
</p>
<p>
<i>Solution</i>: We want \(\P(48\leq Y\leq50)=\P(X\leq50)-\P(X\leq47)\). 
</p>



<pre class="example">diff(ppois(c(47, 50), lambda = 50))

</pre>




<pre class="example">
[1] 0.1678485
</pre>



</div>
</div>

</div>

<div id="outline-container-4-7" class="outline-3">
<h3 id="sec-4-7"><span class="section-number-3">4.7</span> Functions of Discrete Random Variables </h3>
<div class="outline-text-3" id="text-4-7">

<p>\label{sec:functions-discrete-rvs}
</p>
<p>
We have built a large catalogue of discrete distributions, but the tools of this section will give us the ability to consider infinitely many more. Given a random variable \(X\) and a given function \(h\), we may consider \(Y=h(X)\). Since the values of \(X\) are determined by chance, so are the values of \(Y\). The question is, what is the PMF of the random variable \(Y\)? The answer, of course, depends on \(h\). In the case that \(h\) is one-to-one (see Appendix <a href="#sec-Differential-and-Integral">Differential-and-Integral</a>), the solution can be found by simple substitution.
</p>

<p>
Let \(X\sim\mathsf{nbinom}(\mathtt{size}=r,\,\mathtt{prob}=p)\). We saw in <a href="#sec-other-discrete-distributions">other-discrete-distributions</a> that \(X\) represents the number of failures until \(r\) successes in a sequence of Bernoulli trials. Suppose now that instead we were interested in counting the number of trials (successes and failures) until the \(r^{\mathrm{th}}\) success occurs, which we will denote by \(Y\). In a given performance of the experiment, the number of failures (\(X\)) and the number of successes (\(r\)) together will comprise the total number of trials (\(Y\)), or in other words, \(X+r=Y\). We may let \(h\) be defined by \(h(x)=x+r\) so that \(Y=h(X)\), and we notice that \(h\) is linear and hence one-to-one. Finally, \(X\) takes values \(0,\ 1,\ 2,\ldots\) implying that the support of \(Y\) would be \(\{ r,\ r+1,\ r+2,\ldots \}\). Solving for \(X\) we get \(X=Y-r\). Examining the PMF of \(X\)
</p>


\begin{equation}
f_{X}(x)={r+x-1 \choose r-1}\, p^{r}(1-p)^{x},
\end{equation}

<p>
we can substitute \( x = y - r \) to get
</p>


\begin{eqnarray*}
f_{Y}(y) & = & f_{X}(y-r),\\
 & = & {r+(y-r)-1 \choose r-1}\, p^{r}(1-p)^{y-r},\\
 & = & {y-1 \choose r-1}\, p^{r}(1-p)^{y-r},\quad y=r,\, r+1,\ldots
\end{eqnarray*}



<p>
Even when the function \(h\) is not one-to-one, we may still find the PMF of \(Y\) simply by accumulating, for each \(y\), the probability of all the \(x\)'s that are mapped to that \(y\).
</p>


\begin{prop}
Let $X$ be a discrete random variable with PMF $f_{X}$ supported on the set $S_{X}$. Let $Y=h(X)$ for some function $h$. Then $Y$ has PMF $f_{Y}$ defined by
\begin{equation}
f_{Y}(y)=\sum_{\{x\in S_{X}|\, h(x)=y\}}f_{X}(x)
\end{equation}
\end{prop}

<p>
Let \(X\sim\mathsf{binom}(\mathtt{size}=4,\,\mathtt{prob}=1/2)\), and let \(Y=(X-1)^{2}\). Consider the following table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="right" /><col class="right" /><col class="right" /><col class="right" /><col class="right" />
</colgroup>
<thead>
<tr><th scope="col" class="left">x</th><th scope="col" class="right">0</th><th scope="col" class="right">1</th><th scope="col" class="right">2</th><th scope="col" class="right">3</th><th scope="col" class="right">4</th></tr>
</thead>
<tbody>
<tr><td class="left">\(f_{X}(x)\)</td><td class="right">1/16</td><td class="right">1/4</td><td class="right">6/16</td><td class="right">1/4</td><td class="right">1/16</td></tr>
</tbody>
<tbody>
<tr><td class="left">\(y=(x-2)^{2}\)</td><td class="right">1</td><td class="right">0</td><td class="right">1</td><td class="right">4</td><td class="right">9</td></tr>
</tbody>
</table>


<p>
From this we see that \(Y\) has support \(S_{Y}=\{0,1,4,9\}\). We also see that \(h(x)=(x-1)^{2}\) is not one-to-one on the support of \(X\), because both \(x=0\) and \(x=2\) are mapped by \(h\) to \(y=1\). Nevertheless, we see that \(Y=0\) only when \(X=1\), which has probability \(1/4\); therefore, \(f_{Y}(0)\) should equal \(1/4\). A similar approach works for \(y=4\) and \(y=9\). And \(Y=1\) exactly when \(X=0\) or \(X=2\), which has total probability \(7/16\). In summary, the PMF of \(Y\) may be written:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">y</th><th scope="col" class="left">0</th><th scope="col" class="left">1</th><th scope="col" class="left">4</th><th scope="col" class="left">9</th></tr>
</thead>
<tbody>
<tr><td class="left">\(f_{Y}(y)\)</td><td class="left">1/4</td><td class="left">7/16</td><td class="left">1/4</td><td class="left">1/16</td></tr>
</tbody>
</table>


<p>
There is not a special name for the distribution of \(Y\), it is just an example of what to do when the transformation of a random variable is not one-to-one. The method is the same for more complicated problems.
</p>




\begin{prop}
If $X$ is a random variable with $\E X=\mu$ and $\mbox{Var}(X)=\sigma^{2}$, then the mean and variance of $Y=mX+b$ is
\begin{equation}
\mu_{Y}=m\mu+b,\quad\sigma_{Y}^{2}=m^{2}\sigma^{2},\quad\sigma_{Y}=|m|\sigma.
\end{equation}
\end{prop}


</div>

</div>

<div id="outline-container-4-8" class="outline-3">
<h3 id="sec-4-8"><span class="section-number-3">4.8</span> Chapter Exercises </h3>
<div class="outline-text-3" id="text-4-8">





\begin{xca}
A recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let $X$ equal the number of students in a random sample of size $n=31$ who have used Wikipedia as a source. 

\begin{enumerate}
\item How is $X$ distributed? 

\[
X\sim\mathsf{binom}(\mathtt{size}=31,\,\mathtt{prob}=0.447)
\]

\item Sketch the probability mass function (roughly).

<<echo = FALSE, fig=true, height = 3, width = 6>>=
xmin <- qbinom(.0005, size=31 , prob=0.447) 
xmax <- qbinom(.9995, size=31 , prob=0.447) 
.x <- xmin:xmax 
plot(.x, dbinom(.x, size=31, prob=0.447), xlab="Number of Successes", ylab="Probability Mass",    main="Binomial Dist'n: Trials = 31, Prob of success = 0.447", type="h") 
points(.x, dbinom(.x, size=31, prob=0.447), pch=16) 
abline( h = 0, lty = 2, col = "grey" ) 
remove(.x, xmin, xmax)

\item Sketch the cumulative distribution function (roughly).


<<echo = FALSE, fig=true, height = 3, width = 6>>=
xmin <- qbinom(.0005, size=31 , prob=0.447) 
xmax <- qbinom(.9995, size=31 , prob=0.447) 
.x <- xmin:xmax 
plot( stepfun(.x, pbinom((xmin-1):xmax, size=31, prob=0.447)), verticals=FALSE, do.p=FALSE, xlab="Number of Successes", ylab="Cumulative Probability", main="Binomial Dist'n: Trials = 31, Prob of success = 0.447") 
points( .x, pbinom(xmin:xmax, size=31, prob=0.447), pch = 16, cex=1.2 ) 
points( .x, pbinom((xmin-1):(xmax-1), size=31, prob=0.447), pch = 1,    cex=1.2 ) 
abline( h = 1, lty = 2, col = "grey" ) 
abline( h = 0, lty = 2, col = "grey" ) 
remove(.x, xmin, xmax) 



\item Find the probability that $X$ is equal to 17.



<pre class="example">dbinom(17, size = 31, prob = 0.447)

</pre>




: [1] 0.07532248

\item Find the probability that $X$ is at most 13.



<pre class="example">pbinom(13, size = 31, prob = 0.447)

</pre>




: [1] 0.451357

\item Find the probability that $X$ is bigger than 11.



<pre class="example">pbinom(11, size = 31, prob = 0.447, lower.tail = FALSE)

</pre>




: [1] 0.8020339

\item Find the probability that $X$ is at least 15.



<pre class="example">pbinom(14, size = 31, prob = 0.447, lower.tail = FALSE)

</pre>




: [1] 0.406024

\item Find the probability that $X$ is between 16 and 19, inclusive.



<pre class="example">sum(dbinom(16:19, size = 31, prob = 0.447))
diff(pbinom(c(19,15), size = 31, prob = 0.447, lower.tail = FALSE))

</pre>




: [1] 0.2544758
: [1] 0.2544758

\item Give the mean of $X$, denoted $\E X$.



<pre class="example">library(distrEx)
X = Binom(size = 31, prob = 0.447)
E(X)

</pre>




: [1] 13.857

\item Give the variance of $X$.



<pre class="example">var(X)

</pre>




: [1] 7.662921

\item Give the standard deviation of $X$.


<pre class="example">sd(X)

</pre>




: [1] 2.768198

\item Find $\E(4X+51.324)$


<pre class="example">E(4*X + 51.324)

</pre>




: [1] 106.752

\end{enumerate}
\end{xca}


\begin{xca}
For the following situations, decide what the distribution of $X$ should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)
\end{xca}

<ul>
<li>We shoot basketballs at a basketball hoop, and count the number of shots until we make a goal. Let \(X\) denote the number of missed shots. On a normal day we would typically make about 37% of the shots.

</li>
<li>In a local lottery in which a three digit number is selected randomly, let \(X\) be the number selected.

</li>
<li>We drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let \(X\) be the number of times the cup lands perfectly right side up.

</li>
<li>We toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let \(X\) denote the number of missed shots.

</li>
<li>Working for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let \(X\) count the number of containers that illegally contain contraband.

</li>
<li>At the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let \(X\) denote the number of birds in the bush. 

</li>
<li>We count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.

</li>
<li>We count the number of moth eggs on our window screen.

</li>
<li>We count the number of blades of grass in a one square foot patch of land.

</li>
<li>We count the number of pats on a baby's back until (s)he burps.
</li>
</ul>


\begin{xca}
\label{xca:variance-shortcut}
Show that $\E(X-\mu)^{2}=\E X^{2}-\mu^{2}$. /Hint/: expand the quantity $(X-\mu)^{2}$ and distribute the expectation over the resulting terms.
\end{xca}

\begin{xca}
\label{xca:binom-factorial-expectation}
If $X\sim\mathsf{binom}(\mathtt{size}=n,\,\mathtt{prob}=p)$ show that $\E X(X-1)=n(n-1)p^{2}$.
\end{xca}

\begin{xca}
\label{xca:hyper-mean-variance}
Calculate the mean and variance of the hypergeometric distribution. Show that 
\begin{equation}
\mu=K\frac{M}{M+N},\quad\sigma^{2}=K\frac{MN}{(M+N)^{2}}\frac{M+N-K}{M+N-1}.
\end{equation}
\end{xca}


</div>
</div>
</div>
<div id="postamble">
<p class="date">Date: \fontsize{24}{28}\selectfont \noun{Second Edition}</p>
<p class="author">Author: \fontsize{24}{28}\selectfont G. Jay Kerns</p>
<p class="creator">Org version 7.6 with Emacs version 23</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>
</div>
</div>
</body>
</html>
